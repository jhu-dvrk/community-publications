@article{mocellin2026origrasp,
  title={OriGrasp: A Multifunctional Origami-Inspired Instrument for Delicate Manipulation in Abdominal Surgery},
  author={Mocellin, Lorenzo and Pagliarani, Niccol{\`o} and Gamberini, Giulia and Ciuti, Gastone and Cianchetti, Matteo and Menciassi, Arianna},
  journal={Advanced Intelligent Systems},
  pages={e202501249},
  year={2026},
  doi={10.1002/aisy.202501249},
  publisher={Wiley Online Library},
  research_field = {HW},
  data_type = {KD},
  dvrk_site = {SSSA},
}

@inproceedings{WangJ2025ISMR,
  author = {Wang, Junxiang and Barragan, Juan Antonio and Ishida, Hisashi and Guo, Jingkai and Ku, Yu-Chun and Kazanzides, Peter},
  title = {A Digital Twin for Telesurgery under Intermittent Communication},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2025},
  doi = {10.1109/ISMR67322.2025.11025988},
  semanticscholar = {https://www.semanticscholar.org/paper/52ac37919a99926e944a05214a0674330298199c},
  arxiv = {https://arxiv.org/abs/2411.13449},
  dvrk_site = {JHU},
  abstract = {Telesurgery is an effective way to deliver service from expert surgeons to areas without immediate access to specialized resources. However, many of these areas, such as rural districts or battlefields, might be subject to different problems in communication, especially latency and intermittent periods of communication outage. This challenge motivates the use of a digital twin for the surgical system, where a simulation would mirror the robot hardware and surgical environment in the real world. The surgeon would then be able to interact with the digital twin during communication outage, followed by a recovery strategy on the real robot upon reestablishing communication. This paper builds the digital twin for the da Vinci surgical robot, with a buffering and replay strategy that reduces the mean task completion time by 23 % when compared to the baseline, for a peg transfer task subject to intermittent communication outage. The relevant code can be found here: https://github.com/LCSR-CIIS/dvrk_digital_twin_teleoperation.},
  keywords = {dvrk},
  date = {2025-05-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2025-05-01},
}

@inproceedings{iovene2025situation,
  author = {Iovene, Elisa and Lev, Hanna Kossowsky and Sharon, Yarden and Netz, Uri and Geftler, Alex and Ferrigno, Giancarlo and De Momi, Elena and Nisky, Ilana},
  title = {A Situation-Aware Autonomous Camera Alignment for Enhanced Suturing in Robot-Assisted Minimally Invasive Surgery},
  booktitle = {2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2025},
  volume = {},
  number = {},
  pages = {2126-2133},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {AU and TR and SS},
  data_type = {KD},
}

@article{Chen2025AccuracyAA,
  author = {Xinhao Chen and M. Kam and Nural Yilmaz and A. Deguet and Axel Krieger},
  title = {Accuracy Analysis and Enhancement via Transformer-based Robot Calibration of the da Vinci Research Kit Si (dVRK-Si)},
  journal = {J. Medical Robotics Res.},
  booktitle = {J. Medical Robotics Res.},
  year = {2025},
  volume = {10},
  pages = {2550001:1-2550001:11},
  doi = {10.48550/arXiv.2502.18586},
  semanticscholar = {https://www.semanticscholar.org/paper/f468bb674fac8fbbcbece1d58c89ce2fd6f1cedc},
  arxiv = {https://arxiv.org/abs/2502.18586},
  dvrk_site = {JHU},
  abstract = {Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.},
}

@inproceedings{WangS2025ISMR,
  author = {Wang, Shiyue and Wang, Jing-Fen and Yubin, Koh and Zhou, Haoying and Deguet, Anton and Kazanzides, Peter},
  title = {An Augmented Reality Measurement Tool for the da Vinci Research Kit},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2025},
  doi = {10.1109/ISMR67322.2025.11025981},
  semanticscholar = {https://www.semanticscholar.org/paper/6fde89f2ef30ccbc62568d0088018096d9b18b3e},
  dvrk_site = {JHU},
  abstract = {We present an augmented reality surgical robotics console for the da Vinci Research Kit (dVRK) that integrates a real-time distance measurement tool to enhance spatial awareness during robotic-assisted surgical procedures through an interactive system. The system integrates an RViz-based interface with ROS topics, enabling virtual cursor visualization and interactive controls for precise measurements on tissue or phantom models. It supports two distinct measurement approaches: (1) a virtual measurement capability, where the operator uses the two Master Tool Manipulators (MTMs) to drive virtual 3D measurement cursors in the stereo display (a “masters as mice” mode), and (2) a physical measurement tool, where the user teleoperates one or more Patient Side Manipulators (PSMs) to touch two features and report the intervening distance. We conduct a user study on virtual (MTM) and teleoperation (PSM) modes to assess performance and accuracy and gather user feedback. The results show a Root Mean Square Error (RMSE) of 2.19 mm to 7.25 mm across different modes and tasks. This work underscores the importance of such systems for enhancing accuracy and usability in endoscopic surgery, where spatial awareness and intraoperative assessment are critical. Code is available at https://github.com/capricieuxV/RVinci_ISMR2025.git.},
  keywords = {dvrk},
  date = {2025-05-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2025-05-01},
}

@article{Yang2025TMRB,
  author = {Yang, Hao and Acar, Ayberk and Xu, Keshuai and Deguet, Anton and Kazanzides, Peter and Wu, Jie Ying},
  title = {An Effectiveness Study Across Baseline and Learning-Based Force Estimation Methods on the da Vinci Research Kit Si System},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2025},
  volume = {7},
  number = {4},
  pages = {1393-1397},
  doi = {10.1109/TMRB.2025.3589744},
  semanticscholar = {https://www.semanticscholar.org/paper/0b3a9084602035b6f30d0e4e47bc446c79908476},
  arxiv = {https://arxiv.org/abs/2405.07453},
  dvrk_site = {JHU and VU},
  abstract = {Robot-assisted minimally invasive surgery, such as through the da Vinci systems, improves precision and patient outcomes. However, da Vinci systems prior to da Vinci 5 lacked direct force-sensing capabilities, forcing surgeons to operate without the haptic feedback they get through laparoscopy. Our prior work restored force sensing through machine learning-based force estimation for an open-source surgical robotics research platform, the da Vinci Research Kit (dVRK) Classic. This study extends our previous method to the newer dVRK system, the dVRK-Si. Additionally, we benchmark the performance of the learning-based algorithm against baseline methods (which make simplifying assumptions on the torque) to study how the two systems differ. In both systems, the learning-based method outperforms baselines, but the difference is much larger in the dVRK-Si. Nonetheless, dVRK-Si force estimation accuracy lags behind the dVRK Classic, with Root Mean Square Error (RMSE) 2 to 3 times higher. Further analysis reveals suboptimal PID control in the dVRK-Si. We hypothesize that this is because, unlike the dVRK Classic, the dVRK-Si is not mechanically balanced and exhibits more complex internal dynamics. This study advances the understanding of learning-based force estimation and is the first work to implement learning-based dynamics estimation of the new dVRK-Si system.},
  keywords = {dvrk},
  date = {2025-11-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2025-01-01},
}

@article{Wu2025HTL,
  author = {Wu, Zijian and Adam, Birdi and Kazanzides, Peter and Salcudean, Septimiu E.},
  title = {Augmenting efficient real-time surgical instrument segmentation in video with point tracking and Segment Anything},
  journal = {Healthcare Technology Letters},
  year = {2025},
  volume = {12},
  number = {1},
  doi = {10.1049/htl2.12111},
  semanticscholar = {https://www.semanticscholar.org/paper/1cff596218109a2ece4a1c335e5118262dc3bfab},
  arxiv = {https://arxiv.org/abs/2403.08003},
  dvrk_site = {UBC},
  abstract = {Abstract The Segment Anything model (SAM) is a powerful vision foundation model that is revolutionizing the traditional paradigm of segmentation. Despite this, a reliance on prompting each frame and large computational cost limit its usage in robotically assisted surgery. Applications, such as augmented reality guidance, require little user intervention along with efficient inference to be usable clinically. This study addresses these limitations by adopting lightweight SAM variants to meet the efficiency requirement and employing fine‐tuning techniques to enhance their generalization in surgical scenes. Recent advancements in tracking any point have shown promising results in both accuracy and efficiency, particularly when points are occluded or leave the field of view. Inspired by this progress, a novel framework is presented that combines an online point tracker with a lightweight SAM model that is fine‐tuned for surgical instrument segmentation. Sparse points within the region of interest are tracked and used to prompt SAM throughout the video sequence, providing temporal consistency. The quantitative results surpass the state‐of‐the‐art semi‐supervised video object segmentation method XMem on the EndoVis 2015 dataset with 84.8 IoU and 91.0 Dice. The method achieves promising performance that is comparable to XMem and transformer‐based fully supervised segmentation methods on ex vivo UCL dVRK and in vivo CholecSeg8k datasets. In addition, the proposed method shows promising zero‐shot generalization ability on the label‐free STIR dataset. In terms of efficiency, the method was tested on a single GeForce RTX 4060/4090 GPU respectively, achieving an over 25/90 FPS inference speed. Code is available at: https://github.com/zijianwu1231/SIS‐PT‐SAM.},
  keywords = {dvrk},
  date = {2025-01-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2025-01-01},
}

@article{sharon2025augmenting,
  author = {Sharon, Yarden and Nevo, Tifferet and Naftalovich, Daniel and Bahar, Lidor and Refaely, Yael and Nisky, Ilana},
  title = {Augmenting Robot-Assisted Pattern Cutting with Periodic Perturbations-Can We Make Dry Lab Training More Realistic?},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2025},
  volume = {72},
  number = {1},
  pages = {264-275},
  publisher = {IEEE},
  doi = {10.1109/TBME.2024.3450702},
  semanticscholar = {https://www.semanticscholar.org/paper/aeffcb6913486171744cd0bf91635a5032a15476},
  research_field = {TR},
  data_type = {RI and KD and ED},
  dvrk_site = {BGU},
  abstract = {Objective: Teleoperated robot-assisted minimally-invasive surgery (RAMIS) offers many advan tages over open surgery, but RAMIS training still requires optimization. Existing motor learning theories could improve RAMIS training. However, there is a gap between current knowledge based on simple movements and training approaches required for the more complicated work of RAMIS surgeons. Here, we studied how surgeons cope with time-dependent perturbations. Methods: We used the da Vinci Research Kit and investigated the effect of time-dependent force and motion perturbations on learning a circular pattern-cutting surgical task. Fifty-four participants were assigned to two experiments, with two groups for each: a control group trained without perturbations and an experimental group trained with 1 Hz perturbations. In the first experiment, force perturbations alternatingly pushed participants' hands inwards and outwards in the radial direction. In the second experiment, the perturbation constituted a periodic up-and-down motion of the task platform. Results: Participants trained with perturbations learned how to overcome them and improve their performances during training without impairing them after the perturbations were removed. Moreover, training with motion perturbations provided participants with an advantage when encountering the same or other perturbations after training, compared to training without perturbations. Conclusion: Periodic perturbations can enhance RAMIS training without impeding the learning of the perturbed task. Significance: Our results demonstrate that using challenging training tasks that include perturbations can better prepare surgical trainees for the dynamic environment they will face with patients in the operating room.},
}

@misc{zbinden2025cosmossurgdvrk,
  author = {Lukas Zbinden and Nigel Nelson and Juo-Tung Chen and Xinhao Chen and Ji Woong Kim and Mahdi Azizian and Axel Krieger and Sean Huver},
  title = {Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning},
  year = {2025},
  doi = {10.48550/arXiv.2510.16240},
  url = {https://arxiv.org/abs/2510.16240},
  semanticscholar = {https://www.semanticscholar.org/paper/a181f98f8d6f15acf8edf32d6c105e22b8dd27ce},
  arxiv = {https://arxiv.org/abs/2510.16240},
  abstract = {The rise of surgical robots and vision-language-action models has accelerated the development of autonomous surgical policies and efficient assessment strategies. However, evaluating these policies directly on physical robotic platforms such as the da Vinci Research Kit (dVRK) remains hindered by high costs, time demands, reproducibility challenges, and variability in execution. World foundation models (WFM) for physical AI offer a transformative approach to simulate complex real-world surgical tasks, such as soft tissue deformation, with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune of the Cosmos WFM, which, together with a trained video classifier, enables fully automated online evaluation and benchmarking of surgical policies. We evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop suture pad tasks, the automated pipeline achieves strong correlation between online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si platform, as well as good agreement between human labelers and the V-JEPA 2-derived video classifier. Additionally, preliminary experiments with ex-vivo porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising alignment with real-world evaluations, highlighting the platform's potential for more complex surgical procedures.},
  archiveprefix = {arXiv},
  eprint = {2510.16240},
  primaryclass = {cs.RO},
}

@article{sharon2025dataset,
  author = {Sharon, Yarden and Geftler, Alex, and Kossowsky Lev, Hanna and Nisky, Ilana},
  title = {Dataset and Analysis of Long-Term Skill Acquisition in Robot-Assisted Minimally Invasive Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2025},
  volume = {7},
  number = {4},
  pages = {1513-1524},
  publisher = {IEEE},
  doi = {10.1109/TMRB.2025.3617958},
  semanticscholar = {https://www.semanticscholar.org/paper/c7e352245078f1429e419c17d040a346e931cbce},
  arxiv = {https://arxiv.org/abs/2503.21591},
  research_field = {TR},
  data_type = {RI and KD},
  dvrk_site = {BGU},
  abstract = {Objective: We aim to investigate long-term robotic surgical skill acquisition among surgical residents and the effects of training intervals and fatigue on performance. Methods: For six months, surgical residents participated in three training sessions once a month, surrounding a single 26-hour hospital shift. In each shift, they participated in training sessions scheduled before, during, and after the shift. In each training session, they performed three dry-lab training tasks: Ring Tower Transfer, Knot-Tying, and Suturing. We collected a comprehensive dataset, including videos synchronized with kinematic data, activity tracking, and scans of the suturing pads. Results: We are releasing the dataset resulting in 972 trials performed by 18 residents of different surgical specializations. Participants demonstrated consistent performance improvement across all tasks. In addition, we found variations in between-shift learning and forgetting across metrics and tasks, and hints for possible effects of fatigue. Conclusion: The findings from our first analysis shed light on the long-term learning processes of robotic surgical skills with extended intervals and varying levels of fatigue. Significance: This study lays the groundwork for future research aimed at optimizing training protocols and enhancing AI applications in surgery, ultimately contributing to improved patient outcomes.},
}

@article{Abkhofte2025DeepLS,
  author = {Sara Abkhofte and A. Naidu and Rajni V. Patel and J. Jagadeesan},
  title = {Deep Learning-Based Segmentation for Autonomous Robot-Assisted Tumor Localization using Ultrasound B-Mode and Strain Elastography Imaging},
  journal = {2025 International Symposium on Medical Robotics (ISMR)},
  booktitle = {International Symposium on Medical Robotics},
  year = {2025},
  pages = {30-36},
  doi = {10.1109/ISMR67322.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
}

@article{Khan2025DevelopmentOA,
  author = {Aimal Khan and Hao Yang and Daniel Roy Sadek Habib and Danish Ali and Jie Ying Wu},
  title = {Development of a machine learning-based tension measurement method in robotic surgery},
  journal = {Surgical Endoscopy},
  booktitle = {Surgical Endoscopy},
  year = {2025},
  volume = {39},
  pages = {3422 - 3428},
  doi = {10.1038/s41551-023-01018-0},
  semanticscholar = {https://www.semanticscholar.org/paper/a9bddcf7ce391e9e75bf119789ba751ce2545f26},
  abstract = {Laryngeal lesions can be endoscopically detected with high sensitivity and in real time by measuring differences in the polarization signatures between cancerous and healthy tissues. The standard-of-care for the detection of laryngeal pathologies involves distinguishing suspicious lesions from surrounding healthy tissue via contrasts in colour and texture captured by white-light endoscopy. However, the technique is insufficiently sensitive and thus leads to unsatisfactory rates of false negatives. Here we show that laryngeal lesions can be better detected in real time by taking advantage of differences in the light-polarization properties of cancer and healthy tissues. By measuring differences in polarized-light retardance and depolarization, the technique, which we named ‘surgical polarimetric endoscopy’ (SPE), generates about one-order-of-magnitude greater contrast than white-light endoscopy, and hence allows for the better discrimination of cancerous lesions, as we show with patients diagnosed with squamous cell carcinoma. Polarimetric imaging of excised and stained slices of laryngeal tissue indicated that changes in the retardance of polarized light can be largely attributed to architectural features of the tissue. We also assessed SPE to aid routine transoral laser surgery for the removal of a cancerous lesion, indicating that SPE can complement white-light endoscopy for the detection of laryngeal cancer.},
}

@article{Liang2025DifferentiableRP,
  author = {Zekai Liang and Zih-Yun Chiu and Florian Richter and Michael C. Yip},
  title = {Differentiable Rendering-based Pose Estimation for Surgical Robotic Instruments},
  journal = {2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
  year = {2025},
  pages = {20898-20905},
  doi = {10.1109/iros47612.2022.9981344},
  semanticscholar = {https://www.semanticscholar.org/paper/dda867de88aff74f70fb9b2de0387297183afea5},
  dvrk_site = {UCSD},
  abstract = {The 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) will be held on October 23–27, 2022 in The Kyoto International Conference Center, Kyoto, Japan. The IROS is one of the largest and most impacting robotics research conferences worldwide. It provides an international forum for the international robotics research community to explore the frontier of science and technology in intelligent robots and smart machines. The theme of IROS 2022 is “Embodied AI for a Symbiotic Society’’. In addition to technical sessions and multi-media presentations, IROS conferences also hold panel discussions, forums, workshops, tutorials, exhibits, and technical tours to enrich the fruitful discussions among conference attendees.},
}

@inproceedings{Xu2025,
  author = {Xu, Keshuai and Wu, Jie Ying and Deguet, Anton and Kazanzides, Peter},
  title = {dVRK-Si: The Next Generation da Vinci Research Kit},
  booktitle = {2025 International Symposium on Medical Robotics (ISMR)},
  year = {2025},
  volume = {},
  number = {},
  pages = {185-191},
  doi = {10.1109/ISMR67322.2025.11025986},
  ieeexplore = {https://ieeexplore.ieee.org/document/11025986},
  semanticscholar = {https://www.semanticscholar.org/paper/7db7f0ed9019634e439e2727debffb70105c3b28},
  research_field = {HW},
  data_type = {},
  dvrk_site = {JHU},
  abstract = {The da Vinci Research Kit (dVRK) is an opensource and open-hardware surgical robotics research platform that provides complete access to all levels of control on decommissioned da Vinci Surgical Systems. The original dVRK, released to the research community in 2012 and now installed at more than 40 institutions worldwide, is based on the first-generation da Vinci system released in 2000. In this work, we present dVRK-Si, an update that extends support to decommissioned second/third generation da Vinci S/Si patientside robots. We describe extensions to the system architecture that maintain compatibility and interoperability with the original dVRK system, thereby enabling a heterogeneous mix of dVRK and dVRK-Si arms in the same system. Additionally, we describe the following components specific to the dVRK-Si: (1) a new controller that features 10 channels of PWM motor drivers with digital current loops; (2) alternative (closed-source) firmwares for embedded devices in the robot arm that stream serialized sensor data using an open protocol; and (3) electronics to support the passive Setup Joints included in full systems. All electronics designs, software, and firmware (except as noted above) are provided open source to the community.},
  keywords = {Medical robotics;Systems architecture;Arms;Pulse width modulation;Robot sensing systems;Motors;Manipulators;Software;Interoperability;Microprogramming},
}

@article{Thompson2025EarlyFD,
  author = {Jordan Thompson and Ronald Koe and Anthony Le and Gabriella Goodman and Daniel S. Brown and Alan Kuntz},
  title = {Early Failure Detection in Autonomous Surgical Soft-Tissue Manipulation via Uncertainty Quantification},
  journal = {ArXiv},
  booktitle = {arXiv.org},
  year = {2025},
  volume = {abs/2501.10561},
  doi = {10.48550/arXiv.2501.10561},
  semanticscholar = {https://www.semanticscholar.org/paper/a2788aea43e6e09c3b414f9de90da04581e3a935},
  arxiv = {https://arxiv.org/abs/2501.10561},
  abstract = {Autonomous surgical robots are a promising solution to the increasing demand for surgery amid a shortage of surgeons. Recent work has proposed learning-based approaches for the autonomous manipulation of soft tissue. However, due to variability in tissue geometries and stiffnesses, these methods do not always perform optimally, especially in out-of-distribution settings. We propose, develop, and test the first application of uncertainty quantification to learned surgical soft-tissue manipulation policies as an early identification system for task failures. We analyze two different methods of uncertainty quantification, deep ensembles and Monte Carlo dropout, and find that deep ensembles provide a stronger signal of future task success or failure. We validate our approach using the physical daVinci Research Kit (dVRK) surgical robot to perform physical soft-tissue manipulation. We show that we are able to successfully detect out-of-distribution states leading to task failure and request human intervention when necessary while still enabling autonomous manipulation when possible. Our learned tissue manipulation policy with uncertainty-based early failure detection achieves a zero-shot sim2real performance improvement of 47.5% over the prior state of the art in learned soft-tissue manipulation. We also show that our method generalizes well to new types of tissue as well as to a bimanual soft-tissue manipulation task.},
}

@article{Vuong2025EffectsOW,
  author = {Brian B. Vuong and Josie Davidson and Sangheui Cheon and Kyu-Jin Cho and Allison M. Okamura},
  title = {Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed During a Teleoperated Robotic Surgery Task},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2025},
  volume = {10},
  pages = {12923-12930},
  doi = {10.1109/LRA.2025.3626226},
  semanticscholar = {https://www.semanticscholar.org/paper/b205e96ef096c33a219c4841a79a82bac6699678},
  arxiv = {https://arxiv.org/abs/2507.07327},
  dvrk_site = {SU},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Abdelaal2025ForceAwareAR,
  author = {A. Abdelaal and Jiaying Fang and Tim N. Reinhart and Jacob A. Mejia and Tony Z. Zhao and Jeannette Bohg and Allison M. Okamura},
  title = {Force-Aware Autonomous Robotic Surgery},
  journal = {ArXiv},
  booktitle = {arXiv.org},
  year = {2025},
  volume = {abs/2501.11742},
  doi = {10.48550/arXiv.2501.11742},
  semanticscholar = {https://www.semanticscholar.org/paper/c49610d21632f3948fb11f55ac493be9e2d29d37},
  arxiv = {https://arxiv.org/abs/2501.11742},
  abstract = {This work demonstrates the benefits of using tool-tissue interaction forces in the design of autonomous systems in robot-assisted surgery (RAS). Autonomous systems in surgery must manipulate tissues of different stiffness levels and hence should apply different levels of forces accordingly. We hypothesize that this ability is enabled by using force measurements as input to policies learned from human demonstrations. To test this hypothesis, we use Action-Chunking Transformers (ACT) to train two policies through imitation learning for automated tissue retraction with the da Vinci Research Kit (dVRK). To quantify the effects of using tool-tissue interaction force data, we trained a"no force policy"that uses the vision and robot kinematic data, and compared it to a"force policy"that uses force, vision and robot kinematic data. When tested on a previously seen tissue sample, the force policy is 3 times more successful in autonomously performing the task compared with the no force policy. In addition, the force policy is more gentle with the tissue compared with the no force policy, exerting on average 62% less force on the tissue. When tested on a previously unseen tissue sample, the force policy is 3.5 times more successful in autonomously performing the task, exerting an order of magnitude less forces on the tissue, compared with the no force policy. These results open the door to design force-aware autonomous systems that can meet the surgical guidelines for tissue handling, especially using the newly released RAS systems with force feedback capabilities such as the da Vinci 5.},
}

@article{Spagnulo2025FromPT,
  author = {Roberto Spagnulo and Francesco Marzola and Federica Corso and Giovanni Distefano and Matteo Pescio and Kengo Hayashi and Federica Barontini and Giulio Dagnino and K. Althoefer and Bruno Siciliano and S. Ourselin and Alberto Arezzo},
  title = {From precision to strength: computer vision for suture quality assessment-an ex vivo pilot study.},
  journal = {Surgical endoscopy},
  booktitle = {Surgical Endoscopy},
  year = {2025},
  doi = {10.1038/s41551-023-01018-0},
  semanticscholar = {https://www.semanticscholar.org/paper/a9bddcf7ce391e9e75bf119789ba751ce2545f26},
  abstract = {Laryngeal lesions can be endoscopically detected with high sensitivity and in real time by measuring differences in the polarization signatures between cancerous and healthy tissues. The standard-of-care for the detection of laryngeal pathologies involves distinguishing suspicious lesions from surrounding healthy tissue via contrasts in colour and texture captured by white-light endoscopy. However, the technique is insufficiently sensitive and thus leads to unsatisfactory rates of false negatives. Here we show that laryngeal lesions can be better detected in real time by taking advantage of differences in the light-polarization properties of cancer and healthy tissues. By measuring differences in polarized-light retardance and depolarization, the technique, which we named ‘surgical polarimetric endoscopy’ (SPE), generates about one-order-of-magnitude greater contrast than white-light endoscopy, and hence allows for the better discrimination of cancerous lesions, as we show with patients diagnosed with squamous cell carcinoma. Polarimetric imaging of excised and stained slices of laryngeal tissue indicated that changes in the retardance of polarized light can be largely attributed to architectural features of the tissue. We also assessed SPE to aid routine transoral laser surgery for the removal of a cancerous lesion, indicating that SPE can complement white-light endoscopy for the detection of laryngeal cancer.},
}

@article{Zhou2025GravityCO,
  author = {Haoying Zhou and Hao Yang and A. Deguet and L. Fichera and Jie Ying Wu and Peter Kazanzides},
  title = {Gravity Compensation of the dVRK-Si Patient Side Manipulator based on Dynamic Model Identification},
  journal = {ArXiv},
  booktitle = {HSMR 2025},
  year = {2025},
  volume = {abs/2501.19058},
  doi = {10.31256/hsmr25.59},
  semanticscholar = {https://www.semanticscholar.org/paper/1fce8afc1c245f75f17530ef154c7a1d03d6d7ae},
  arxiv = {https://arxiv.org/abs/2506.14467},
  dvrk_site = {JHU and VU and WPI},
  abstract = {Rapid and reliable vascular access is critical in trauma and critical care. Central vascular catheterization enables high-volume resuscitation, hemodynamic monitoring, and advanced interventions like ECMO and REBOA. While peripheral access is common, central access is often necessary but requires specialized ultrasound-guided skills, posing challenges in prehospital settings. The complexity arises from deep target vessels and the precision needed for needle placement. Traditional techniques, like the Seldinger method, demand expertise to avoid complications. Despite its importance, ultrasound-guided central access is underutilized due to limited field expertise. While autonomous needle insertion has been explored for peripheral vessels, only semi-autonomous methods exist for femoral access. This work advances toward full automation, integrating robotic ultrasound for minimally invasive emergency procedures. Our key contribution is the successful femoral vein and artery cannulation in a porcine hemorrhagic shock model.},
}

@article{Zhang2025HybridDR,
  author = {Hanyi Zhang and Kaizhong Deng and Zhaoyang Jacopo Hu and Baoru Huang and Daniel S. Elson},
  title = {Hybrid Deep Reinforcement Learning for Radio Tracer Localisation in Robotic-Assisted Radioguided Surgery},
  journal = {2025 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2025},
  pages = {15465-15471},
  doi = {10.7210/JRSJ.10.552},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
}

@article{meli2025inductive,
  author = {Meli, Daniele and Fiorini, Paolo},
  title = {Inductive learning of robot task knowledge from raw data and online expert feedback},
  journal = {Machine Learning},
  year = {2025},
  volume = {114},
  number = {4},
  pages = {91},
  publisher = {Springer},
  doi = {10.1007/s10994-024-06636-6},
  semanticscholar = {https://www.semanticscholar.org/paper/22b923def342bf1cc007428ffb56f3de9b067613},
  arxiv = {https://arxiv.org/abs/2501.07507},
  research_field = {AU and SS and TR},
  data_type = {RI and KD},
  abstract = {The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios. In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user’s feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.},
}

@article{caccianiga2025open,
  author = {Caccianiga, Guido and Sharon, Yarden and Javot, Bernard and Polikovsky, Senya and Erg{\"u}n, G{\"o}kce and Capobianco, Ivan and Mihaljevic, Andr{\'e} L and Deguet, Anton and Kuchenbecker, Katherine J},
  title = {Open-Source Multi-Viewpoint Surgical Telerobotics},
  journal = {arXiv preprint arXiv:2505.11142},
  year = {2025},
  doi = {10.48550/arXiv.2505.11142},
  url = {https://arxiv.org/abs/2505.11142},
  semanticscholar = {https://www.semanticscholar.org/paper/8d535d0d235ccf27b5015a61242cf47afaa82e87},
  arxiv = {https://arxiv.org/abs/2505.11142},
  abstract = {As robots for minimally invasive surgery (MIS) gradually become more accessible and modular, we believe there is a great opportunity to rethink and expand the visualization and control paradigms that have characterized surgical teleoperation since its inception. We conjecture that introducing one or more additional adjustable viewpoints in the abdominal cavity would not only unlock novel visualization and collaboration strategies for surgeons but also substantially boost the robustness of machine perception toward shared autonomy. Immediate advantages include controlling a second viewpoint and teleoperating surgical tools from a different perspective, which would allow collaborating surgeons to adjust their views independently and still maneuver their robotic instruments intuitively. Furthermore, we believe that capturing synchronized multi-view 3D measurements of the patient's anatomy would unlock advanced scene representations. Accurate real-time intraoperative 3D perception will allow algorithmic assistants to directly control one or more robotic instruments and/or robotic cameras. Toward these goals, we are building a synchronized multi-viewpoint, multi-sensor robotic surgery system by integrating high-performance vision components and upgrading the da Vinci Research Kit control logic. This short paper reports a functional summary of our setup and elaborates on its potential impacts in research and future clinical practice. By fully open-sourcing our system, we will enable the research community to reproduce our setup, improve it, and develop powerful algorithms, effectively boosting clinical translation of cutting-edge research.},
}

@article{Maguire2025RoboticAC,
  author = {G. Maguire and E. Tang and T. Looi and D. Podolsky},
  title = {Robotic Assisted Cleft Palate Repair Using Novel 3 mm Tools: A Reachability and Collision Analysis},
  journal = {IEEE Transactions on Biomedical Engineering},
  booktitle = {IEEE Transactions on Biomedical Engineering},
  year = {2025},
  volume = {72},
  pages = {2085-2094},
  doi = {10.1109/tbme.10},
  semanticscholar = {https://www.semanticscholar.org/paper/ba89509cbe938a3d6e2ac3b17a97fdb52bc5e0d6},
  abstract = {Guest Editors : Miguel Nicolelis, M.D., Ph.D. Professor of Neurobiology, Biomedical Research and Experimental Psychology. Department of Neurobiology, Box 3209, Room 327E Bryan Research Bldg. Duke University Medical Center 101 Research Drive Durham, NC 27710 e-mail: nicoleli@neuro.duke.edu Neils Birbaumer, Ph.D. Professor of Behavioral Neurobiology Faculty of Medicine Inst. Medical Psychology and Behavioral Neurobiology Gartenstr. 29, D-72074, Tübingen Germany e-mail: niels.birbaumer@unituebingen.de Klaus Müeller, Ph.D. University of Potsdam and Fraunhofer Institut FIRST Intelligent Data Analysis Group (IDA) Kekulestr. 7, 12489 Berlin},
}

@article{Haiderbhai2025Sim2RealRC,
  author = {Mustafa Haiderbhai and R. Gondokaryono and Andrew Wu and L. Kahrs},
  title = {Sim2Real Rope Cutting With a Surgical Robot Using Vision-Based Reinforcement Learning},
  journal = {IEEE Transactions on Automation Science and Engineering},
  booktitle = {IEEE Transactions on Automation Science and Engineering},
  year = {2025},
  volume = {22},
  pages = {4354-4365},
  doi = {10.1109/tase.2018.2887129},
  semanticscholar = {https://www.semanticscholar.org/paper/6edf9e85d10f3342aae626dfa68583132834edaf},
  abstract = {IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING is published by the IEEE Robotics and Automation Society. All members of the IEEE are eligible for membership in the Society and will receive this TRANSACTIONS upon payment of the annual Society membership fee of $9.00 plus an annual subscription fee of $50.00. For information on joining, write to the lEEE Service Center at the address below. Member copies of Transactions/Journals are for personal use only.},
}

@article{Long2025,
  author = {Long, Yonghao and Lin, Anran and Kwok, Derek Hang Chun and Zhang, Lin and Yang, Zhenya and Shi, Kejian and Song, Lei and Fu, Jiawei and Lin, Hongbin and Wei, Wang and Chen, Kai and Chu, Xiangyu and Hu, Yang and Yip, Hon Chi and Chiu, Philip Wai Yan and Kazanzides, Peter and Taylor, Russell H. and Liu, Yunhui and Chen, Zihan and Wang, Zerui and Au, Samuel Kwok Wai and Dou, Qi},
  title = {Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery},
  journal = {Science Robotics},
  year = {2025},
  volume = {10},
  number = {104},
  pages = {eadt3093},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/scirobotics.adt3093},
  semanticscholar = {https://www.semanticscholar.org/paper/6de1cfdd5d12a13352d5b0926abd57faf62ef573},
  abstract = {Surgical robots capable of autonomously performing various tasks could enhance efficiency and augment human productivity in addressing clinical needs. Although current solutions have automated specific actions within defined contexts, they are challenging to generalize across diverse environments in general surgery. Embodied intelligence enables general-purpose robot learning with applications for daily tasks, yet its application in the medical domain remains limited. We introduced an open-source surgical embodied intelligence simulator for an interactive environment to develop reinforcement learning methods for minimally invasive surgical robots. Using such embodied artificial intelligence, this study further addresses surgical task automation, enabling zero-shot transfer of simulation-trained policies to real-world scenarios. The proposed method encompasses visual parsing, a perceptual regressor, policy learning, and a visual servoing controller, forming a paradigm that combines the advantages of data-driven policy and classic controller. The visual parsing uses stereo depth estimation and image segmentation with a visual foundation model to handle complex scenes. Experiments demonstrated autonomy in seven game-based skill training tasks on the da Vinci Research Kit, with a proof-of-concept study on haptic-assisted skill training as a practical application. Moreover, we conducted automation of five surgical assistive tasks with the Sentire surgical system on ex vivo animal tissues with various scenes, object sizes, instrument types, and illuminations. The learned policies were also validated in a live-animal trial for three tasks in dynamic in vivo surgical environments. We hope this open-source infrastructure, coupled with a general-purpose learning paradigm, will inspire and facilitate future research on embodied intelligence toward autonomous surgical robots.},
  keywords = {dvrk},
  date = {2025-01-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2025-01-01},
}

@article{Chen2025SurgiPoseES,
  author = {Juo-Tung Chen and Xinhao Chen and Ji Woong Kim and P. M. Scheikl and R. Cha and Axel Krieger},
  title = {SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning},
  journal = {2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
  year = {2025},
  pages = {20912-20919},
  doi = {10.1109/iros47612.2022.9981344},
  semanticscholar = {https://www.semanticscholar.org/paper/dda867de88aff74f70fb9b2de0387297183afea5},
  dvrk_site = {JHU},
  abstract = {The 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) will be held on October 23–27, 2022 in The Kyoto International Conference Center, Kyoto, Japan. The IROS is one of the largest and most impacting robotics research conferences worldwide. It provides an international forum for the international robotics research community to explore the frontier of science and technology in intelligent robots and smart machines. The theme of IROS 2022 is “Embodied AI for a Symbiotic Society’’. In addition to technical sessions and multi-media presentations, IROS conferences also hold panel discussions, forums, workshops, tutorials, exhibits, and technical tours to enrich the fruitful discussions among conference attendees.},
}

@article{Ho2025SurgIRLTL,
  author = {Yun-Jie Ho and Zih-Yun Chiu and Yuheng Zhi and Michael C. Yip},
  title = {SurgIRL: Toward Life-Long Learning for Surgical Automation by Incremental Reinforcement Learning},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2025},
  volume = {10},
  pages = {13145-13152},
  doi = {10.1109/LRA.2025.3627088},
  semanticscholar = {https://www.semanticscholar.org/paper/88a3d021d92008fbe1e6b69dd54cd251bc18a5e6},
  dvrk_site = {UCSD},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@inproceedings{Wu2025ICRA,
  author = {Wu, Zijian and Schmidt, Adam and Moore, Randy and Zhou, Haoying and Banks, Alexandre and Kazanzides, Peter and Salcudean, Septimiu E.},
  title = {SurgPose: a Dataset for Articulated Robotic Surgical Tool Pose Estimation and Tracking},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2025},
  doi = {10.1109/ICRA55743.2025.11127958},
  semanticscholar = {https://www.semanticscholar.org/paper/cdbf4e7c0d4e55cfcf89f3c3264f2ff4d3438553},
  arxiv = {https://arxiv.org/abs/2502.11534},
  abstract = {Accurate and efficient surgical robotic tool pose estimation is of fundamental significance to downstream applications such as augmented reality (AR) in surgical training and learning-based autonomous manipulation. While significant advancements have been made in pose estimation for humans and animals, it is still a challenge in surgical robotics due to the scarcity of published data. The relatively large absolute error of the da Vinci end effector kinematics and arduous calibration procedure make calibrated kinematics data collection expensive. Driven by this limitation, we collected a dataset, dubbed SurgPose, providing instance-aware semantic keypoints for visual surgical tool pose estimation and tracking. By marking keypoints using ultraviolet (UV) reactive paint, which is invisible under white light and fluorescent under UV light, we execute the same trajectory under different lighting conditions to collect raw videos and keypoint annotations, respectively. The SurgPose dataset consists of approximately 120 K surgical instrument instances of 6 categories as shown in Fig. 1. Since the videos are collected in stereo pairs, the 2D pose can be lifted to 3D based on stereo-matching depth. In addition to releasing the dataset, we tested a few baseline approaches to surgical instrument tracking to demonstrate the utility of SurgPose. More details can be found at surgpose.github.io.},
  keywords = {dvrk},
  address = {Atlanta, US},
  date = {2025-05-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2025-05-01},
}

@article{Haworth2025SutureBotAP,
  author = {Jesse Haworth and Juo-Tung Chen and Nigel Nelson and Ji Woong Kim and Masoud Moghani and Chelsea Finn and Axel Krieger},
  title = {SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing},
  journal = {ArXiv},
  booktitle = {arXiv.org},
  year = {2025},
  volume = {abs/2510.20965},
  doi = {10.48550/arXiv.2510.20965},
  semanticscholar = {https://www.semanticscholar.org/paper/629bd3a531ee2c823a3216eea32f3d66008353d1},
  arxiv = {https://arxiv.org/abs/2510.20965},
  abstract = {Robotic suturing is a prototypical long-horizon dexterous manipulation task, requiring coordinated needle grasping, precise tissue penetration, and secure knot tying. Despite numerous efforts toward end-to-end autonomy, a fully autonomous suturing pipeline has yet to be demonstrated on physical hardware. We introduce SutureBot: an autonomous suturing benchmark on the da Vinci Research Kit (dVRK), spanning needle pickup, tissue insertion, and knot tying. To ensure repeatability, we release a high-fidelity dataset comprising 1,890 suturing demonstrations. Furthermore, we propose a goal-conditioned framework that explicitly optimizes insertion-point precision, improving targeting accuracy by 59\%-74\% over a task-only baseline. To establish this task as a benchmark for dexterous imitation learning, we evaluate state-of-the-art vision-language-action (VLA) models, including $\pi_0$, GR00T N1, OpenVLA-OFT, and multitask ACT, each augmented with a high-level task-prediction policy. Autonomous suturing is a key milestone toward achieving robotic autonomy in surgery. These contributions support reproducible evaluation and development of precision-focused, long-horizon dexterous manipulation policies necessary for end-to-end suturing. Dataset is available at: https://huggingface.co/datasets/jchen396/suturebot},
}

@misc{Yang2024,
  author = {Yang, Hao and Zhou, Haoying and Fischer, Gregory S. and Wu, Jie Ying},
  title = {A Hybrid Model and Learning-Based Force Estimation Framework for Surgical Robots},
  year = {2024},
  doi = {10.1109/IROS58592.2024.10802648},
  url = {https://arxiv.org/abs/2409.19970},
  semanticscholar = {https://www.semanticscholar.org/paper/8afee642d1c86d1ec92e0f495210eae6d69085cb},
  arxiv = {https://arxiv.org/abs/2409.19970},
  abstract = {Haptic feedback to the surgeon during robotic surgery would enable safer and more immersive surgeries but estimating tissue interaction forces at the tips of robotically controlled surgical instruments has proven challenging. Few existing surgical robots can measure interaction forces directly and the additional sensor may limit the life of instruments. We present a hybrid model and learning-based framework for force estimation for the Patient Side Manipulators (PSM) of a da Vinci Research Kit (dVRK). The model-based component identifies the dynamic parameters of the robot and estimates free-space joint torque, while the learning-based component compensates for environmental factors, such as the additional torque caused by trocar interaction between the PSM instrument and the patient’s body wall. We evaluate our method in an abdominal phantom and achieve an error in force estimation of under 10% normalized root-mean-squared error. We show that by using a model-based method to perform dynamics identification, we reduce reliance on the training data covering the entire workspace. Although originally developed for the dVRK, the proposed method is a generalizable framework for other compliant surgical robots. The code is available at https://github.com/vu-maple-lab/dvrk_force_estimation.},
  archiveprefix = {arXiv},
  eprint = {2409.19970},
  primaryclass = {cs.RO},
}

@inproceedings{Ou2024,
  author = {Ou, Yafei and Zargarzadeh, Sadra and Sedighi, Paniz and Tavakoli, Mahdi},
  title = {A Realistic Surgical Simulator for Non-Rigid and Contact-Rich Manipulation in Surgeries with the da Vinci Research Kit},
  booktitle = {2024 21st International Conference on Ubiquitous Robots (UR)},
  year = {2024},
  pages = {64–70},
  month = {June},
  publisher = {IEEE},
  doi = {10.1109/UR61395.2024.10597513},
  semanticscholar = {https://www.semanticscholar.org/paper/2610b7ad97c66a380a4bb4934bfd46f1aa4812f6},
  arxiv = {https://arxiv.org/abs/2404.05888},
  research_field = {SS},
  abstract = {Realistic real-time surgical simulators play an increasingly important role in surgical robotics research, such as surgical robot learning and automation, and surgical skills assessment. Although there are a number of existing surgical simulators for research, they generally lack the ability to simulate the diverse types of objects and contact-rich manipulation tasks typically present in surgeries, such as tissue cutting and blood suction. In this work, we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the da Vinci Research Kit (dVRK) that enables simulating various contact-rich surgical tasks involving different surgical instruments, soft tissue, and body fluids. The real-world dVRK console and the master tool manipulator (MTM) robots are incorporated into the system to allow for teleoperation through virtual reality (VR). To showcase the advantages and potentials of the simulator, we present three examples of surgical tasks, including tissue grasping and deformation, blood suction, and tissue cutting. These tasks are performed using the simulated surgical instruments, including the large needle driver, suction irrigator, and curved scissor, through VR-based teleoperation.},
}

@article{Strohmeyer2024ASD,
  author = {Nicholas A. Strohmeyer and Ji Hwan Park and Braden P. Murphy and Farshid Alambeigi},
  title = {A Semi-Autonomous Data-Driven Shared Control Framework for Robotic Manipulation and Cutting of an Unknown Deformable Tissue},
  journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2024},
  pages = {9881-9886},
}

@article{Zheng2024AUS,
  author = {Haoyi Zheng and Zhaoyang Jacopo Hu and Yanpei Huang and Xiaoxiao Cheng and Ziwei Wang and Etienne Burdet},
  title = {A User-Centered Shared Control Scheme with Learning from Demonstration for Robotic Surgery},
  journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2024},
  pages = {15195-15201},
}

@article{Yang2024AnES,
  author = {Hao Yang and Ayberk Acar and Keshuai Xu and A. Deguet and P. Kazanzides and Jie Ying Wu},
  title = {An Effectiveness Study Across Baseline and Neural Network-based Force Estimation Methods on the da Vinci Research Kit Si System},
  journal = {ArXiv},
  booktitle = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
  year = {2024},
  volume = {abs/2405.07453},
  doi = {10.31256/hsmr2024.50},
  semanticscholar = {https://www.semanticscholar.org/paper/e16eceb7d574d504c31e13babe85f516455cd195},
  abstract = {Conventional endoscopic instruments restricted by their bulky size, lack the dexterity and intuitiveness required for complex therapeutic interventions. To address this shortfall, our research introduces an innovative endo- scopic robotic system featuring through-the-scope dex- terous instruments coupled with a handwriting-inspired human-robot interface. This novel system is designed to improve surgical precision, intuitiveness, and dexterity, potentially revolutionizing the capabilities of therapeutic endoscopic procedures.},
}

@article{Zargarzadeh2024AugmentedRT,
  author = {Sadra Zargarzadeh and August Sieben and Ericka Wiebe and L. Peiris and Mahdi Tavakoli},
  title = {Augmented Reality-Based Tumor Localization and Visualization for Robot-Assisted Breast Surgeries},
  journal = {2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS)},
  booktitle = {International Conferences on Human-Machine Systems},
  year = {2024},
  pages = {1-6},
  doi = {10.1109/ICHMS65439.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4a1dd22eb950621d85ec5acebb9aa85cefae2e33},
}

@article{Wu2024AugmentingER,
  author = {Zijian Wu and Adam Schmidt and P. Kazanzides and S. Salcudean},
  title = {Augmenting efficient real‐time surgical instrument segmentation in video with point tracking and Segment Anything},
  journal = {Healthcare Technology Letters},
  booktitle = {Healthcare technology letters},
  year = {2024},
  volume = {12},
  doi = {10.1049/htl.2019.0091},
  semanticscholar = {https://www.semanticscholar.org/paper/74d76b6da473e6f140de87e1a56d3065d211491a},
  abstract = {In breast reconstruction following a single mastectomy, the surgeon needs to choose between tens of available implants to find the one that can reproduce the symmetry of the patient's breasts. However, due to the lack of measurement tools this decision is made purely visually, which means the surgeon has to order multiple implants to confirm the size for every single patient. In this Letter, the authors present an augmented reality application, which enables surgeons to see the shape of the implants, as 3D holograms on the patient's body. They custom developed a two-chamber implant that can gain different shapes and be used to test the system. Furthermore, the system was tested in a user study with 13 subjects. The study showed that subjects were able to do a comparison between real and holographic implants and come to a decision about which should be used. This method can be quicker than the traditional way and eliminates sizer implants from the process. Further advantages of the method include the use of a more accurate, user-friendly device, which is easily extendable as new implants that are on the market can be easily added to the system dataset.},
}

@article{Yang2024AutomatedTG,
  author = {Ziqi Yang and Ruiyang Zhang and Junhong Chen and Xuhui Zhou and Yunxiao Ren and Ziyue Tong and Benny Lo},
  title = {Automated Trajectory Generation for Robotic Surgical Tasks},
  journal = {2024 International Conference on Advanced Robotics and Mechatronics (ICARM)},
  booktitle = {International Conference on Advanced Robotics and Mechatronics},
  year = {2024},
  pages = {39-44},
  doi = {10.1109/ICARM62033.2024.10715872},
  semanticscholar = {https://www.semanticscholar.org/paper/c4072a7010f7f96f447658736af6ebb441748c54},
  abstract = {The study aims to develop an emotional logic engine based on a large language model (LLM), providing emotional connections, personalized interactions, knowledge representation, and logical inference. Using this emotional logic engine, we intend to realize the goals of high-level cognition, autonomous knowledge reasoning, long-horizon planning, and action execution described in embodied artificial intelligence (embodied AI). Ultimately, we will implement an efficient intelligent companion interaction robot (ICIR) based on a novel human-robot interaction (HRI) framework to enhance the interaction between humans and robots. The proposed framework integrates multiple components including a visual language model (VLM), logic reasoning model, pre-trained database integration, and the development of a multi-modal template. Additionally, we introduce a complementary framework termed perception-action loop (PALoop), which is meticulously modeled and constructed to facilitate seamless interactions between human operators and robotic systems. Detailed design aspects of both frameworks are elucidated, providing insights into their architecture and functionality. The research outcomes will be practically applied, offering the robotics industry innovative and practical technology solutions.},
}

@article{Cui2024CalibrationFF,
  author = {Zejian Cui and F. R. Y. Baena},
  title = {Calibration Framework for Positioning Accuracy Improvement of da Vinci Surgical Instruments},
  journal = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
  booktitle = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
  year = {2024},
  doi = {10.31256/hsmr2024.50},
  semanticscholar = {https://www.semanticscholar.org/paper/e16eceb7d574d504c31e13babe85f516455cd195},
  abstract = {Conventional endoscopic instruments restricted by their bulky size, lack the dexterity and intuitiveness required for complex therapeutic interventions. To address this shortfall, our research introduces an innovative endo- scopic robotic system featuring through-the-scope dex- terous instruments coupled with a handwriting-inspired human-robot interface. This novel system is designed to improve surgical precision, intuitiveness, and dexterity, potentially revolutionizing the capabilities of therapeutic endoscopic procedures.},
}

@article{Soberanis-Mukul2024CognitiveLI,
  author = {Roger D. Soberanis-Mukul and Paola Ruiz Puentes and Ayberk Acar and Iris Gupta and Joyraj Bhowmick and Yizhou Li and Ahmed Ghazi and Jie Ying Wu and Mathias Unberath},
  title = {Cognitive load in tele-robotic surgery: a comparison of eye tracker designs},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  booktitle = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2024},
  volume = {19},
  pages = {1281 - 1284},
  doi = {10.1007/s11548-024-03304-x},
  semanticscholar = {https://www.semanticscholar.org/paper/eb831fb9983aa01133923fb47c970036e066ea8f},
}

@article{Walder2024CostEfficientAO,
  author = {Alexander Walder and Simon Winkler and Yeongmi Kim},
  title = {Cost-Efficient and Open-Source Desktop Teleoperated Surgical Training System},
  journal = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
  booktitle = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
  year = {2024},
  doi = {10.31256/hsmr2024.50},
  semanticscholar = {https://www.semanticscholar.org/paper/e16eceb7d574d504c31e13babe85f516455cd195},
  abstract = {Conventional endoscopic instruments restricted by their bulky size, lack the dexterity and intuitiveness required for complex therapeutic interventions. To address this shortfall, our research introduces an innovative endo- scopic robotic system featuring through-the-scope dex- terous instruments coupled with a handwriting-inspired human-robot interface. This novel system is designed to improve surgical precision, intuitiveness, and dexterity, potentially revolutionizing the capabilities of therapeutic endoscopic procedures.},
}

@article{Argin2024daVinciRK,
  author = {O. F. Argin and R. Moccia and Cristina Iacono and Fanny Ficuciello},
  title = {daVinci Research Kit Patient Side Manipulator Dynamic Model Using Augmented Lagrangian Particle Swarm Optimization},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  booktitle = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2024},
  volume = {6},
  pages = {589-599},
  doi = {10.1109/TMRB.2024.3387070},
  semanticscholar = {https://www.semanticscholar.org/paper/615f4c9955dc2ede7d74bb4a0c8c40e12af993ff},
  abstract = {In surgical robotics, accurate characterization of the dynamic model is crucial. It serves as a foundation for developing robust control algorithms that effectively handle the complex dynamics of the robot and its interactions with the environment. Additionally, an accurate dynamic model aids in estimating external forces and disturbances, enhancing the safety and stability of the control. Among surgical robots, the da Vinci Research Kit (dVRK) is one of the most used, and it has been a crucial instrument in removing a barrier to entry for new research groups in surgical robotics by facilitating the development of improved control algorithms. This paper presents a method for dynamic model identification of the dVRK *psm robot that employs a novel friction model definition. The model formulation has been modified by including the Stribeck effect at low velocities, and the friction has been estimated using the superposition method. The dynamic parameters are identified utilizing a restricted optimization method with physical consistency requirements in an Augmented Lagrangian Particle Swarm Algorithm (ALPSO) methodology. The identified model is thoroughly evaluated, and the results are compared with existing literature methods. Also, a model-based sensorless force estimation method was used to test the dynamic model.},
}

@article{Iacono2024DesignAV,
  author = {Cristina Iacono and Marco Caianiello and Serena Bartiromo and Aldo Smaldone and Fanny Ficuciello},
  title = {Design and Validation of a Multimodal Dataset of Robot-Assisted Suturing Gestures based on Kinematic and Force Information},
  journal = {2024 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO)},
  booktitle = {Workshop on Advanced Robotics and its Social Impacts},
  year = {2024},
  pages = {98-103},
  doi = {10.1109/ARSO60199.2024.10557814},
  semanticscholar = {https://www.semanticscholar.org/paper/b68489d2a260239539fe6e6f16558601def065bb},
  abstract = {Healthy ageing is a challenge in societies that can be coped with the help of socially assistive robots. This study introduces CelesTE, a theomorphic device designed to support the well-being of older adults. Building upon the foundations laid by SanTO, the Catholic robot, CelesTE takes the form of an angel in prayer and aims to engage users, particularly those of the Christian Catholic faith. The paper delves into CelesTE’s conceptual evolution, addressing challenges related to religious perceptions, fallibility, and user interaction. Quantitative and qualitative feedback was collected from 14 participants across three European countries. Results indicate generally positive acceptance, although limitation were found. Negative responses are considered particularly valuable for CelesTE’s future development.},
}

@article{Yang2024EfficientPS,
  author = {Zhenya Yang and Yonghao Long and Kai Chen and Wang Wei and Qi Dou},
  title = {Efficient Physically-based Simulation of Soft Bodies in Embodied Environment for Surgical Robot},
  journal = {ArXiv},
  booktitle = {arXiv.org},
  year = {2024},
  volume = {abs/2402.01181},
  doi = {10.48550/arXiv.2402.01181},
  semanticscholar = {https://www.semanticscholar.org/paper/94734477b0a642dea4482869e400d1528cfaef08},
  arxiv = {https://arxiv.org/abs/2402.01181},
  abstract = {Surgical robot simulation platform plays a crucial role in enhancing training efficiency and advancing research on robot learning. Much effort have been made by scholars on developing open-sourced surgical robot simulators to facilitate research. We also developed SurRoL formerly, an open-source, da Vinci Research Kit (dVRK) compatible and interactive embodied environment for robot learning. Despite its advancements, the simulation of soft bodies still remained a major challenge within the open-source platforms available for surgical robotics. To this end, we develop an interactive physically based soft body simulation framework and integrate it to SurRoL. Specifically, we utilized a high-performance adaptation of the Material Point Method (MPM) along with the Neo-Hookean model to represent the deformable tissue. Lagrangian particles are used to track the motion and deformation of the soft body throughout the simulation and Eulerian grids are leveraged to discretize space and facilitate the calculation of forces, velocities, and other physical quantities. We also employed an efficient collision detection and handling strategy to simulate the interaction between soft body and rigid tool of the surgical robot. By employing the Taichi programming language, our implementation harnesses parallel computing to boost simulation speed. Experimental results show that our platform is able to simulate soft bodies efficiently with strong physical interpretability and plausible visual effects. These new features in SurRoL enable the efficient simulation of surgical tasks involving soft tissue manipulation and pave the path for further investigation of surgical robot learning. The code will be released in a new branch of SurRoL github repo.},
}

@article{Yilmaz2024IJCARS,
  author = {Yilmaz, Nural and Burkhart, Brendan and Deguet, Anton and Kazanzides, Peter and Tumerdem, Ugur},
  title = {Enhancing robotic telesurgery with sensorless haptic feedback},
  journal = {Intl. Journal of Computer Assisted Radiology and Surgery (IJCARS)},
  year = {2024},
  volume = {19},
  number = {6},
  pages = {1147-1155},
  publisher = {Springer},
  doi = {10.1007/s11548-024-03117-y},
  semanticscholar = {https://www.semanticscholar.org/paper/aa27ac609cec644d0761cfcf1e3cfc10c315da89},
  dvrk_site = {JHU},
  keywords = {dvrk},
  date = {2024-06-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2024-06-01},
}

@inproceedings{Rajarajeswari2024,
  author = {Rajarajeswari, S and S, Pruthvi Darshan S and L, Jayanth M and Lohar, Hrishikesh Keraba},
  title = {Enhancing Surgical Training Through Immersive Simulation and Visualization of the da Vinci Research Kit (dVRK) Surgical System Using ROS 2 and Rviz},
  booktitle = {2024 2nd International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)},
  year = {2024},
  volume = {},
  number = {},
  pages = {118-123},
  doi = {10.1109/ICRAIS62903.2024.10811720},
  ieeexplore = {https://ieeexplore.ieee.org/document/10811720},
  semanticscholar = {https://www.semanticscholar.org/paper/60646681890889589c02e631068d6f5e28fe1eda},
  abstract = {In the modern era, robotic engineering has transformed education, training, and skill development, particularly in the field of surgery. Simulation training offers a safe and controlled environment for surgeons to learn, practice and visualize surgical techniques before performing procedures on patients. The simulation and visualization of the da Vinci Research Kit (dVRK), a platform for creating and evaluating surgical methods, algorithms, and technologies, are the main focus of this research, visualizing the interaction between a dVRK surgical system and a virtual human body using the Rviz simulator and Robot Operating System (ROS). These models are imported as meshes of stereo-lithography format (.STL), defined using xacro. The study explores the role of visualization in enhancing surgical education and training, highlighting it potential to improve procedural competency and patient outcomes. This research contributes to the advancement of surgical education through visualization by providing a realistic and immersive environment for surgical simulation training.},
  keywords = {Training;Visualization;Technological innovation;Medical robotics;Operating systems;Instruments;Surgery;Virtual environments;Maintenance;Sustainable development;dVRK;ROS2;Rviz;Simulation;Visualization},
}

@article{Oh2024ExpandedCR,
  author = {Ki Hwan Oh and Leonardo Borgioli and Alberto Mangano and Valentina Valle and Marco Di Pangrazio and F. Toti and Gioia Pozza and Luciano Ambrosini and A. Ducas and Milos Zefran and Liaohai Chen and P. Giulianotti},
  title = {Expanded Comprehensive Robotic Cholecystectomy Dataset (CRCD)},
  journal = {ArXiv},
  booktitle = {arXiv.org},
  year = {2024},
  volume = {abs/2412.12238},
  doi = {10.48550/arXiv.2412.12238},
  semanticscholar = {https://www.semanticscholar.org/paper/e7ee00483b2a1437108b0fb7faa67af2b4a33ed2},
  arxiv = {https://arxiv.org/abs/2412.12238},
  abstract = {In recent years, the application of machine learning to minimally invasive surgery (MIS) has attracted considerable interest. Datasets are critical to the use of such techniques. This paper presents a unique dataset recorded during ex vivo pseudo-cholecystectomy procedures on pig livers using the da Vinci Research Kit (dVRK). Unlike existing datasets, it addresses a critical gap by providing comprehensive kinematic data, recordings of all pedal inputs, and offers a time-stamped record of the endoscope's movements. This expanded version also includes segmentation and keypoint annotations of images, enhancing its utility for computer vision applications. Contributed by seven surgeons with varied backgrounds and experience levels that are provided as a part of this expanded version, the dataset is an important new resource for surgical robotics research. It enables the development of advanced methods for evaluating surgeon skills, tools for providing better context awareness, and automation of surgical tasks. Our work overcomes the limitations of incomplete recordings and imprecise kinematic data found in other datasets. To demonstrate the potential of the dataset for advancing automation in surgical robotics, we introduce two models that predict clutch usage and camera activation, a 3D scene reconstruction example, and the results from our keypoint and segmentation models.},
}

@article{Li2024GMMBasedHD,
  author = {Bin Li and Yiang Lu and Wei Chen and Bo Lu and Fangxun Zhong and Qi Dou and Yunhui Liu},
  title = {GMM-Based Heuristic Decision Framework for Safe Automated Laparoscope Control},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2024},
  volume = {9},
  pages = {1969-1976},
  doi = {10.1109/LRA.2024.3352308},
  semanticscholar = {https://www.semanticscholar.org/paper/75bfd775108676661dd2c1051d2f0569775f4796},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{oquendo2024haptic,
  author = {Oquendo, Yousi A. and Coad, Margaret M. and Wren, Sherry M. and Lendvay, Thomas S. and Nisky, Ilana and Jarc, Anthony M. and Okamura, Allison M. and Chua, Zonghe},
  title = {Haptic Guidance and Haptic Error Amplification in a Virtual Surgical Robotic Training Environment},
  journal = {IEEE Transactions on Haptics},
  year = {2024},
  volume = {17},
  number = {3},
  pages = {417-428},
  publisher = {IEEE},
  doi = {10.1109/TOH.2024.3350128},
  semanticscholar = {https://www.semanticscholar.org/paper/4f2b40f626bd30197879bf945f9a33206d54245c},
  arxiv = {https://arxiv.org/abs/2309.05187},
  research_field = {TR and SS},
  data_type = {KD},
  abstract = {Teleoperated robotic systems have introduced more intuitive control for minimally invasive surgery, but the optimal method for training remains unknown. Recent motor learning studies have demonstrated that exaggeration of errors helps trainees learn to perform tasks with greater speed and accuracy. We hypothesized that training in a force field that pushes the user away from a desired path would improve their performance on a virtual reality ring-on-wire task. Thirty-eight surgical novices trained under a no-force, guidance, or error-amplifying force field over five days. Completion time, translational and rotational path error, and combined error-time were evaluated under no force field on the final day. The groups significantly differed in combined error-time, with the guidance group performing the worst. Error-amplifying field participants did not plateau in their performance during training, suggesting that learning was still ongoing. Guidance field participants had the worst performance on the final day, confirming the guidance hypothesis. Observed trends also suggested that participants who had high initial path error benefited more from guidance. Error-amplifying and error-reducing haptic training for robot-assisted telesurgery benefits trainees of different abilities differently, with our results indicating that participants with high initial combined error-time benefited more from guidance and error-amplifying force field training.},
}

@inproceedings{rota2024implementation,
  author = {Rota, Alberto and Fan, Ke and De Momi, Elena},
  title = {Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics},
  booktitle = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2024},
  pages = {9666--9673},
  doi = {10.1109/ICRA57147.2024.10610411},
  semanticscholar = {https://www.semanticscholar.org/paper/0f19fb8a556ffcfad4c976b97a502b502a5a4352},
  arxiv = {https://arxiv.org/abs/2507.07718},
  research_field = {TR},
  data_type = {KD and SD},
  abstract = {The integration of high-level assistance algorithms in surgical robotics training curricula may be beneficial in establishing a more comprehensive and robust skillset for aspiring surgeons, improving their clinical performance as a consequence. This work presents the development and validation of a haptic-enhanced Virtual Reality simulator for surgical robotics training, featuring 8 surgical tasks that the trainee can interact with thanks to the embedded physics engine. This virtual simulated environment is augmented by the introduction of high-level haptic interfaces for robotic assistance that aim at re-directing the motion of the trainee’s hands and wrists toward targets or away from obstacles, and providing a quantitative performance score after the execution of each training exercise.An experimental study shows that the introduction of enhanced robotic assistance into a surgical robotics training curriculum improves performance during the training process and, crucially, promotes the transfer of the acquired skills to an unassisted surgical scenario, like the clinical one.},
  organization = {IEEE},
}

@inproceedings{Barragan2024ISMR,
  author = {Barragan, Juan Antonio and Ishida, Hisashi and Munawar, Adnan and Kazanzides, Peter},
  title = {Improving the realism of robotic surgery simulation through injection of learning-based estimated errors},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2024},
  doi = {10.1109/ISMR63436.2024.10585672},
  semanticscholar = {https://www.semanticscholar.org/paper/96b906922390e3a1c2171bb54082a412be2f5d5e},
  arxiv = {https://arxiv.org/abs/2406.07375},
  dvrk_site = {JHU},
  abstract = {The development of algorithms for automation of subtasks during robotic surgery can be accelerated by the availability of realistic simulation environments. In this work, we focus on one aspect of the realism of a surgical simulator, which is the positional accuracy of the robot. In current simulators, robots have perfect or near-perfect accuracy, which is not representative of their physical counterparts. We therefore propose a pair of neural networks, trained by data collected from a physical robot, to estimate both the controller error and the kinematic and non-kinematic error. These error estimates are then injected within the simulator to produce a simulated robot that has the characteristic performance of the physical robot. In this scenario, we believe it is sufficient for the estimated error used in the simulation to have a statistically similar distribution to the actual error of the physical robot. This is less stringent, and therefore more tenable, than the requirement for error compensation of a physical robot, where the estimated error should equal the actual error. Our results demonstrate that error injection reduces the mean position and orientation differences between the simulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg, respectively, which represents reductions by factors of 3.8 and 2.1.},
  keywords = {dvrk, machine-learning},
  date = {2024-06-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2024-06-01},
}

@article{Rivas-Blanco2024InstrumentDA,
  author = {I. Rivas-Blanco and C. López-Casado and Juan M. Herrera-López and José Cabrera-Villa and C. Pérez-del-Pulgar},
  title = {Instrument Detection and Descriptive Gesture Segmentation on a Robotic Surgical Maneuvers Dataset},
  journal = {Applied Sciences},
  booktitle = {Applied Sciences},
  year = {2024},
  doi = {10.1109/ICRA40945.2020.9197436},
  semanticscholar = {https://www.semanticscholar.org/paper/be31b693f95dbd3a12b4cb3c706c5f40eb4cb9c9},
  arxiv = {https://arxiv.org/abs/2003.03337},
  abstract = {Here we present HAMR-Jr, a 22.5mm, 320mg quadrupedal microrobot. With eight independently actuated degrees of freedom, HAMR-Jr is, to our knowledge, the most mechanically dexterous legged robot at its scale and is capable of high-speed locomotion (13.91bodylengthss−1) at a variety of stride frequencies (1-200Hz) using multiple gaits. We achieved this using a design and fabrication process that is flexible, allowing scaling with minimum changes to our workflow. We further characterized HAMR-Jr’s open-loop locomotion and compared it with the larger scale HAMR-VI microrobot to demonstrate the effectiveness of scaling laws in predicting running performance.},
}

@inproceedings{kim2024learning,
  author = {Kim, Ji Woong and Schmidgall, Samuel and Krieger, Axel and Kobilarov, Marin},
  title = {Learning a library of surgical manipulation skills for robotic surgery},
  booktitle = {Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions},
  year = {2024},
  url = {https://openreview.net/forum?id=fYRlaylCI3},
  semanticscholar = {https://www.semanticscholar.org/paper/3f19cceee65ea1cd94cf2e48db7b6dacee4d632d},
  dvrk_site = {JHU},
}

@article{Ai2024,
  author = {Ai, Letian and Kazanzides, Peter and Azimi, Ehsan},
  title = {Mixed reality based teleoperation and visualization of surgical robotics},
  journal = {IET Healthcare Technology Letters},
  year = {2024},
  volume = {11},
  number = {2-3},
  pages = {179-188},
  doi = {10.1049/htl2.12079},
  semanticscholar = {https://www.semanticscholar.org/paper/2906091e2708fa206d4db64775508f0a7a1d678e},
  abstract = {Abstract Surgical robotics has revolutionized the field of surgery, facilitating complex procedures in operating rooms. However, the current teleoperation systems often rely on bulky consoles, which limit the mobility of surgeons. This restriction reduces surgeons' awareness of the patient during procedures and narrows the range of implementation scenarios. To address these challenges, an alternative solution is proposed: a mixed reality‐based teleoperation system. This system leverages hand gestures, head motion tracking, and speech commands to enable the teleoperation of surgical robots. The implementation focuses on the da Vinci research kit (dVRK) and utilizes the capabilities of Microsoft HoloLens 2. The system's effectiveness is evaluated through camera navigation tasks and peg transfer tasks. The results indicate that, in comparison to manipulator‐based teleoperation, the system demonstrates comparable viability in endoscope teleoperation. However, it falls short in instrument teleoperation, highlighting the need for further improvements in hand gesture recognition and video display quality.},
  keywords = {dvrk, hmd},
  date = {2024-04-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2024-04-01},
}

@article{Marra2024MPCFS,
  author = {Pasquale Marra and Sajjad Hussain and Marco Caianiello and Fanny Ficuciello},
  title = {MPC for Suturing Stitch Automation},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  booktitle = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2024},
  volume = {6},
  pages = {1468-1477},
  doi = {10.1109/TMRB.2024.3472796},
  semanticscholar = {https://www.semanticscholar.org/paper/394dfcba718cffe80cc84a0f58268bc94b66bbf9},
  abstract = {Robot-assisted surgery (RAS) requires effective control strategies to ensure safety and accuracy while respecting the physical limits of the robot during tasks such as suturing and tissue manipulation. Model Predictive Control (MPC), with its inherent capability to handle complex dynamic systems, predict the future response and enforce constraints, is well-suited for these tasks. In this paper, MPC is employed to automate the suturing stitch task by mapping the operational space trajectory to the joint space while ensuring compliance with system kinematics constraints and safety requirements. To address varying requirements during suturing sub-tasks, two different objective functions and their corresponding constraint sets are used. The proposed framework is implemented using the ACADO toolkit to solve the Optimal Control Problem (OCP) and ROS to connect ACADO to CoppeliaSim/DVRK. Validation through simulations in CoppeliaSim and real-time experiments on the DVRK demonstrated that our approach achieved a positional/orientational accuracy of less than <inline-formula> <tex-math notation="LaTeX">$1mm/4 ^\{\circ \}$ </tex-math></inline-formula> in simulations, and an error norm of approximately <inline-formula> <tex-math notation="LaTeX">$1.9mm$ </tex-math></inline-formula> in real world implementations, confirming its effectiveness in automating suturing task.},
}

@article{Fu2024MultiobjectiveCL,
  author = {Jiawei Fu and Yonghao Long and Kai Chen and Wang Wei and Qi Dou},
  title = {Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation},
  journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2024},
  pages = {13362-13368},
}

@article{Li2024OnTM,
  author = {Bin Li and Bo Lu and Hongbin Lin and Yaxiang Wang and Fangxun Zhong and Qi Dou and Yun-hui Liu},
  title = {On the Monocular 3-D Pose Estimation for Arbitrary Shaped Needle in Dynamic Scenes: An Efficient Visual Learning and Geometry Modeling Approach},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  booktitle = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2024},
  volume = {6},
  pages = {460-474},
  doi = {10.1109/TMRB.2024.3377357},
  semanticscholar = {https://www.semanticscholar.org/paper/793fa1931d0b9af90c1c62ad6f7902a587bb6549},
  abstract = {Image-guided needle pose estimation is crucial for robotic autonomous suturing, but it poses significant challenges due to the needle’s slender visual projection and dynamic surgical environments. Current state-of-the-art methods rely on additional prior information (e.g., in-hand grasp, accurate kinematics, etc.) to achieve sub-millimeter accuracy, hindering their applicability in varying surgical scenes. This paper presents a new generic framework for monocular needle pose estimation: Visual learning network for efficient geometric primitives extraction and novel geometry model for accurate pose recovery. To capture needle’s primitives precisely, we introduce a morphology-based mask contour fusion mechanism in a multi-scale manner. We then establish a novel state representation for needle pose and develop a physical projection model to derive its relationship with the primitives. An anti-occlusion objective is formulated to jointly optimize the pose and bias of inference primitives, achieving sub-millimeter accuracy under occlusion scenarios. Our approach requires neither CAD model nor circular shape assumption and can extensively estimate poses of other small planar axisymmetric objects. Experiments on ex-/in-vivo scenarios validate the accuracy of estimated intermediate primitives and final poses of needles. We further deploy our framework on the dVRK platform for automatic and precise needle manipulations, demonstrating the feasibility for use in robotic surgery.},
}

@inproceedings{yu2024orbit,
  author = {Yu, Qinxi and Moghani, Masoud and Dharmarajan, Karthik and Schorp, Vincent and Panitch, William Chung-Ho and Liu, Jingzhou and Hari, Kush and Huang, Huang and Mittal, Mayank and Goldberg, Ken and others},
  title = {Orbit-surgical: An open-simulation framework for learning surgical augmented dexterity},
  booktitle = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2024},
  pages = {15509--15516},
  doi = {10.1109/ICRA57147.2024.10611637},
  ieeexplore = {https://ieeexplore.ieee.org/abstract/document/10611637},
  semanticscholar = {https://www.semanticscholar.org/paper/b561a0fd92149ff0b3beb82d9112801f6df2f487},
  arxiv = {https://arxiv.org/abs/2404.16027},
  abstract = {Physics-based simulations have accelerated progress in robot learning for driving, manipulation, and locomotion. Yet, a fast, accurate, and robust surgical simulation environment remains a challenge. In this paper, we present Orbit-Surgical, a physics-based surgical robot simulation framework with photorealistic rendering in NVIDIA Omniverse. We provide 14 benchmark surgical tasks for the da Vinci Research Kit (dVRK) and Smart Tissue Autonomous Robot (STAR) which represent common subtasks in surgical training. Orbit-Surgical leverages GPU parallelization to train reinforcement learning and imitation learning algorithms to facilitate study of robot learning to augment human surgical skills. Orbit-Surgical also facilitates realistic synthetic data generation for active perception tasks. We demonstrate Orbit-Surgical sim-to-real transfer of learned policies onto a physical dVRK robot.Project website: orbit-surgical.github.io},
  organization = {IEEE},
}

@inproceedings{rota2024performance,
  author = {Rota, Alberto and Sun, Xianyi Federica and De Momi, Elena},
  title = {Performance-driven tasks with adaptive difficulty for enhanced surgical robotics training},
  booktitle = {2024 10th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)},
  year = {2024},
  pages = {465--470},
  doi = {10.1109/BioRob60516.2024.10719830},
  semanticscholar = {https://www.semanticscholar.org/paper/66e82998beab20dd3a64e5b90e7e155b5bbd1f5d},
  research_field = {TR},
  data_type = {KD},
  abstract = {Surgical robotics training most often occurs through standardized curricula and exercises that lack customization and do not adapt to the different levels of proficiency that trainees often present. This work proposes a Virtual Reality (VR) simulator for surgical robotics that autonomously adjusts difficulty levels based on trainee performance, aiming to enhance skill retention and transfer. The study employs a performance-based adaptive difficulty approach, dynamically adjusting parameters of each task's morphology to match individual proficiency levels. The proposed adaptive simulator is evaluated against a non-adaptive counterpart through a week-long training program. The results demonstrate the effectiveness of the adaptive simulator in enhancing performance at higher difficulty levels, supporting its potential to benefit surgical education by providing a tailored and scalable training approach.},
  organization = {IEEE},
}

@article{Li2024RealTimeGJ,
  author = {Bin Li and Hongbin Lin and Fangxun Zhong and Yunhui Liu},
  title = {Real-Time Geometric Joint Uncertainty Tracking for Surgical Automation on the dVRK System},
  journal = {2024 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
  booktitle = {IEEE International Conference on Robotics and Biomimetics},
  year = {2024},
  pages = {2144-2148},
  semanticscholar = {https://www.semanticscholar.org/paper/7bafb41c9eea715c1aa6b0b0b0fb93a2a1f50b50},
}

@inproceedings{Barragan2024ICRA,
  author = {Barragan, Juan Antonio and Zhang, Jintan and Zhou, Haoying and Munawar, Adnan and Kazanzides, Peter},
  title = {Realistic Data Generation for 6D Pose Estimation of Surgical Instruments},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2024},
  doi = {10.1109/ICRA57147.2024.10611638},
  semanticscholar = {https://www.semanticscholar.org/paper/72bb0b2bd405f1acf6faab4ac11305ace8a286fc},
  arxiv = {https://arxiv.org/abs/2406.07328},
  abstract = {Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions. To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline’s success in training and evaluating novel vision algorithms for surgical robotics applications.},
  keywords = {ambf, dvrk, simulation},
  address = {Yokohama, Japan},
  date = {2024-05-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2024-05-01},
}

@article{YilmazRAL2024,
  author = {Yilmaz, Nural and Burkhart, Brendan and Deguet, Anton and Kazanzides, Peter and Tumerdem, Ugur},
  title = {Sensorless Transparency Optimized Haptic Teleoperation on the da Vinci Research Kit},
  journal = {IEEE Robotics and Automation Letters},
  year = {2024},
  volume = {9},
  number = {2},
  pages = {971-978},
  doi = {10.1109/LRA.2023.3335779},
  semanticscholar = {https://www.semanticscholar.org/paper/6bb0bfb973672681a7712f092236e01d9e9f549f},
  abstract = {The da Vinci surgical robot introduced remote control of instruments, providing surgeons with increased dexterity and precision. A major drawback, however, is the loss of sense of touch due to a lack of kinesthetic coupling between the surgical field and the surgeon. This paper presents a framework for sensorless transparency optimized four channel teleoperation. It is sensorless because forces are estimated from existing actuator feedback, with a deep network for dynamics identification. Performance is further optimized by introducing robust acceleration control, with disturbance observers. Experiments performed on the da Vinci Research Kit (dVRK), an open research platform based on the clinically deployed robotic hardware, show improvements in control, force estimation and reflection. The significance is that we demonstrate that high-performance bilateral teleoperation is feasible in clinical systems, without hardware changes, and is available to the dVRK community through a software update.},
  keywords = {dvrk},
  date = {2024-02-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2024-02-01},
}

@article{Borgioli2024SensoryGS,
  author = {Leonardo Borgioli and Ki Hwan Oh and Alberto Mangano and A. Ducas and Luciano Ambrosini and Federico Pinto and Paula Lopez and Jessica Cassiani and Milos Zefran and Liaohai Chen and P. Giulianotti},
  title = {Sensory Glove-Based Surgical Robot User Interface},
  journal = {2025 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2024},
  pages = {10487-10493},
}

@article{Haiderbhai2024SimulatingSR,
  author = {Mustafa Haiderbhai and L. Kahrs},
  title = {Simulating Surgical Robot Cutting of Thin Deformable Materials Using a Rope Grid Structure},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  booktitle = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2024},
  volume = {6},
  pages = {1401-1404},
  doi = {10.1109/tmrb.2024.3475509},
  semanticscholar = {https://www.semanticscholar.org/paper/e140670f5477abdc874e3e45d15426483e9596c9},
  abstract = {Traditional methods for autonomous cutting in surgical robotics have relied on trajectory-based planning algorithms. These methods fail to compensate for dynamic changes in soft materials such as deformation and topological change. To apply recent advances such as reinforcement learning (RL), a simulation is needed that models the cutting of soft materials. In this work, we develop a surgical robotics simulation environment for cutting deformable meshes with the da Vinci Research Kit (dVRK). Our environment is built using a particle-based physics simulation to simulate a rope grid structure to create realistic physics behavior and visual rendering. Cutting is implemented with the EndoWrist Round Tip Scissors (RTS) through a system of collision checking and callbacks to detect and update cuts. To showcase the deformable mesh cutting simulation, we design a cutting task of cutting along a desired path that can be solved through manual control. The grid structure can be adapted to render different materials, and we highlight how it can be made to resemble deformable tissue or fabric while being stable with no visible artifacts. This environment is a stepping stone towards training autonomous agents for cutting 2D deformable materials and building towards cutting more complex deformable shapes.},
}

@article{Shinde2024SURESTEPAU,
  author = {N. Shinde and Zih-Yun Chiu and Florian Richter and Jason Lim and Yuheng Zhi and Sylvia L. Herbert and Michael C. Yip},
  title = {SURESTEP: An Uncertainty-Aware Trajectory Optimization Framework to Enhance Visual Tool Tracking for Robust Surgical Automation},
  journal = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
  year = {2024},
  pages = {6953-6960},
}

@article{Ho2024SurgIRLTL,
  author = {Yun-Jie Ho and Zih-Yun Chiu and Yuheng Zhi and Michael C. Yip},
  title = {SurgIRL: Towards Life-Long Learning for Surgical Automation by Incremental Reinforcement Learning},
  journal = {ArXiv},
  booktitle = {arXiv.org},
  year = {2024},
  volume = {abs/2409.15651},
  doi = {10.48550/arXiv.2409.15651},
  semanticscholar = {https://www.semanticscholar.org/paper/7d7069ec9bae7a3eb4522cca196c90e378f37067},
  arxiv = {https://arxiv.org/abs/2409.15651},
  dvrk_site = {UCSD},
  abstract = {Surgical automation holds immense potential to improve the outcome and accessibility of surgery. Recent studies use reinforcement learning to learn policies that automate different surgical tasks. However, these policies are developed independently and are limited in their reusability when the task changes, making it more time-consuming when robots learn to solve multiple tasks. Inspired by how human surgeons build their expertise, we train surgical automation policies through Surgical Incremental Reinforcement Learning (SurgIRL). SurgIRL aims to (1) acquire new skills by referring to external policies (knowledge) and (2) accumulate and reuse these skills to solve multiple unseen tasks incrementally (incremental learning). Our SurgIRL framework includes three major components. We first define an expandable knowledge set containing heterogeneous policies that can be helpful for surgical tasks. Then, we propose Knowledge Inclusive Attention Network with mAximum Coverage Exploration (KIAN-ACE), which improves learning efficiency by maximizing the coverage of the knowledge set during the exploration process. Finally, we develop incremental learning pipelines based on KIAN-ACE to accumulate and reuse learned knowledge and solve multiple surgical tasks sequentially. Our simulation experiments show that KIAN-ACE efficiently learns to automate ten surgical tasks separately or incrementally. We also evaluate our learned policies on the da Vinci Research Kit (dVRK) and demonstrate successful sim-to-real transfers.},
}

@article{Cartucho2024SurgT,
  author = {Cartucho, João and Weld, Alistair and Tukra, Samyakh and Xu, Haozheng and Matsuzaki, Hiroki and Ishikawa, Taiyo and Kwon, Minjun and Jang, Yong Eun and Kim, Kwang-Ju and Lee, Gwang and Bai, Bizhe and Kahrs, Lueder A and Boecking, Lars and Allmendinger, Simeon and Müller, Leopold and Zhang, Yitong and Jin, Yueming and Bano, Sophia and Vasconcelos, Francisco and Reiter, Wolfgang and Hajek, Jonas and Silva, Bruno and Lima, Estevão and Vilaça, João L and Queirós, Sandro and Giannarou, Stamatia},
  title = {SurgT challenge: Benchmark of soft-tissue trackers for robotic surgery},
  journal = {Medical Image Analysis},
  year = {2024},
  volume = {91},
  pages = {102985},
  doi = {10.1016/j.media.2023.102985},
  semanticscholar = {https://www.semanticscholar.org/paper/104bd583ee064c9473b62532e4a8142bbefab650},
  arxiv = {https://arxiv.org/abs/2302.03022},
  abstract = {This paper introduces the "SurgT: Surgical Tracking" challenge which was organized in conjunction with the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2022). There were two purposes for the creation of this challenge: (1) the establishment of the first standardized benchmark for the research community to assess soft-tissue trackers; and (2) to encourage the development of unsupervised deep learning methods, given the lack of annotated data in surgery. A dataset of 157 stereo endoscopic videos from 20 clinical cases, along with stereo camera calibration parameters, have been provided. Participants were assigned the task of developing algorithms to track the movement of soft tissues, represented by bounding boxes, in stereo endoscopic videos. At the end of the challenge, the developed methods were assessed on a previously hidden test subset. This assessment uses benchmarking metrics that were purposely developed for this challenge, to verify the efficacy of unsupervised deep learning algorithms in tracking soft-tissue. The metric used for ranking the methods was the Expected Average Overlap (EAO) score, which measures the average overlap between a tracker's and the ground truth bounding boxes. Coming first in the challenge was the deep learning submission by ICVS-2Ai with a superior EAO score of 0.617. This method employs ARFlow to estimate unsupervised dense optical flow from cropped images, using photometric and regularization losses. Second, Jmees with an EAO of 0.583, uses deep learning for surgical tool segmentation on top of a non-deep learning baseline method: CSRT. CSRT by itself scores a similar EAO of 0.563. The results from this challenge show that currently, non-deep learning methods are still competitive. The dataset and benchmarking tool created for this challenge have been made publicly available at https://surgt.grand-challenge.org/. This challenge is expected to contribute to the development of autonomous robotic surgery and other digital surgical technologies.},
  archiveprefix = {arXiv},
  eprint = {2302.03022},
}

@inproceedings{Zhou2024ISMR,
  author = {Zhou, Haoying and Jiang, Yiwei and Gao, Shang and Wang, Shiyue and Kazanzides, Peter and Fischer, Gregory S},
  title = {Suturing Tasks Automation Based on Skills Learned From Demonstrations: A Simulation Study},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2024},
  doi = {10.1109/ISMR63436.2024.10586017},
  semanticscholar = {https://www.semanticscholar.org/paper/12663f654c2cc1588bc366d8c88d36b173ea3b12},
  arxiv = {https://arxiv.org/abs/2403.00956},
  abstract = {In this work, we develop an open-source surgical simulation environment that includes a realistic model obtained by MRI-scanning a physical phantom, for the purpose of training and evaluating a Learning from Demonstration (LfD) algorithm for autonomous suturing. The LfD algorithm utilizes Dynamic Movement Primitives (DMP) and Locally Weighted Regression (LWR), but focuses on the needle trajectory, rather than the instruments, to obtain better generality with respect to needle grasps. We conduct a user study to collect multiple suturing demonstrations and perform a comprehensive analysis of the ability of the LfD algorithm to generalize from a demonstration at one location in one phantom to different locations in the same phantom and to a different phantom. Our results indicate good generalization, on the order of 91.5%, when learning from more experienced subjects, indicating the need to integrate skill assessment in the future.},
  keywords = {ambf, dvrk, machine-learning, simulation},
  date = {2024-06-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2024-06-01},
}

@article{ding2024towards,
  author = {Ding, Hao and Seenivasan, Lalithkumar and Shu, Hongchao and Byrd, Grayson and Zhang, Han and Xiao, Pu and Barragan, Juan Antonio and Taylor, Russell H and Kazanzides, Peter and Unberath, Mathias},
  title = {Towards robust automation of surgical systems via digital twin-based scene representations from foundation models},
  journal = {arXiv preprint arXiv:2409.13107},
  year = {2024},
  doi = {10.48550/arXiv.2409.13107},
  url = {https://arxiv.org/abs/2409.13107},
  semanticscholar = {https://www.semanticscholar.org/paper/84db0a87c3613b1b418bced000bd06cae1bb8f05},
  arxiv = {https://arxiv.org/abs/2409.13107},
  abstract = {Large language model-based (LLM) agents are emerging as a powerful enabler of robust embodied intelligence due to their capability of planning complex action sequences. Sound planning ability is necessary for robust automation in many task domains, but especially in surgical automation. These agents rely on a highly detailed natural language representation of the scene. Thus, to leverage the emergent capabilities of LLM agents for surgical task planning, developing similarly powerful and robust perception algorithms is necessary to derive a detailed scene representation of the environment from visual input. Previous research has focused primarily on enabling LLM-based task planning while adopting simple yet severely limited perception solutions to meet the needs for bench-top experiments but lack the critical flexibility to scale to less constrained settings. In this work, we propose an alternate perception approach -- a digital twin-based machine perception approach that capitalizes on the convincing performance and out-of-the-box generalization of recent vision foundation models. Integrating our digital twin-based scene representation and LLM agent for planning with the dVRK platform, we develop an embodied intelligence system and evaluate its robustness in performing peg transfer and gauze retrieval tasks. Our approach shows strong task performance and generalizability to varied environment settings. Despite convincing performance, this work is merely a first step towards the integration of digital twin-based scene representations. Future studies are necessary for the realization of a comprehensive digital twin framework to improve the interpretability and generalizability of embodied intelligence in surgery.},
}

@article{Ferro2023ACD,
  author = {Marco Ferro and Alessandro Mirante and F. Ficuciello and Marilena Vendittelli},
  title = {A CoppeliaSim Dynamic Simulator for the Da Vinci Research Kit},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2023},
  volume = {8},
  pages = {129-136},
  doi = {10.1109/LRA.2022.3222994},
  semanticscholar = {https://www.semanticscholar.org/paper/7e240acfaf4daabad78050edee7beebe0a43e1c3},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Zhang2023AST,
  author = {Rui Zhang and Junhong Chen and Zeyu Wang and Ziqi Yang and Yunxiao Ren and Peilun Shi and James Calo and K. Lam and S. Purkayastha and Benny P. L. Lo},
  title = {A Step Towards Conditional Autonomy - Robotic Appendectomy},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2023},
  volume = {8},
  pages = {2429-2436},
  doi = {10.1109/LRA.2023.3254859},
  semanticscholar = {https://www.semanticscholar.org/paper/77877e028e5dace94f1f042c2a4e5e71e840c6a5},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Rivas-Blanco2021a,
  author = {Rivas-Blanco, Irene and P{\'{e}}rez-del-Pulgar, Carlos J. and Mariani, Andrea and Quaglia, Claudio and Tortora, Giuseppe and Menciassi, Arianna and Mu{\~{n}}oz, V{\'{i}}ctor F.},
  title = {A surgical dataset from the da Vinci Research Kit for task automation and recognition},
  journal = {IEEE Access},
  year = {2023},
  volume = {11},
  doi = {10.1109/ICECCME57830.2023.10253032},
  semanticscholar = {https://www.semanticscholar.org/paper/0da7e883d55dec0d669fbfd338c469a72f08fbce},
  arxiv = {https://arxiv.org/abs/2102.03643},
  research_field = {TR},
  data_type = {RI and KD and SD},
  dvrk_site = {SSSA},
  abstract = {The use of large datasets is essential in surgical robotics to advance in the field of recognition and automation of surgical tasks. Furthermore, public datasets allow the comparison of different algorithms and methods to evaluate their performance. The objective of this work is to provide a complete dataset of three common training surgical tasks performed with the da Vinci Research Kit (dVRK). The dataset contains a total of 206 trials performed by twelve different subjects. For each trial, the dataset includes 154 kinematic variables from the dVRK (both master and slave sides) together with the associated video recorded with a camera. All the data has been carefully timestamped and provided in a readable csv format. A MATLAB interface integrated with ROS for using and replicating the data is also provided.},
}

@article{Dharmarajan2023ATF,
  author = {K. Dharmarajan and Will Panitch and Baiyu Shi and Huang Huang and L. Chen and Thomas Low and Danyal M. Fer and Ken Goldberg},
  title = {A Trimodal Framework for Robot-Assisted Vascular Shunt Insertion When a Supervising Surgeon is Local, Remote, or Unavailable},
  journal = {2023 International Symposium on Medical Robotics (ISMR)},
  booktitle = {International Symposium on Medical Robotics},
  year = {2023},
  pages = {1-8},
  doi = {10.1109/ISMR67322.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
}

@article{Moccia2023AutonomousEC,
  author = {R. Moccia and F. Ficuciello},
  title = {Autonomous Endoscope Control Algorithm with Visibility and Joint Limits Avoidance Constraints for da Vinci Research Kit Robot},
  journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2023},
  pages = {776-781},
}

@article{Chu2023BootstrappingRS,
  author = {X. Chu and Yunxi Tang and L. Kwok and Yuanpei Cai and K. W. S. Au},
  title = {Bootstrapping Robotic Skill Learning With Intuitive Teleoperation: Initial Feasibility Study},
  booktitle = {International Symposium on Experimental Robotics},
  year = {2023},
  pages = {42-52},
  doi = {10.1007/978-3-319-23778-7},
  semanticscholar = {https://www.semanticscholar.org/paper/64d9374af1ddf5b6f19e65ec5a8af107ea39284c},
}

@article{Oh2023ComprehensiveRC,
  author = {Ki Hwan Oh and Leonardo Borgioli and Alberto Mangano and Valentina Valle and Marco Di Pangrazio and F. Toti and Gioia Pozza and Luciano Ambrosini and A. Ducas and Milos Zefran and Liaohai Chen and P. Giulianotti},
  title = {Comprehensive Robotic Cholecystectomy Dataset (CRCD): Integrating Kinematics, Pedal Signals, and Endoscopic Videos},
  journal = {2024 International Symposium on Medical Robotics (ISMR)},
  booktitle = {International Symposium on Medical Robotics},
  year = {2023},
  pages = {1-7},
  doi = {10.1109/ISMR67322.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
}

@article{Huang2023DemonstrationGuidedRL,
  author = {Tao Huang and Kai Chen and Bin Li and Yunhui Liu and Qingxu Dou},
  title = {Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot},
  journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2023},
  pages = {4640-4647},
}

@article{Soudan2023DevelopmentAV,
  author = {Mulham Soudan and Shannon L. King and Scotty A. Chung and Philip Brown},
  title = {Development and Validation of a 3DOF Force Sensing Tool for In-Situ Surgical Robotics},
  journal = {Journal of Medical and Biological Engineering},
  booktitle = {Journal of Medical and Biological Engineering},
  year = {2023},
  volume = {43},
  pages = {332-338},
  doi = {10.1007/s40846-015-0091-y},
  semanticscholar = {https://www.semanticscholar.org/paper/fa63e4936e26f82cfbdc80b707bd0416887a58b7},
}

@inproceedings{GreeneISMR2023,
  author = {Greene, Nicholas and Luo, Wenkai and Kazanzides, Peter},
  title = {dVPose: Automated Data Collection and Dataset for 6D Pose Estimation of Robotic Surgical Instruments},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2023},
  doi = {10.1109/ISMR57123.2023.10130238},
  semanticscholar = {https://www.semanticscholar.org/paper/4a3fcc3d99ca77809e037ec584dd4041b8b5f460},
  abstract = {We present dVPose, a realistic multi-modality dataset intended for use in the development and evaluation of real-time single-shot deep-learning based 6D pose estimation algorithms on a head mounted display (HMD). In addition to the dataset, our contribution includes an automated (robotic) data collection platform that integrates an accurate optical tracking system to provide the ground-truth poses. We collected a comprehensive set of data for vision-based 6D pose estimation, including images and poses of the extra-corporeal portions of the instruments and endoscope of a da Vinci surgical robot. The images are collected using the multi-camera rig of the Microsoft HoloLens 2 HMD, mounted on a UR10 robot, and the corresponding poses are collected by optically tracking both the instruments/endoscope and HMD. The intended application is to enable markerless localization of the HMD with respect to the da Vinci robot, considering that the instruments and endoscope are among the few robotic components that are not covered by sterile drapes. Our dataset features synchronized images from the RGB, depth, and grayscale cameras of the HoloLens 2 device. It is unique in that it provides medically focused images, provides images from a HoloLens 2 device where object tracking is a fundamental task, and provides data from multiple visible-light cameras in addition to depth. Furthermore, the automated data collection platform can be easily adapted to collect images and ground-truth poses of other objects.},
  keywords = {arssist, dvrk, hmd},
  date = {2023-04-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2023-04-01},
}

@article{He2023dVRKbasedTO,
  author = {Changyan He and Robert H. Nguyen and E. Diller and James M. Drake and T. Looi},
  title = {dVRK-based teleoperation of a CTR robot with stereovision feedback for neurosurgery},
  journal = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
  booktitle = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
  year = {2023},
  doi = {10.31256/hsmr2023.75},
  semanticscholar = {https://www.semanticscholar.org/paper/1e93093119bb9106c0303e97200ee4ddada832c8},
  abstract = {Raman spectroscopy is a photonic modality defined as the inelastic backscattering of excitation coherent laser light. It is particularly beneficial for rapid tissue diagnosis in sensitive intraoperative environments like those involving the brain, due to its nonionizing potential, point-scanning capability, and highly-specific spectral fingerprint signatures that can characterize tissue pathology [1]. While Raman scattering is an inherently weak process, Surface-Enhanced Raman Spectroscopy (SERS), which is based on the use of metal nanostructure surfaces to amplify Raman signals, has become a compelling method for achieving highly specific Raman spectra with detection sensitivity comparable to conventional modalities such as fluorescence [2]. A unique plasmonics-active nanoplatform, SERS gold nanostars (GNS) have previously been designed in our group to accumulate preferentially in brain tumors [2]. Raman detection, when combined with machine learning and robotics, stands to enhance the diagnosis of ambiguous tissue during tumor resection surgery, with the potential to improve extent-of-resection and rapidly reconstruct the dynamic surgical field. Here we demonstrate preliminary results from the use of a SERS-based robotics platform to efficiently recreate a tumor embedded in healthy tissue, which is modeled here as a GNS-infused phantom. Transfer learning, specifically through use of the open-source RRUFF mineral database, is employed here to address the dearth of collected biomedical Raman data [3].},
}

@article{Baweja2023ExperimentalTW,
  author = {Paramjit Singh Baweja and R. Gondokaryono and L. Kahrs},
  title = {Experimental Trials with a Shared Autonomy Controller Framework and the da Vinci Research Kit: Pattern Cutting Tasks using Thin Elastic Materials},
  journal = {2023 International Symposium on Medical Robotics (ISMR)},
  booktitle = {International Symposium on Medical Robotics},
  year = {2023},
  pages = {1-7},
  doi = {10.1109/ISMR67322.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
}

@article{Büter2023EyeTF,
  author = {Regine Büter and Roger D. Soberanis-Mukul and Paola Ruiz Puentes and Ahmed Ghazi and Jie Ying Wu and Mathias Unberath},
  title = {Eye tracking for tele-robotic surgery: a comparative evaluation of head-worn solutions},
  booktitle = {Medical Imaging},
  year = {2023},
  volume = {12928},
  pages = {129281Y - 129281Y-6},
  doi = {10.1038/nrclinonc.2017.141},
  semanticscholar = {https://www.semanticscholar.org/paper/f9b5c08711b30103ffc7ba144d4f590bbd9b065b},
}

@inproceedings{Deo2023,
  author = {Deo, Akhil and Kazanzides, Peter},
  title = {Feasibility of Mobile Application for Surgical Robot Teleoperation},
  booktitle = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
  year = {2023},
  pages = {121--122},
  month = {jun},
  publisher = {The Hamlyn Centre, Imperial College London London, UK},
  doi = {10.31256/HSMR2023.63},
  semanticscholar = {https://www.semanticscholar.org/paper/ffc2919d3c966c096e2b89993f5031b8f2a1738a},
  research_field = {TR and HW},
  data_type = {KD and SD},
  abstract = {In recent years, robotic surgery has gained popularity due to its numerous advantages, including greater control and access for surgeons, shorter recovery times, and lower levels of pain for patients [1]. However, surgical robot systems require extensive training for surgeons to master their use. Simulation-based training has become a component of surgical education [2], providing a safe environment for trainees to acquire and refine their skills. However, most existing training platforms require bulky and expensive control consoles, which limits their availability and convenience. The development of a low- cost and easily deployed control console can address these limitations, thereby potentially enhancing the effectiveness of robotic surgery training. A system that satisfies these criteria can also enable medical robotics research in low-resource environments, where cost and accessibility are the most significant impediments to research. This paper describes the creation and evaluation of an iPhone application for these purposes.},
}

@article{Pasini2023,
  author = {Pasini, Nicolo and Mariani, Andrea and Deguet, Anton and Kazanzides, Peter and Momi, Elena De},
  title = {GRACE: Online Gesture Recognition for Autonomous Camera-Motion Enhancement in Robot-Assisted Surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2023},
  volume = {8},
  number = {12},
  pages = {8263-8270},
  doi = {10.1109/LRA.2023.3326690},
  semanticscholar = {https://www.semanticscholar.org/paper/e2e33ae352b72bd0a48c4543ad70d0545e377e67},
  abstract = {Camera navigation in minimally invasive surgery changed significantly since the introduction of robotic assistance. Robotic surgeons are subjected to a cognitive workload increase due to the asynchronous control over tools and camera, which also leads to interruptions in the workflow. Camera motion automation has been addressed as a possible solution, but still lacks situation awareness. We propose an online surgical Gesture Recognition for Autonomous Camera-motion Enhancement (GRACE) system to introduce situation awareness in autonomous camera navigation. A recurrent neural network is used in combination with a tool tracking system to offer gesture-specific camera motion during a robotic-assisted suturing task. GRACE was integrated with a research version of the da Vinci surgical system and a user study (involving 10 participants) was performed to evaluate the benefits introduced by situation awareness in camera motion, both with respect to a state of the art autonomous system (S) and current clinical approach (P). Results show GRACE improving completion time by a median reduction of <inline-formula><tex-math notation="LaTeX">$\text\{18.9\} \text\{s\}$</tex-math></inline-formula> (8.1%) with respect to S and <inline-formula><tex-math notation="LaTeX">$\text\{65.1\}\text\{s\}$</tex-math></inline-formula> (21.1%) with respect to P. Also, workload reduction was confirmed by statistical difference in the NASA Task Load Index with respect to S (<inline-formula><tex-math notation="LaTeX">$p < \text\{0.05\}$</tex-math></inline-formula>). Reduction of motion sickness, a common issue related to continuous camera motion of autonomous systems, was assessed by a post-experiment survey (<inline-formula><tex-math notation="LaTeX">$p < \text\{0.01\}$</tex-math></inline-formula>).},
  keywords = {dvrk, machine-learning},
  date = {2023-12-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2023-12-01},
}

@inproceedings{ChenISMR2023,
  author = {Chen, An Chi and Hadi, Muhammad and Kazanzides, Peter and Azimi, Ehsan},
  title = {Mixed Reality Based Teleoperation of Surgical Robotics},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2023},
  doi = {10.1109/ISMR57123.2023.10130178},
  semanticscholar = {https://www.semanticscholar.org/paper/dccfbc6e37fe0a1cef7fc49598d4b70b6234f288},
  abstract = {Many surgical robotic systems are controlled by mechanical based devices that require the operator to remain at a fixed location away from the robot. This restriction in mobility and physical barrier between the surgeon and the robot may reduce procedural efficiency. Thus, we propose an alternative teleoperation approach and mixed reality based system that uses the surgeon's tracked hand poses to control the robot through the use of an untethered head mounted display. We conducted a controlled user study to assess the efficacy of our system. Our experimental results indicate that, for the ring-wire task we tested, there is not a considerable difference in the performance of users compared to existing mechanical based teleoperation devices.},
  keywords = {dvrk, hmd},
  date = {2023-04-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2023-04-01},
}

@article{Vries2023MRguidedHP,
  author = {M. de Vries and M. Wijntjes and J. Sikorski and P. Moreira and N. J. van de Berg and J. J. van den Dobbelsteen and S. Misra},
  title = {MR-guided HDR prostate brachytherapy with teleoperated steerable needles},
  journal = {Journal of Robotic Surgery},
  booktitle = {Journal of Robotic Surgery},
  year = {2023},
  volume = {17},
  pages = {2461 - 2469},
  doi = {10.1007/s11701-025-02399-x},
  semanticscholar = {https://www.semanticscholar.org/paper/58253e7b7ceea8a141829529091c75d0bda3190f},
}

@article{Sallam2023PrototypeRO,
  author = {M. Sallam and G. A. Fontanelli and A. Gallo and R. Rocca and A. D. S. Sardo and Nicola Longo and F. Ficuciello},
  title = {Prototype Realization of a Human Hand-Inspired Needle Driver for Robotic-Assisted Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  booktitle = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2023},
  volume = {5},
  pages = {843-856},
  doi = {10.1109/TMRB.2023.3309942},
  semanticscholar = {https://www.semanticscholar.org/paper/95b40fa3b7ccbe83fe678c44a35bd6153752807d},
  abstract = {Surgical robots have been widely employed to help doctors to perform operations through small incisions with precise control and enhanced vision. However, compared to the human hand, the manipulation capabilities of the instruments used in robotic-assisted surgeries are still limited. In earlier research, we designed a new needle driver tool inspired by the human hand rolling capabilities. The new tool allowed for needle reorientation with only one hand, unlike the standard tool that requires the use of both hands to reorient the needle during surgical intervention. The present paper extends our previous work by enhancing the design of the new tool, manufacturing 1–1 scale working prototype, mounting and controlling the tool actuators, and integrating a modified master device to remotely control the new tool. The functionality and grasping capabilities of the developed prototype was tested using the research kit of da Vinci surgical system (dVRK). Moreover, the performance of the new tool was evaluated by a group of surgeons based on the standard NASA Task Load Index (TLX). The results validate the proposed design and its manufacturability and suggest that the tool can effectively enhance the needle manipulation and improve the surgeon’s dexterity.},
}

@article{Puentes2023,
  author = {Puentes, Paola Ruiz and Soberanis-Mukul, Roger D and Acar, Ayberk and Gupta, Iris and Bhowmick, Joyraj and Li, Yizhou and Ghazi, Ahmed and Kazanzides, Peter and Wu, Jie Ying and Unberath, Mathias},
  title = {Pupillometry in telerobotic surgery: A comparative evaluation of algorithms for cognitive effort estimation},
  journal = {Medical Robotics},
  year = {2023},
  volume = {1},
  number = {3},
  pages = {1-8},
  doi = {10.54844/mr.2023.0420},
  semanticscholar = {https://www.semanticscholar.org/paper/520b0ad6d8babb5120f0a90d338a0ce981ddfec9},
  abstract = {Background: Eye gaze tracking and pupillometry are emerging topics in telerobotic surgery as it is believed that they will enable novel gaze-based interaction paradigms and provide insights into the user’s cognitive load (CL). Further, the successful integration of CL estimation into telerobotic systems is thought to catalyze the development of new human-computer interfaces for personalized assistance and training processes. However, this field is in its infancy, and identifying reliable gaze and pupil-tracking solutions in robotic surgery is still an area of ongoing research and high uncertainty. Methods: Considering the potential benefits of pupillometry-based CL assessments in telerobotic surgery, we seek to better understand the possibilities and limitations of contemporary pupillometry-based cognitive effort estimation algorithms in telerobotic surgery. To this end, we conducted a user study using the da Vinci Research Kit (dVRK) and performed two experiments where participants were asked to perform a series of N-Back tests, either while (i) idling or (ii) performing a peg transfer task. We then compare four contemporary CL estimation methods based on direct analysis of pupil diameter in the spatial and frequency domains. Results: We find that some methods can detect the presence of cognitive effort in simple scenarios (e.g., when the user is not performing any manual task), they fail to differentiate the different levels of CL. Similarly, when the manual peg transfer task is added, the reliability of all models is compromised, highlighting the necessity of more robust methods that consider different factors that complement the pupil diameter information. Conclusion: Our results offer a quantitative perspective of the limitations of the current solutions and highlight the necessity of developing tailored designs for the telerobotic surgery environment.},
  keywords = {dvrk},
  date = {2023-08-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2023-08-01},
}

@article{Ou2023RobotLI,
  author = {Yafei Ou and Sadra Zargarzadeh and Mahdi Tavakoli},
  title = {Robot Learning Incorporating Human Interventions in the Real World for Autonomous Surgical Endoscopic Camera Control},
  journal = {J. Medical Robotics Res.},
  booktitle = {J. Medical Robotics Res.},
  year = {2023},
  volume = {8},
  pages = {2340004:1-2340004:12},
  doi = {10.48550/arXiv.2502.18586},
  semanticscholar = {https://www.semanticscholar.org/paper/f468bb674fac8fbbcbece1d58c89ce2fd6f1cedc},
  arxiv = {https://arxiv.org/abs/2502.18586},
  abstract = {Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.},
}

@article{Dharmarajan2023RobotAssistedVS,
  author = {K. Dharmarajan and Will Panitch and Baiyu Shi and Huang Huang and L. Chen and M. Moghani and Qinxi Yu and Kush Hari and Thomas Low and Danyal M. Fer and Animesh Garg and Ken Goldberg},
  title = {Robot-Assisted Vascular Shunt Insertion with the dVRK Surgical Robot},
  journal = {J. Medical Robotics Res.},
  booktitle = {J. Medical Robotics Res.},
  year = {2023},
  volume = {8},
  pages = {2340006:1-2340006:15},
  doi = {10.48550/arXiv.2502.18586},
  semanticscholar = {https://www.semanticscholar.org/paper/f468bb674fac8fbbcbece1d58c89ce2fd6f1cedc},
  arxiv = {https://arxiv.org/abs/2502.18586},
  abstract = {Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.},
}

@article{Liu2023RoboticMO,
  author = {Fei Liu and Entong Su and Jingpei Lu and Ming Li and Michael C. Yip},
  title = {Robotic Manipulation of Deformable Rope-Like Objects Using Differentiable Compliant Position-Based Dynamics},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2023},
  volume = {8},
  pages = {3964-3971},
  doi = {10.1109/LRA.2023.3264766},
  semanticscholar = {https://www.semanticscholar.org/paper/84d074c9b9657172a7adfa5d53140a207297a8b6},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@inproceedings{Ishida2023CommLoss,
  author = {Ishida, Hisashi and Munawar, Adnan and Taylor, Russell H and Kazanzides, Peter},
  title = {Semi-Autonomous Assistance for Telesurgery Under Communication Loss},
  booktitle = {IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)},
  year = {2023},
  pages = {8467-8473},
  doi = {10.1109/IROS55552.2023.10341450},
  semanticscholar = {https://www.semanticscholar.org/paper/6807e3e9e112329f4bdd9b2cbe479b73c4e279f1},
  abstract = {Telesurgery has a clear potential for providing high-quality surgery to medically underserved areas like rural areas, battlefields, and spacecraft; nevertheless, effective methods to overcome unreliable communication systems are still lacking. Furthermore, it is not well understood how users react at the moment of communication loss and also during the loss. In this paper, we aim to analyze human response by proposing a telesurgery simulation framework that models an environment incorporating local and remote sites. Furthermore, this framework generates structural data for human behavior analysis and can provide different forms of assistance during the communication failure and at the communication recovery. We investigated three different types of assistance: User-centered, Robot-centered and Hybrid. A 12-person user-study was carried out using the proposed telesurgery simulation where participants completed a peg transfer task with random communication loss. The collected data was used to analyze the human response to a communication failure. The proposed Hybrid method reduced temporal demand with no increase in completion time compared to the baseline control method where users were unable to move the input device during the communication loss. The Hybrid method also significantly reduced both the task completion time and workload compared to the other two proposed methods (User-centered and Robot-centered).},
  keywords = {dvrk},
  address = {Detroit, MI},
  date = {2023-10-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2023-10-01},
}

@article{Senthilkumar2023SimulatingMC,
  author = {Kanishkan Senthilkumar and R. Gondokaryono and Mustafa Haiderbhai and L. Kahrs},
  title = {Simulating Mesh Cutting with the dVRK in Unity},
  journal = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
  booktitle = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
  year = {2023},
  doi = {10.31256/hsmr2023.75},
  semanticscholar = {https://www.semanticscholar.org/paper/1e93093119bb9106c0303e97200ee4ddada832c8},
  abstract = {Raman spectroscopy is a photonic modality defined as the inelastic backscattering of excitation coherent laser light. It is particularly beneficial for rapid tissue diagnosis in sensitive intraoperative environments like those involving the brain, due to its nonionizing potential, point-scanning capability, and highly-specific spectral fingerprint signatures that can characterize tissue pathology [1]. While Raman scattering is an inherently weak process, Surface-Enhanced Raman Spectroscopy (SERS), which is based on the use of metal nanostructure surfaces to amplify Raman signals, has become a compelling method for achieving highly specific Raman spectra with detection sensitivity comparable to conventional modalities such as fluorescence [2]. A unique plasmonics-active nanoplatform, SERS gold nanostars (GNS) have previously been designed in our group to accumulate preferentially in brain tumors [2]. Raman detection, when combined with machine learning and robotics, stands to enhance the diagnosis of ambiguous tissue during tumor resection surgery, with the potential to improve extent-of-resection and rapidly reconstruct the dynamic surgical field. Here we demonstrate preliminary results from the use of a SERS-based robotics platform to efficiently recreate a tumor embedded in healthy tissue, which is modeled here as a GNS-infused phantom. Transfer learning, specifically through use of the open-source RRUFF mineral database, is employed here to address the dearth of collected biomedical Raman data [3].},
}

@article{Hu2023TowardsHC,
  author = {Zhaoyang Jacopo Hu and Ziwei Wang and Yanpei Huang and Aran Sena and F. Rodriguez y Baena and E. Burdet},
  title = {Towards Human-Robot Collaborative Surgery: Trajectory and Strategy Learning in Bimanual Peg Transfer},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2023},
  volume = {8},
  pages = {4553-4560},
  doi = {10.1109/LRA.2023.3285478},
  semanticscholar = {https://www.semanticscholar.org/paper/a469084002b8c3b84a30715acfc3f77f8c5b2104},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Minelli2023TwoLayerBasedMB,
  author = {M. Minelli and Nicola Piccinelli and Fabio Falezza and F. Ferraguti and R. Muradore and C. Secchi},
  title = {Two-Layer-Based Multiarms Bilateral Teleoperation Architecture},
  journal = {IEEE Transactions on Control Systems Technology},
  booktitle = {IEEE Transactions on Control Systems Technology},
  year = {2023},
  volume = {31},
  pages = {1266-1279},
  doi = {10.1109/tcst.87},
  semanticscholar = {https://www.semanticscholar.org/paper/5144c3b1286fe349ad63ef03e2533d9d5c9f4d8b},
  abstract = {M. MAGGIORE University of Toronto C. MANZIE Univ. of Melbourne A. MARINO University of Salerno P. MHASKAR McMaster Univ. G. NOTARSTEFANO University of Salento P. ODGAARD Vestas Wind Systems M. OISHI University of New Mexico N. OLGAC Univ. of Connecticut T. OOMEN Eindhoven University Y. ORLOV CICESE Mexico Y. PAN National University of Singapore C. PANAYIOTOU Univ. of Cyprus G. PAPAFOTIOU ABB A. PAVLOV NTNU, Norway G. PIN Electrolux S. PIROZZI Università di Napoli H. POTA Univ. of New South Wales at ADFA C. PRIEUR CNRS E. PUNTA National Research Council, Italy N. QUIJANO Universidad de los Andes Colombia D. M. RAIMONDO University of Pavia G. A. ROVITHAKIS Univ. of Thessaloniki K. RUDIE Queen’s Univ.},
}

@article{Wang2023UncertaintyAwareSL,
  author = {Ziheng Wang and A. Mariani and A. Menciassi and E. De Momi and A. M. Fey},
  title = {Uncertainty-Aware Self-Supervised Learning for Cross-Domain Technical Skill Assessment in Robot-Assisted Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  booktitle = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2023},
  volume = {5},
  pages = {301-311},
  doi = {10.1109/TMRB.2023.3272008},
  semanticscholar = {https://www.semanticscholar.org/paper/6f4dea727b22cdcda1ff9d3472aa54639296db57},
  arxiv = {https://arxiv.org/abs/2304.14589},
  abstract = {Objective technical skill assessment is crucial for effective training of new surgeons in robot-assisted surgery. With advancements in surgical training programs in both physical and virtual environments, it is imperative to develop generalizable methods for automatically assessing skills. In this paper, we propose a novel approach for skill assessment by transferring domain knowledge from labeled kinematic data to unlabeled data. Our approach leverages labeled data from common surgical training tasks such as Suturing, Needle Passing, and Knot Tying to jointly train a model with both labeled and unlabeled data. Pseudo labels are generated for the unlabeled data through an iterative manner that incorporates uncertainty estimation to ensure accurate labeling. We evaluate our method on a virtual reality simulated training task (Ring Transfer) using data from the da Vinci Research Kit (dVRK). The results show that trainees with robotic assistance have significantly higher expert probability compared to these without any assistance, $p < 0.05$ , which aligns with previous studies showing the benefits of robotic assistance in improving training proficiency. Our method offers a significant advantage over other existing works as it does not require manual labeling or prior knowledge of the surgical training task for robot-assisted surgery.},
}

@inproceedings{ZhangISMR2023,
  author = {Zhang, Jintan and Kazanzides, Peter},
  title = {Velocity Control for the da Vinci Research Kit},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2023},
  doi = {10.1109/ISMR57123.2023.10130265},
  semanticscholar = {https://www.semanticscholar.org/paper/7a41616900a1e57b910d37fd0265a330540b7351},
  abstract = {The da Vinci Research Kit (dVRK) consists of open-source electronics and software that provides access to all levels of control but, until now, has relied primarily on an inner motor current control loop in analog hardware and an outer position control loop on the PC. In this work, we present a low-level velocity control loop, implemented on the FPGA, as an alternative to the PC-based (existing) position controller or (potential) velocity controller. The proposed velocity controller takes advantage of hardware-based measurement of the encoder period, which is inversely proportional to velocity. To avoid division on the FPGA, we implement closed loop control of the encoder period. Our implementation requires the controller gains to be adjusted based on the reference period; thus, the PC supplies both the reference period and the adjusted gains. We evaluate the proposed controller against a conventional implementation of a velocity control loop wrapped around the existing position control loop on the PC. The results demonstrate that the proposed controller yields improvements in tracking performance and disturbance rejection. The proposed velocity controller will be released open source to the dVRK community.},
  keywords = {dvrk},
  date = {2023-04-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2023-04-01},
}

@article{zruya2022new,
  author = {Zruya, Or and Sharon, Yarden and Kossowsky, Hanna and Forni, Fulvio and Geftler, Alex and Nisky, Ilana},
  title = {A New Power Law Linking the Speed to the Geometry of Tool-Tip Orientation in Teleoperation of a Robot-Assisted Surgical System},
  journal = {IEEE Robotics and Automation Letters},
  year = {2022},
  volume = {7},
  number = {4},
  pages = {10762-10769},
  doi = {10.1101/2022.03.02.482648},
  semanticscholar = {https://www.semanticscholar.org/paper/1bb4488df86b1e5ab4cee3938f3bb112ff7e211d},
  research_field = {TR},
  data_type = {RI and KD},
  abstract = {Fine manipulation is important in dexterous tasks executed via teleoperation, including in robot-assisted surgery. Discovering fundamental laws of human movement can benefit the design and control of teleoperated systems, and the training of their users. These laws are formulated as motor invariants, such as the well-studied speed-curvature power law. However, while the majority of these laws characterize translational movements, fine manipulation requires controlling the orientation of objects as well. This subject has received little attention in human motor control studies. Here, we report a new power law linking the speed to the geometry in orientation control – humans rotate their hands with an angular speed that is exponentially related to the local change in the direction of rotation. We demonstrate this law in teleoperated tasks performed by surgeons using surgical robotics research platforms. Additionally, we show that the law’s parameters change slowly with the surgeons’ training, and are robust within participants across task segments and repetitions. The fact that this power law is a robust motor invariant suggests that it may be an outcome of sensorimotor control. It also opens questions about the nature of this control and how it can be harnessed for better control of human-teleoperated robotic systems.},
}

@article{Huang2022ASP,
  author = {Yisen Huang and Jian Li and Xue Zhang and Ke Xie and Jixiu Li and Yue Liu and C. Ng and P. Chiu and Zheng Li},
  title = {A Surgeon Preference-Guided Autonomous Instrument Tracking Method With a Robotic Flexible Endoscope Based on dVRK Platform},
  journal = {IEEE Robotics and Automation Letters},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2022},
  volume = {PP},
  pages = {1-1},
  doi = {10.1109/LRA.2022.3143305},
  semanticscholar = {https://www.semanticscholar.org/paper/fe07358cc6fc157f2db02ca957f274ea61b238f1},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Lu2022AUM,
  author = {Bo Lu and Bin Li and Q. Dou and Yunhui Liu},
  title = {A Unified Monocular Camera-Based and Pattern-Free Hand-to-Eye Calibration Algorithm for Surgical Robots With RCM Constraints},
  journal = {IEEE/ASME Transactions on Mechatronics},
  booktitle = {IEEE/ASME transactions on mechatronics},
  year = {2022},
  volume = {27},
  pages = {5124-5135},
  doi = {10.1109/tmech.2020.2967763},
  semanticscholar = {https://www.semanticscholar.org/paper/66f6e36718e68024806b8eeba8c9b1b7781dd08d},
  abstract = {This TransacTions is a joint publication of the IEEE and ASME. For membership and subscription information and pricing, please visit www.ieee.org/membership-catalog. Member copies of Transactions/Journals are for personal use only. IEEE/ASME TRANSACTIONS ON MECHATRONICS MANAGEMENT COMMITTEE Chairman: Xiaobo Tan (asME DscD), Treasurer: HirosHi FujiMoTo (iEs), Secretary: Kyujin cHo (ras), jun uEDa (asME DscD), aaron Dollar (ras), and MicHaEl ruDErMan (iEs)},
}

@inproceedings{fan2022unity,
  author = {Fan, Ke and Marzullo, Aldo and Pasini, Nicolo and Rota, Alberto and Pecorella, Matteo and Ferrigno, Giancarlo and De Momi, Elena},
  title = {A unity-based da vinci robot simulator for surgical training},
  booktitle = {2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)},
  year = {2022},
  pages = {1--6},
  doi = {10.1109/BioRob52689.2022.9925319},
  semanticscholar = {https://www.semanticscholar.org/paper/38632f61717336df2693a56e504fb0e7e2416e4b},
  research_field = {TR},
  data_type = {KD},
  abstract = {The development of the Robot-Assisted Minimally Invasive Surgery (RAMIS) imposes an increasing demand for surgical training platforms, especially low-cost simulation-based surgical training through the creation of new open-source modules. For this goal, a da Vinci Surgical robot simulator based on Unity Physics Engine is developed. The simulator is integrated with da Vinci Research Kit (dVRK), robot kinematic models and multiple sensors. The Robot Operating System (ROS) interface is embedded for better integration with ROS based software components. The simulator can provide interactive information such as haptic feedback with master input devices. An application of a virtual fixture is implemented to test and verify the performance of the simulator. The results show that the simulator has high expansibility and support interactive training tasks well.},
  organization = {IEEE},
}

@inproceedings{Pasini2022,
  author = {Pasini, Nicolo and Mariani, Andrea and Munawar, Adnan and De Momi, Elena and Kazanzides, Peter},
  title = {A virtual suturing task: proof of concept for awareness in autonomous camera motion},
  booktitle = {IEEE Intl. Conf. on Robotic Computing (IRC)},
  year = {2022},
  pages = {376-382},
  doi = {10.1109/IRC55401.2022.00073},
  semanticscholar = {https://www.semanticscholar.org/paper/92829ce84e653e7c07452453ac526e8091a641bc},
  abstract = {Robot-assisted Minimally Invasive Surgery (MIS) requires the surgeon to alternatively control both the surgical instruments and the endoscopic camera, or to leave this burden to an assistant. This increases the cognitive load and interrupts the workflow of the operation. Camera motion automation has been examined in the literature to mitigate these aspects, but still lacks situation awareness, a key factor for camera navigation enhancement. This paper presents the development of a phase-specific camera motion automation, implemented in Virtual Reality (VR) during a suturing task. A user study involving 10 users was carried out using the master console of the da Vinci Research Kit. Each subject performed the suturing task undergoing both the proposed autonomous camera motion and the traditional manual camera control. Results show that the proposed system can reduce operational time, decreasing both the user's mental and physical demand. Situational awareness is shown to be fundamental in exploiting the benefits introduced by camera motion automation.},
  keywords = {dvrk},
  address = {Naples, Italy},
  date = {2022-12-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2022-12-01},
}

@article{Varier2022AMBFRLAR,
  author = {Vignesh Manoj Varier and Dhruv Kool Rajamani and Farid Tavakkolmoghaddam and A. Munawar and G. Fischer},
  title = {AMBF-RL: A real-time simulation based Reinforcement Learning toolkit for Medical Robotics},
  journal = {2022 International Symposium on Medical Robotics (ISMR)},
  booktitle = {International Symposium on Medical Robotics},
  year = {2022},
  pages = {1-8},
  doi = {10.1109/ISMR67322.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
}

@article{Dharmarajan2022AutomatingVS,
  author = {K. Dharmarajan and Will Panitch and Muyan Jiang and Kishore Srinivas and Baiyu Shi and Yahav Avigal and Huang Huang and Thomas Low and Danyal M. Fer and Ken Goldberg},
  title = {Automating Vascular Shunt Insertion with the dVRK Surgical Robot},
  journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2022},
  pages = {6781-6788},
}

@conference{Nagy2022AutonomousPT,
  author = {T. D. Nagy and T. Haidegger},
  title = {Autonomous Peg Transfer—a Gateway to Surgery 4.0},
  journal = {2022 IEEE 10th Jubilee International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)},
  booktitle = {2022 IEEE 10th Jubilee International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)},
  year = {2022},
  pages = {000069-000076},
  doi = {10.1109/iccc202255925.2022.9922860},
  semanticscholar = {https://www.semanticscholar.org/paper/24e249b18b27e74886dd154706d8c6a0dad8b508},
}

@inproceedings{Ding2022,
  author = {Ding, Hao and Zhang, Jintan and Kazanzides, Peter and Wu, Jie Ying and Unberath, Mathias},
  title = {CaRTS: Causality-driven robot tool segmentation from vision and kinematics data},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  year = {2022},
  pages = {387-398},
  doi = {10.1007/978-3-031-16449-1_37},
  semanticscholar = {https://www.semanticscholar.org/paper/56ad522f775297d8b5fb63c960f0893d9352e224},
  arxiv = {https://arxiv.org/abs/2203.09475},
  abstract = {Vision-based segmentation of the robotic tool during robot-assisted surgery enables downstream applications, such as augmented reality feedback, while allowing for inaccuracies in robot kinematics. With the introduction of deep learning, many methods were presented to solve instrument segmentation directly and solely from images. While these approaches made remarkable progress on benchmark datasets, fundamental challenges pertaining to their robustness remain. We present CaRTS, a causality-driven robot tool segmentation algorithm, that is designed based on a complementary causal model of the robot tool segmentation task. Rather than directly inferring segmentation masks from observed images, CaRTS iteratively aligns tool models with image observations by updating the initially incorrect robot kinematic parameters through forward kinematics and differentiable rendering to optimize image feature similarity end-to-end. We benchmark CaRTS with competing techniques on both synthetic as well as real data from the dVRK, generated in precisely controlled scenarios to allow for counterfactual synthesis. On training-domain test data, CaRTS achieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when tested on counterfactually altered test data, exhibiting low brightness, smoke, blood, and altered background patterns. This compares favorably to Dice scores of 95.0 and 86.7, respectively, of the SOTA image-based method. Future work will involve accelerating CaRTS to achieve video framerate and estimating the impact occlusion has in practice. Despite these limitations, our results are promising: In addition to achieving high segmentation accuracy, CaRTS provides estimates of the true robot kinematics, which may benefit applications such as force estimation. Code is available at: https://github.com/hding2455/CaRTS},
  keywords = {dvrk, machine-learning},
  address = {Singapore},
  date = {2022-09-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2022-09-01},
}

@article{Cui2022CaveatsOT,
  author = {Zejian Cui and João Cartucho and S. Giannarou and Ferdinando Rodriguez y Baena},
  title = {Caveats on the First-Generation da Vinci Research Kit: Latent Technical Constraints and Essential Calibrations [Survey]},
  journal = {IEEE Robotics & Automation Magazine},
  booktitle = {IEEE robotics & automation magazine},
  year = {2022},
  volume = {32},
  pages = {113-128},
  doi = {10.1109/mra.2005.1577011},
  semanticscholar = {https://www.semanticscholar.org/paper/55c204f73459a88e25cff109d3d05da24d7ae004},
}

@inproceedings{tagliabue2022deliberation,
  author = {Tagliabue, Eleonora and Meli, Daniele and Dall'Alba, Diego and Fiorini, Paolo},
  title = {Deliberation in autonomous robotic surgery: a framework for handling anatomical uncertainty},
  booktitle = {2022 International Conference on Robotics and Automation (ICRA)},
  year = {2022},
  pages = {11080--11086},
  doi = {10.1109/ICRA46639.2022.9811820},
  semanticscholar = {https://www.semanticscholar.org/paper/91dc2eaa204146dcaec3114f3670611997cb611e},
  arxiv = {https://arxiv.org/abs/2203.05438},
  research_field = {AU and SS},
  data_type = {RI and KD},
  abstract = {Autonomous robotic surgery requires deliberation, i.e. the ability to plan and execute a task adapting to uncer-tain and dynamic environments. Uncertainty in the surgical domain is mainly related to the partial pre-operative knowledge about patient-specific anatomical properties. In this paper, we introduce a logic-based framework for surgical tasks with deliberative functions of monitoring and learning. The DE-liberative Framework for Robot-Assisted Surgery (DEFRAS) estimates a pre-operative patient-specific plan, and executes it while continuously measuring the applied force obtained from a biomechanical pre-operative model. Monitoring module compares this model with the actual situation reconstructed from sensors. In case of significant mismatch, the learning module is invoked to update the model, thus improving the estimate of the exerted force. DEFRAS is validated both in simulated and real environment with da Vinci Research Kit executing soft tissue retraction. Compared with state-of-the-art related works, the success rate of the task is improved while minimizing the interaction with the tissue to prevent unintentional damage.},
  organization = {IEEE},
}

@article{itzkovich2022generalization,
  author = {Itzkovich, Danit and Sharon, Yarden and Jarc, Anthony and Refaely, Yael and Nisky, Ilana},
  title = {Generalization of Deep Learning Gesture Classification in Robotic-Assisted Surgical Data: From Dry Lab to Clinical-Like Data},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  year = {2022},
  volume = {26},
  number = {3},
  pages = {1329-1340},
  doi = {10.1109/JBHI.2021.3117784},
  semanticscholar = {https://www.semanticscholar.org/paper/6d0f631f865f16a6f1855e28716e17772cf75c25},
  research_field = {TR},
  data_type = {RI and KD},
  abstract = {Objective: Robotic-assisted minimally invasive surgery (RAMIS) became a common practice in modern medicine and is widely studied. Surgical procedures require prolonged and complex movements; therefore, classifying surgical gestures could be helpful to characterize surgeon performance. The public release of the JIGSAWS dataset facilitates the development of classification algorithms; however, it is not known how algorithms trained on dry-lab data generalize to real surgical situations. Methods: We trained a Long Short-Term Memory (LSTM) network for the classification of dry lab and clinical-like data into gestures. Results: We show that a network that was trained on the JIGSAWS data does not generalize well to other dry-lab data and to clinical-like data. Using rotation augmentation improves performance on dry-lab tasks, but fails to improve the performance on clinical-like data. However, using the same network architecture, adding the six joint angles of the patient-side manipulators (PSMs) features, and training the network on the clinical-like data together lead to notable improvement in the classification of the clinical-like data. Discussion: Using the JIGSAWS dataset alone is insufficient for training a gesture classification network for clinical data. However, it can be very informative for determining the architecture of the network, and with training on a small sample of clinical data, can lead to acceptable classification performance. Significance: Developing efficient algorithms for gesture classification in clinical surgical data is expected to advance understanding of surgeon sensorimotor control in RAMIS, the automation of surgical skill evaluation, and the automation of surgery.},
}

@article{Zhang2022HumanRobotSC,
  author = {Dandan Zhang and Zicong Wu and Junhong Chen and Ruiqi Zhu and A. Munawar and Bo Xiao and Yuan Guan and Hang Su and Wuzhou Hong and Yao Guo and G. Fischer and Benny P. L. Lo and Guang Yang},
  title = {Human-Robot Shared Control for Surgical Robot Based on Context-Aware Sim-to-Real Adaptation},
  journal = {2022 International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2022},
  pages = {7694-7700},
}

@inproceedings{Long2022,
  author = {Long, Yonghao and Cao, Jianfeng and Deguet, Anton and Taylor, Russell H. and Dou, Qi},
  title = {Integrating Artificial Intelligence and Augmented Reality in Robotic Surgery: An Initial dVRK Study Using a Surgical Education Scenario},
  booktitle = {2022 International Symposium on Medical Robotics (ISMR)},
  year = {2022},
  volume = {},
  number = {},
  pages = {1-8},
  doi = {10.1109/ISMR48347.2022.9807505},
  ieeexplore = {https://ieeexplore.ieee.org/document/9807505},
  semanticscholar = {https://www.semanticscholar.org/paper/f79c918fdf59716a15feba1537e883d677976721},
  arxiv = {https://arxiv.org/abs/2201.00383},
  abstract = {Robot-assisted surgery has become progressively more and more popular due to its clinical advantages. In the meanwhile, the artificial intelligence and augmented reality in robotic surgery are developing rapidly and receive lots of attention. However, current methods have not discussed the coherent integration of AI and AR in robotic surgery. In this paper, we develop a novel system by seamlessly merging artificial intelligence module and augmented reality visualization to automatically generate the surgical guidance for robotic surgery education. Specifically, we first leverage reinforcement leaning to learn from expert demonstration and then generate 3D guidance trajectory, providing prior context information of the surgical procedure. Along with other information such as text hint, the 3D trajectory is then overlaid in the stereo view of dVRK, where the user can perceive the 3D guidance and learn the procedure. The proposed system is evaluated through a preliminary experiment on surgical education task peg-transfer, which proves its feasibility and potential as the next generation of robot-assisted surgery education solution.},
  keywords = {Visualization;Three-dimensional displays;Education;Merging;Surgery;Trajectory;Artificial intelligence},
}

@inproceedings{Zhang2022,
  author = {Zhang, Jintan and Yilmaz, Nural and Tumerdem, Ugur and Kazanzides, Peter},
  title = {Learning Based Estimation of 7 DOF Instrument and Grasping Forces on the da Vinci Research Kit},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2022},
  doi = {10.1109/ISMR48347.2022.9807554},
  semanticscholar = {https://www.semanticscholar.org/paper/a0ca57669a8be1a2c4aa0ec5c882a1406ab30575},
  arxiv = {https://arxiv.org/abs/2311.17863},
  abstract = {Positron Emission Tomography (PET) enables functional imaging of deep brain structures, but the bulk and weight of current systems preclude their use during many natural human activities, such as locomotion. The proposed long-term solution is to construct a robotic system that can support an imaging system surrounding the subject’s head, and then move the system to accommodate natural motion. This requires a system to measure the motion of the head with respect to the imaging ring, for use by both the robotic system and the image reconstruction software. We report here the design and experimental evaluation of a parallel string encoder mechanism for sensing this motion. Our preliminary results indicate that the measurement system may achieve accuracy within 0.5mm, especially for small motions, with improved accuracy possible through kinematic calibration.},
  keywords = {dvrk, machine-learning},
  date = {2022-04-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2022-04-01},
}

@article{kossowsky2022predicting,
  author = {Kossowsky, Hanna and Nisky, Ilana},
  title = {Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2022},
  volume = {4},
  number = {2},
  pages = {391-402},
  doi = {10.1109/TMRB.2022.3156635},
  semanticscholar = {https://www.semanticscholar.org/paper/26e69902cda3b3693dac5d2f7d1419e15cfa01dc},
  arxiv = {https://arxiv.org/abs/2109.11192},
  research_field = {TR},
  data_type = {RI and KD},
  abstract = {Robotic surgeries offer many benefits, however do not allow for simultaneous control of the endoscopic camera and the surgical instruments. This leads to frequent interruptions as surgeons adjust their viewpoints. Autonomous camera control could help overcome this challenge. We propose a predictive approach for anticipating when camera movements will occur using artificial neural networks. We used kinematic data of surgical instruments from robotic surgical training. We split the data into segments, and labeled if each segment immediately preceded a camera movement or did not. Due to the large class imbalance, we trained an ensemble of networks on balanced sub-sets of the data. We found that the instruments’ kinematics can be used to predict when camera movements will occur, and evaluated the performance on different segment durations and ensemble sizes. We also studied how much in advance upcoming camera movements can be predicted, and found that predicting camera movements up to 0.5 s in advance led to only a small decrease in performance relative to predicting imminent camera movements. These results serve as a proof-of-concept for predicting the timing of camera movements in robotic surgeries and suggest that an autonomous camera controller for robotic surgeries may someday be feasible.},
}

@article{Haiderbhai2022RobustST,
  author = {Mustafa Haiderbhai and R. Gondokaryono and T. Looi and James M. Drake and L. Kahrs},
  title = {Robust Sim2Real Transfer with the da Vinci Research Kit: A Study On Camera, Lighting, and Physics Domain Randomization},
  journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
  year = {2022},
  pages = {3429-3435},
}

@article{Zheng2022TowardCA,
  author = {Y. Zheng and Marzieh Ershad and A. M. Fey},
  title = {Toward Correcting Anxious Movements Using Haptic Cues on the Da Vinci Surgical Robot},
  journal = {2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)},
  booktitle = {International Conference on Biomedical Robotics and Biomechatronics},
  year = {2022},
  pages = {1-8},
  doi = {10.1109/TNSRE.2008.2008280},
  semanticscholar = {https://www.semanticscholar.org/paper/7ed8f84a096dec775b806ebaa524dcefd1803ffb},
}

@article{Lu2022TowardIA,
  author = {Bo Lu and Bin Li and Wei Chen and Yueming Jin and Zixu Zhao and Q. Dou and P. Heng and Yunhui Liu},
  title = {Toward Image-Guided Automated Suture Grasping Under Complex Environments: A Learning-Enabled and Optimization-Based Holistic Framework},
  journal = {IEEE Transactions on Automation Science and Engineering},
  booktitle = {IEEE Transactions on Automation Science and Engineering},
  year = {2022},
  volume = {19},
  pages = {3794-3808},
  doi = {10.1109/tase.2018.2887129},
  semanticscholar = {https://www.semanticscholar.org/paper/6edf9e85d10f3342aae626dfa68583132834edaf},
  abstract = {IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING is published by the IEEE Robotics and Automation Society. All members of the IEEE are eligible for membership in the Society and will receive this TRANSACTIONS upon payment of the annual Society membership fee of $9.00 plus an annual subscription fee of $50.00. For information on joining, write to the lEEE Service Center at the address below. Member copies of Transactions/Journals are for personal use only.},
}

@article{Yilmaz2022IJCARS,
  author = {Yilmaz, Nural and Zhang, Jintan and Kazanzides, Peter and Tumerdem, Ugur},
  title = {Transfer of learned dynamics between different surgical robots and operative configurations},
  journal = {Intl. Journal of Computer Assisted Radiology and Surgery (IJCARS)},
  year = {2022},
  volume = {17},
  pages = {903-910},
  publisher = {Springer},
  doi = {10.1007/s11548-022-02601-7},
  semanticscholar = {https://www.semanticscholar.org/paper/7d2dad9a09cf672fb249f61263a35433a6394662},
  keywords = {dvrk, machine-learning},
  date = {2022-04-01},
  pubstate = {published},
  tppubtype = {article},
  urldate = {2022-04-01},
}

@article{Attanasio2021,
  author = {Attanasio, Aleks and Alberti, Chiara and Scaglioni, Bruno and Marahrens, Nils and Frangi, Alejandro F. and Leonetti, Matteo and Biyani, Chandra Shekhar and {De Momi}, Elena and Valdastri, Pietro},
  title = {A Comparative Study of Spatio-Temporal U-Nets for Tissue Segmentation in Surgical Robotics},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2021},
  volume = {3},
  number = {1},
  month = {feb},
  doi = {10.1109/TMRB.2021.3054326},
  semanticscholar = {https://www.semanticscholar.org/paper/9bdcbb04224ab1a99bb8fcc629e2e3d3d088d990},
  research_field = {IM},
  data_type = {RI},
  dvrk_site = {UL},
  abstract = {In surgical robotics, the ability to achieve high levels of autonomy is often limited by the complexity of the surgical scene. Autonomous interaction with soft tissues requires machines able to examine and understand the endoscopic video streams in real-time and identify the features of interest. In this work, we show the first example of spatio-temporal neural networks, based on the U-Net, aimed at segmenting soft tissues in endoscopic images. The networks, equipped with Long Short-Term Memory and Attention Gate cells, can extract the correlation between consecutive frames in an endoscopic video stream, thus enhancing the segmentation’s accuracy with respect to the standard U-Net. Initially, three configurations of the spatio-temporal layers are compared to select the best architecture. Afterwards, the parameters of the network are optimised and finally the results are compared with the standard U-Net. An accuracy of 83.77% ± 2.18% and a precision of 78.42% ± 7.38% are achieved by implementing both Long Short Term Memory (LSTM) convolutional layers and Attention Gate blocks. The results, although originated in the context of surgical tissue retraction, could benefit many autonomous tasks such as ablation, suturing and debridement.},
  issn = {2576-3202},
}

@article{Munawar2021,
  author = {Munawar, Adnan and Wu, Jie Ying and Taylor, Russell H. and Kazanzides, Peter and Fischer, Gregory S.},
  title = {A Framework for Customizable Multi-User Teleoperated Control},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {2},
  month = {apr},
  doi = {10.1109/LRA.2021.3062604},
  semanticscholar = {https://www.semanticscholar.org/paper/97e6cc631d17f7c7132e24b974b1cb3c7611cc87},
  research_field = {HW},
  data_type = {RI and KD and DD and SD and ED},
  dvrk_site = {JHU},
  abstract = {Traditional teleoperation (leader/follower) systems primarily focus on one operator controlling one remote robot, but as robots become ubiquitous, there is an increasing need for multiple operators, including autonomous agents, to collaboratively control multiple robots. However, existing teleoperation frameworks do not inherently support the variety of possible collaborations, such as multiple operators, each with an input device (leader), controlling a robot and camera or different degrees of freedom of a single robot (follower). The same concept applies to teleoperating robots in a simulation environment through physical input devices. In this letter, we extend our novel simulation framework that is capable of incorporating multiple input devices asynchronously with a real-time dynamic simulation to incorporate a customizable shared control. For this purpose, we have identified and implemented a sufficient set of coordinate frames to encapsulate the pairing of multiple leaders, followers and cameras in a shared asynchronous manner with force feedback. We demonstrate the utility of this framework in accelerating user training, ease of learning, and enhanced task completion times through shared control by a supervisor.},
  issn = {2377-3766},
}

@article{Gao2021AHL,
  author = {Qian Gao and Ning Tan and Zhenglong Sun},
  title = {A hybrid learning‐based hysteresis compensation strategy for surgical robots},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  booktitle = {The international journal of medical robotics + computer assisted surgery : MRCAS},
  year = {2021},
  volume = {17},
  doi = {10.1002/rcs.2489},
  semanticscholar = {https://www.semanticscholar.org/paper/2e30f3b395f00a583cf20d167d80dc97e394ec95},
  abstract = {Computer‐assisted Surgery system (CAS) is an effective medical imaging simulation tool, which is widely used in preoperative planning of surgery. The objective of this study is to investigate the clinical application of CAS in pediatric mediastinal tumor resection.},
}

@article{Cai2021,
  author = {Cai, Yuanpei and Choi, Pangfai and Hui, Chiu-Wai Vincent and Taylor, Russell and Au, Kwok Wai Samuel},
  title = {A Task Space Virtual Fixture Architecture for Tele-operated Surgical System with Slave Joint Limit Constraints},
  journal = {IEEE/ASME Transactions on Mechatronics},
  year = {2021},
  doi = {10.1109/TMECH.2021.3058174},
  semanticscholar = {https://www.semanticscholar.org/paper/f54d773b2bc0c8c58aec6b6c78df21c85ed9edc2},
  research_field = {},
  data_type = {},
  dvrk_site = {CUHK},
  abstract = {Unintended motion is one of the major causes of intraoperative injuries in teleoperated surgeries. Due to the large workspace discrepancy between master and slave manipulators, the tip of the slave may deviate from the intended master command without prior notice, when the slave is moved beyond its joint limits. Conventional solutions such as the constrained-optimization-based virtual fixture (VF) method suffer from nonintuitive tip motion management and unnatural haptics in high-dimensional systems. To this end, we propose a task space virtual fixture (TSVF) architecture to systematically address those issues by forbidden-region VF design. It decomposes the high-dimensional task space into low-dimensional task subspaces according to its inherent topology. In each subspace, we design a human-centric TSVF geometry and controller to manage the tip behavior by exploiting the nonlinear kinematics mapping. This architecture builds a real-time TSVF system with natural and predictable haptics. To showcase this concept, we design and implement the proposed TSVF system for the state-of-the-art surgical system, da Vinci Research Kit. Simulations, experiments, and human-factor user study verify its effectiveness and intuitiveness. In the user study, our proposed TSVF system demonstrates the most easy-to-understand tip behavior and shows a significant positive effect over haptics likeability.},
  issn = {1083-4435},
}

@article{DEttorre2021,
  author = {D’Ettorre, Claudia and Mariani, Andrea and Stilli, Agostino and Rodriguez y Baena, Ferdinando and Valdastri, Pietro and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H. and Fischer, Gregory S. and DiMaio, Simon P. and Menciassi, Arianna and Stoyanov, Danail},
  title = {Accelerating Surgical Robotics Research: A Review of 10 Years With the da Vinci Research Kit},
  journal = {IEEE Robotics & Automation Magazine},
  year = {2021},
  volume = {28},
  number = {4},
  pages = {56-78},
  doi = {10.1109/MRA.2021.3101646},
  ieeexplore = {https://ieeexplore.ieee.org/document/9531355},
  research_field = {RE},
  keywords = {Robots;Surgery;Automation;Cameras;Robot vision systems;Instruments;Tools},
}

@article{Loschi2021AnOT,
  author = {Filippo Loschi and Nicola Piccinelli and D. Dall’Alba and R. Muradore and P. Fiorini and C. Secchi},
  title = {An Optimized Two-Layer Approach for Efficient and Robustly Stable Bilateral Teleoperation},
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2021},
  pages = {12449-12455},
}

@article{d2021autonomous,
  author = {D’Ettorre, Claudia and Stilli, Agostino and Dwyer, George and Tran, Maxine and Stoyanov, Danail},
  title = {Autonomous pick-and-place using the dVRK},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2021},
  volume = {16},
  number = {7},
  pages = {1141--1149},
  publisher = {Springer},
  doi = {10.1007/s11548-021-02397-y},
  semanticscholar = {https://www.semanticscholar.org/paper/3f2b07fd67179c4181a096f245248696a510db43},
  abstract = {Robotic-assisted partial nephrectomy (RAPN) is a tissue-preserving approach to treating renal cancer, where ultrasound (US) imaging is used for intra-operative identification of tumour margins and localisation of blood vessels. With the da Vinci Surgical System (Sunnyvale, CA), the US probe is inserted through an auxiliary access port, grasped by the robotic tool and moved over the surface of the kidney. Images from US probe are displayed separately to the surgical site video within the surgical console leaving the surgeon to interpret and co-registers information which is challenging and complicates the procedural workflow. We introduce a novel software architecture to support a hardware soft robotic rail designed to automate intra-operative US acquisition. As a preliminary step towards complete task automation, we automatically grasp the rail and position it on the tissue surface so that the surgeon is then able to manipulate manually the US probe along it. A preliminary clinical study, involving five surgeons, was carried out to evaluate the potential performance of the system. Results indicate that the proposed semi-autonomous approach reduced the time needed to complete a US scan compared to manual tele-operation. Procedural automation can be an important workflow enhancement functionality in future robotic surgery systems. We have shown a preliminary study on semi-autonomous US imaging, and this could support more efficient data acquisition.},
}

@inproceedings{meli2021autonomous,
  author = {Meli, Daniele and Tagliabue, Eleonora and Dall’Alba, Diego and Fiorini, Paolo},
  title = {Autonomous tissue retraction with a biomechanically informed logic based framework},
  booktitle = {2021 International Symposium on Medical Robotics (ISMR)},
  year = {2021},
  pages = {1--7},
  doi = {10.1109/ismr48346.2021.9661573},
  semanticscholar = {https://www.semanticscholar.org/paper/d5db8a867ed9c47287580b7db2ec1b276031b143},
  arxiv = {https://arxiv.org/abs/2109.02316},
  research_field = {AU and SS},
  data_type = {KD},
  abstract = {Autonomy in robot-assisted surgery is essential to reduce surgeons’ cognitive load and eventually improve the overall surgical outcome. A key requirement for autonomy in a safety-critical scenario as surgery lies in the generation of interpretable plans that rely on expert knowledge. Moreover, the Autonomous Robotic Surgical System (ARSS) must be able to reason on the dynamic and unpredictable anatomical environment, and quickly adapt the surgical plan in case of unexpected situations. In this paper, we present a modular Framework for Robot-Assisted Surgery (FRAS) in deformable anatomical environments. Our framework integrates a logic module for task-level interpretable reasoning, a biomechanical simulation that complements data from real sensors, and a situation awareness module for context interpretation. The framework performance is evaluated on simulated soft tissue retraction, a common surgical task to remove the tissue hiding a region of interest. Results show that the framework has the adaptability required to successfully accomplish the task, handling dynamic environmental conditions and possible failures, while guaranteeing the computational efficiency required in a real surgical scenario. The framework is made publicly available.},
  organization = {IEEE},
}

@article{Wu2021,
  author = {Wu, Jie Ying and Tamhane, Aniruddha and Kazanzides, Peter and Unberath, Mathias},
  title = {Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2021},
  month = {mar},
  doi = {10.1007/s11548-021-02343-y},
  semanticscholar = {https://www.semanticscholar.org/paper/0093de51220636684b9bd511ddd3946a46836ef7},
  research_field = {TR},
  data_type = {RI and KD},
  dvrk_site = {JHU},
  issn = {1861-6410},
}

@article{Tagliabue2021,
  author = {Tagliabue, Eleonora and Dall'Alba, Diego and Pfeiffer, Micha and Piccinelli, Marco and Marin, Riccardo and Castellani, Umberto and Speidel, Stefanie and Fiorini, Paolo},
  title = {Data-Driven Intra-Operative Estimation of Anatomical Attachments for Autonomous Tissue Dissection},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {2},
  month = {apr},
  doi = {10.1109/LRA.2021.3060655},
  semanticscholar = {https://www.semanticscholar.org/paper/68c946bf268de5b03f05b356dbf97ef2c23916d2},
  research_field = {AU},
  data_type = {RI and KD and ED},
  dvrk_site = {UV},
  abstract = {The execution of surgical tasks by an Autonomous Robotic System (ARS) requires an up-to-date model of the current surgical environment, which has to be deduced from measurements collected during task execution. In this work, we propose to automate tissue dissection tasks by introducing a convolutional neural network, called BA-Net, to predict the location of attachment points between adjacent tissues. BA-Net identifies the attachment areas from a single partial view of the deformed surface, without any a-priori knowledge about their location. The proposed method guarantees a very fast prediction time, which makes it ideal for intra-operative applications. Experimental validation is carried out on both simulated and real world phantom data of soft tissue manipulation performed with the da Vinci Research Kit (dVRK). The obtained results demonstrate that BA-Net provides robust predictions at varying geometric configurations, material properties, distributions of attachment points and grasping point locations. The estimation of attachment points provided by BA-Net improves the simulation of the anatomical environment where the system is acting, leading to a median simulation error below 5 mm in all the tested conditions. BA-Net can thus further support an ARS by providing a more robust test bench for the robotic actions intra-operatively, in particular when replanning is needed. The method and collected dataset are available at https://gitlab.com/altairLab/banet.},
  issn = {2377-3766},
}

@article{Yasin2021,
  author = {Yasin, Rashid and Chalasani, Preetham and Zevallos, Nicolas and Shahbazi, Mahya and Li, Zhaoshuo and Deguet, Anton and Kazanzides, Peter and Choset, Howie and Taylor, Russell H and Simaan, Nabil},
  title = {Evaluation of Hybrid Control and Palpation Assistance for Situational Awareness in Telemanipulated Task Execution},
  journal = {IEEE Trans. on Medical Robotics and Bionics},
  year = {2021},
  volume = {3},
  number = {1},
  pages = {31-43},
  doi = {10.1109/TMRB.2020.3042992},
  semanticscholar = {https://www.semanticscholar.org/paper/e064660d114c01271825555a2f1224a9faef071c},
  abstract = {The use of intelligent feedback modalities to control and react to interaction forces during surgical procedures is an important factor in enabling safe and precise surgery. We explore the use of a model-mediated telemanipulation framework to enhance a user’s situational awareness using assistive virtual fixtures and semi-automated task execution for safe and intuitive environment interaction during robotic laparoscopic surgery. The framework allows stiffness mapping with semi-autonomous excitation, hybrid position-force control, and model updates during soft geometry contact. A 24-person study was carried out at 3 sites in simulated ablation and palpation of phantom anatomy. Compared to methods lacking intelligent feedback and guidance, the proposed framework improved task execution metrics (force regulation, completion time, path-following error) and reduced user effort.},
  keywords = {dvrk},
  date = {2021-02-01},
  pubstate = {published},
  tppubtype = {article},
}

@inproceedings{9661536,
  author = {Richter, Florian and Funk, Emily K. and Seo Park, Won and Orosco, Ryan K. and Yip, Michael C.},
  title = {From Bench to Bedside: The First Live Robotic Surgery on the dVRK to Enable Remote Telesurgery with Motion Scaling},
  booktitle = {2021 International Symposium on Medical Robotics (ISMR)},
  year = {2021},
  volume = {},
  number = {},
  pages = {1-7},
  doi = {10.1109/ISMR48346.2021.9661536},
  ieeexplore = {https://ieeexplore.ieee.org/abstract/document/9661536},
  semanticscholar = {https://www.semanticscholar.org/paper/2ab58942e069700e6fe11dc4a3fdc15bf4c02412},
  arxiv = {https://arxiv.org/abs/2109.12177},
  research_field = {TR},
  abstract = {Innovations from surgical robotic research rarely translates to live surgery due to the significant difference between the lab and a live environment. Live environments require considerations that are often overlooked during early stages of research such as surgical staff, surgical procedure, and the challenges of working with live tissue. One such example is the da Vinci Research Kit (dVRK) which is used by over 40 robotics research groups and represents an open-sourced version of the da Vinci ® Surgical System. Despite dVRK being available for nearly a decade and the ideal candidate for translating research to practice on over 5,000 da Vinci ® Systems used in hospitals around the world, not one live surgery has been conducted with it. In this paper, we address the challenges, considerations, and solutions for translating surgical robotic research from bench-to-bedside. This is explained from the perspective of a remote telesurgery scenario where motion scaling solutions previously experimented in a lab setting are translated to a live pig surgery. This study presents results from the first ever use of a dVRK in a live animal and discusses how the surgical robotics community can approach translating their research to practice.},
  keywords = {Technological innovation;Translational research;Hospitals;Animals;Surgery;Robots},
}

@article{Gonzalez2021,
  author = {Gonzalez, Glebys T. and Kaur, Upinder and Rahma, Masudur and Venkatesh, Vishnunandan and Sanchez, Natalia and Hager, Gregory and Xue, Yexiang and Voyles, Richard and Wachs, Juan},
  title = {From the Dexterous Surgical Skill to the Battlefield - A Robotics Exploratory Study},
  journal = {Military Medicine},
  year = {2021},
  volume = {186},
  doi = {10.1093/milmed/usaa253},
  semanticscholar = {https://www.semanticscholar.org/paper/2a9bc46c1bcaf00950bd768038236a02b35986c6},
  research_field = {},
  data_type = {},
  abstract = {INTRODUCTION Short response time is critical for future military medical operations in austere settings or remote areas. Such effective patient care at the point of injury can greatly benefit from the integration of semi-autonomous robotic systems. To achieve autonomy, robots would require massive libraries of maneuvers collected with the goal of training machine learning algorithms. Although this is attainable in controlled settings, obtaining surgical data in austere settings can be difficult. Hence, in this article, we present the Dexterous Surgical Skill (DESK) database for knowledge transfer between robots. The peg transfer task was selected as it is one of the six main tasks of laparoscopic training. In addition, we provide a machine learning framework to evaluate novel transfer learning methodologies on this database. METHODS A set of surgical gestures was collected for a peg transfer task, composed of seven atomic maneuvers referred to as surgemes. The collected Dexterous Surgical Skill dataset comprises a set of surgical robotic skills using the four robotic platforms: Taurus II, simulated Taurus II, YuMi, and the da Vinci Research Kit. Then, we explored two different learning scenarios: no-transfer and domain-transfer. In the no-transfer scenario, the training and testing data were obtained from the same domain; whereas in the domain-transfer scenario, the training data are a blend of simulated and real robot data, which are tested on a real robot. RESULTS Using simulation data to train the learning algorithms enhances the performance on the real robot where limited or no real data are available. The transfer model showed an accuracy of 81% for the YuMi robot when the ratio of real-tosimulated data were 22% to 78%. For the Taurus II and the da Vinci, the model showed an accuracy of 97.5% and 93%, respectively, training only with simulation data. CONCLUSIONS The results indicate that simulation can be used to augment training data to enhance the performance of learned models in real scenarios. This shows potential for the future use of surgical data from the operating room in deployable surgical robots in remote areas.},
}

@article{VanAmsterdam2021,
  author = {van Amsterdam, Beatrice and Clarkson, Matthew J. and Stoyanov, Danail},
  title = {Gesture Recognition in Robotic Surgery: A Review},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2021},
  volume = {68},
  number = {6},
  month = {jun},
  doi = {10.1109/TBME.2021.3054828},
  semanticscholar = {https://www.semanticscholar.org/paper/9bd76023b5dae31eb29eb34b414d6fe8799dfcc3},
  arxiv = {https://arxiv.org/abs/2102.00027},
  research_field = {RE},
  data_type = {},
  abstract = {Objective: Surgical activity recognition is a fundamental step in computer-assisted interventions. This paper reviews the state-of-the-art in methods for automatic recognition of fine-grained gestures in robotic surgery focusing on recent data-driven approaches and outlines the open questions and future research directions. Methods: An article search was performed on 5 bibliographic databases with the following search terms: robotic, robot-assisted, JIGSAWS, surgery, surgical, gesture, fine-grained, surgeme, action, trajectory, segmentation, recognition, parsing. Selected articles were classified based on the level of supervision required for training and divided into different groups representing major frameworks for time series analysis and data modelling. Results: A total of 52 articles were reviewed. The research field is showing rapid expansion, with the majority of articles published in the last 4 years. Deep-learning-based temporal models with discriminative feature extraction and multi-modal data integration have demonstrated promising results on small surgical datasets. Currently, unsupervised methods perform significantly less well than the supervised approaches. Conclusion: The development of large and diverse open-source datasets of annotated demonstrations is essential for development and validation of robust solutions for surgical gesture recognition. While new strategies for discriminative feature extraction and knowledge transfer, or unsupervised and semi-supervised approaches, can mitigate the need for data and labels, they have not yet been demonstrated to achieve comparable performance. Important future research directions include detection and forecast of gesture-specific errors and anomalies. Significance: This paper is a comprehensive and structured analysis of surgical gesture recognition methods aiming to summarize the status of this rapidly evolving field.},
  issn = {0018-9294},
}

@article{Gultekin2021,
  author = {G{\"{u}}ltekin, İsmail Burak and Karab{\"{u}}k, Emine and K{\"{o}}se, Mehmet Faruk and G{\"{u}}ltekin, İsmail Burak and Karab{\"{u}}k, Emine and K{\"{o}}se, Mehmet Faruk},
  title = {Hey Siri! Perform a type 3 hysterectomy. Please watch out for the ureter!” What is autonomous surgery and what are the latest developments?},
  journal = {Journal of the Turkish-German Gynecological Association},
  year = {2021},
  volume = {22},
  number = {1},
  month = {mar},
  doi = {10.4274/jtgga.galenos.2021.2020.0187},
  semanticscholar = {https://www.semanticscholar.org/paper/dd0f4deb0e21dedd80766908b57a7e709326e37f},
  research_field = {},
  data_type = {},
  abstract = {As a result of major advances in deep learning algorithms and computer processing power, there have been important developments in the fields of medicine and robotics. Although fully autonomous surgery systems where human impact will be minimized are still a long way off, systems with partial autonomy have gradually entered clinical use. In this review, articles on autonomous surgery classified and summarized, with the aim of informing the reader about questions such as “What is autonomic surgery?” and in which areas studies are progressing.},
  issn = {1309-0399},
}

@article{Lin2021,
  author = {Lin, Hongbin and Gao, Qian and Chu, Xiangyu and Dou, Qi and Deguet, Anton and Kazanzides, Peter and Au, K. W. Samuel},
  title = {Learning Deep Nets for Gravitational Dynamics With Unknown Disturbance Through Physical Knowledge Distillation: Initial Feasibility Study},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {2},
  month = {apr},
  doi = {10.1109/LRA.2021.3062351},
  semanticscholar = {https://www.semanticscholar.org/paper/168c2a83ce165501c7fc79e921d82ab2edee55ea},
  arxiv = {https://arxiv.org/abs/2210.01398},
  research_field = {},
  data_type = {},
  dvrk_site = {CUHK},
  abstract = {Learning high-performance deep neural networks for dynamic modeling of high Degree-Of-Freedom (DOF) robots remains challenging due to the sampling complexity. Typical unknown system disturbance caused by unmodeled dynamics (such as internal compliance, cables) further exacerbates the problem. In this letter, a novel framework characterized by both high data efficiency and disturbance-adapting capability is proposed to address the problem of modeling gravitational dynamics using deep nets in feedforward gravity compensation control for high-DOF master manipulators with unknown disturbance. In particular, Feedforward Deep Neural Networks (FDNNs) are learned from both prior knowledge of an existing analytical model and observation of the robot system by Knowledge Distillation (KD). Through extensive experiments in high-DOF master manipulators with significant disturbance, we show that our method surpasses a standard Learning-from-Scratch (LfS) approach in terms of data efficiency and disturbance adaptation. Our initial feasibility study has demonstrated the potential of outperforming the analytical teacher model as the training data increases.},
  issn = {2377-3766},
}

@article{Ghalamzan2021,
  author = {Ghalamzan, Amir},
  title = {Learning needle insertion from sample task executions},
  journal = {arXiv preprint},
  year = {2021},
  month = {mar},
  url = {http://arxiv.org/abs/2103.07938},
  semanticscholar = {https://www.semanticscholar.org/paper/c9f4a09d5e30010bd08f8e8f0f4e98705b827bad},
  arxiv = {https://arxiv.org/abs/2103.07938},
  research_field = {},
  data_type = {},
  dvrk_site = {CUHK},
  abstract = {Automating a robotic task, e.g., robotic suturing can be very complex and time-consuming. Learning a task model to autonomously perform the task is invaluable making the technology, robotic surgery, accessible for a wider community. The data of robotic surgery can be easily logged where the collected data can be used to learn task models. This will result in reduced time and cost of robotic surgery in which a surgeon can supervise the robot operation or give high-level commands instead of low-level control of the tools. We present a data-set of needle insertion in soft tissue with two arms where Arm 1 inserts the needle into the tissue and Arm 2 actively manipulate the soft tissue to ensure the desired and actual exit points are the same. This is important in real-surgery because suturing without active manipulation of tissue may yield failure of the suturing as the stitch may not grip enough tissue to resist the force applied for the suturing. We present a needle insertion dataset including 60 successful trials recorded by 3 pair of stereo cameras. Moreover, we present Deep-robot Learning from Demonstrations that predicts the desired state of the robot at the time step after t (which the optimal action taken at t yields) by looking at the video of the past time steps, i.e. n step time history where N is the memory time window, of the task execution. The experimental results illustrate our proposed deep model architecture is outperforming the existing methods. Although the solution is not yet ready to be deployed on a real robot, the results indicate the possibility of future development for real robot deployment.},
  archiveprefix = {arXiv},
  arxivid = {2103.07938},
  eprint = {2103.07938},
}

@article{Wilcox2021LearningTL,
  author = {Albert Wilcox and J. Kerr and Brijen Thananjeyan and Jeffrey Ichnowski and M. Hwang and Samuel Paradis and Danyal M. Fer and Ken Goldberg},
  title = {Learning to Localize, Grasp, and Hand Over Unmodified Surgical Needles},
  journal = {2022 International Conference on Robotics and Automation (ICRA)},
  booktitle = {IEEE International Conference on Robotics and Automation},
  year = {2021},
  pages = {9637-9643},
}

@inproceedings{Fu2021,
  author = {Fu, Guanhao and Azimi, Ehsan and Kazanzides, Peter},
  title = {Mobile Teleoperation: Evaluation of Wireless Wearable Sensing of the Operator's Arm Motion},
  booktitle = {arXiv preprint},
  year = {2021},
  doi = {10.1145/3613904.3642858},
  url = {https://arxiv.org/abs/2103.08119},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  arxiv = {https://arxiv.org/abs/2310.09985},
  research_field = {},
  data_type = {},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
  archiveprefix = {arXiv},
  arxivid = {2103.08119},
  eprint = {2103.08119},
}

@inproceedings{Kohlgrueber2021,
  author = {Kohlgrueber, Stefan and Kim, Yeongmi and Kazanzides, Peter},
  title = {Model-based Design and Digital Implementation to Improve Control of the da Vinci Research Kit Telerobotic Surgical System},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2021},
  pages = {12435-12441},
  doi = {10.1109/ICRA48506.2021.9560842},
  semanticscholar = {https://www.semanticscholar.org/paper/2d9b66a6a63df458183b00e663bb476ff06238ce},
  abstract = {The da Vinci Research Kit (dVRK) was introduced in 2012 to provide an affordable, open-source platform for research in robotic minimally-invasive surgery. It provides access to all levels of control but, until now, has relied on an analog controller for the motor current, which cannot easily be customized to improve performance. This paper aims to implement the low-level control digitally and to improve the overall control performance. To enable model-based controller design, the system is first identified using measurements provided by the encoders and internal electronics. The digital current controller is then implemented on the existing field programmable gate array (FPGA). Experiments demonstrate that the new digital current controller yields superior performance compared to the original analog design. In addition, the identified system model is used to design an improved position controller that is also implemented on the FPGA and provides better trajectory tracking than the position controller currently implemented on the control PC. The comparison between simulation and measurement, for both the current and position control, verifies the validity of the model based on the system identification, enabling utilization for future adaptations. The improved low-level control enlarges the possibilities for more accurate operation and the achieved digital implementation enables researchers worldwide to easily adapt the low-level control in future versions of the dVRK.},
  keywords = {dvrk},
  date = {2021-06-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2021-06-01},
}

@article{Huang2021a,
  author = {Huang, Jingbin and Liu, Fei and Richter, Florian and Yip, Michael C.},
  title = {Model-Predictive Control of Blood Suction for Surgical Hemostasis using Differentiable Fluid Simulations},
  journal = {arXiv preprint},
  year = {2021},
  doi = {10.1109/ICRA48506.2021.9561624},
  semanticscholar = {https://www.semanticscholar.org/paper/ae7034217bb850a0989aef5745d1269e24cd7726},
  arxiv = {https://arxiv.org/abs/2102.01436},
  research_field = {},
  data_type = {},
  dvrk_site = {UCSD},
  abstract = {Recent developments in surgical robotics have led to new advancements in the automation of surgical sub-tasks such as suturing, soft tissue manipulation, tissue tensioning and cutting. However, integration of dynamics to optimize these control policies for the variety of scenes encountered in surgery remains unsolved. Towards this effort, we investigate the integration of differentiable fluid dynamics to optimizing a suction tool’s trajectory to clear the surgical field from blood as fast as possible. The fully differentiable fluid dynamics is integrated with a novel suction model for effective model predictive control of the tool. The differentiability of the fluid model is crucial because we utilize the gradients of the fluid states with respect to the suction tool position to optimize the trajectory. Through a series of experiments, we demonstrate how, by incorporating fluid models, the trajectories generated by our method can perform as good as or better than handcrafted human-intuitive suction policies. We also show that our method is adaptable and can work in different cavity conditions while using a single handcrafted strategy fails.},
}

@article{Huang2021b,
  author = {Huang, Jingbin and Liu, Fei and Richter, Florian and Yip, Michael C.},
  title = {Model-Predictive Control of Blood Suction for Surgical Hemostasis using Differentiable Fluid Simulations},
  journal = {arXiv preprint},
  year = {2021},
  month = {feb},
  doi = {10.1109/ICRA48506.2021.9561624},
  url = {https://arxiv.org/abs/2102.01436},
  semanticscholar = {https://www.semanticscholar.org/paper/ae7034217bb850a0989aef5745d1269e24cd7726},
  arxiv = {https://arxiv.org/abs/2102.01436},
  research_field = {AU},
  data_type = {RI and KD and DD and SD},
  dvrk_site = {UCSD},
  abstract = {Recent developments in surgical robotics have led to new advancements in the automation of surgical sub-tasks such as suturing, soft tissue manipulation, tissue tensioning and cutting. However, integration of dynamics to optimize these control policies for the variety of scenes encountered in surgery remains unsolved. Towards this effort, we investigate the integration of differentiable fluid dynamics to optimizing a suction tool’s trajectory to clear the surgical field from blood as fast as possible. The fully differentiable fluid dynamics is integrated with a novel suction model for effective model predictive control of the tool. The differentiability of the fluid model is crucial because we utilize the gradients of the fluid states with respect to the suction tool position to optimize the trajectory. Through a series of experiments, we demonstrate how, by incorporating fluid models, the trajectories generated by our method can perform as good as or better than handcrafted human-intuitive suction policies. We also show that our method is adaptable and can work in different cavity conditions while using a single handcrafted strategy fails.},
  archiveprefix = {arXiv},
  arxivid = {2102.01436},
  eprint = {2102.01436},
}

@article{Caccianiga2021,
  author = {Caccianiga, Guido and Mariani, Andrea and {Galli de Paratesi}, Chiara and Menciassi, Arianna and {De Momi}, Elena},
  title = {Multi-sensory Guidance and Feedback for Simulation-based Training in Robot Assisted Surgery: a Preliminary Comparison of Visual, Haptic, and Visuo-Haptic},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  doi = {10.1109/LRA.2021.3063967},
  semanticscholar = {https://www.semanticscholar.org/paper/a63da0819790a9cb542321bcbef5f63c16443701},
  research_field = {AU},
  data_type = {RI and KD},
  dvrk_site = {POLIMI},
  abstract = {Nowadays, robot assisted surgery training relies more and more on computer-based simulation. However, the application of such training technologies is still limited to the early stages of practical training. To broaden the usefulness of simulators, multi-sensory feedback augmentation has been recently investigated. This study aims at combining initial predictive (guidance) and subsequent error-based (feedback) training augmentation in the visual and haptic domain. 32 participants performed 30 repetitions of a virtual reality task resembling needle-driving by using the surgeon console of the da Vinci Research Kit. These trainees were randomly and equally divided into four groups: one group had no training augmentation, while the other groups underwent visual, haptic and visuo-haptic augmentation, respectively. Results showed a significant improvement, initially introduced by guidance, in the task completion capabilities of all the experimental groups against control. In terms of accuracy, the experimental groups outperformed the control group at the end of training. Specifically, visual guidance and haptic feedback played a significant role in error reduction. Further investigations on long term learning could better delineate the optimal combination of guidance and feedback in these sensory domains.},
  issn = {2377-3766},
}

@article{NagyneElek2021,
  author = {{Nagyn{\'{e}} Elek}, Ren{\'{a}}ta and Haidegger, Tam{\'{a}}s},
  title = {Non-Technical Skill Assessment and Mental Load Evaluation in Robot-Assisted Minimally Invasive Surgery},
  journal = {Sensors},
  year = {2021},
  volume = {21},
  number = {8},
  month = {apr},
  doi = {10.3390/s21082666},
  semanticscholar = {https://www.semanticscholar.org/paper/326ea42ae3b6614b0926c24d83913717f13c16a7},
  research_field = {},
  data_type = {},
  dvrk_site = {OU},
  abstract = {BACKGROUND: Sensor technologies and data collection practices are changing and improving quality metrics across various domains. Surgical skill assessment in Robot-Assisted Minimally Invasive Surgery (RAMIS) is essential for training and quality assurance. The mental workload on the surgeon (such as time criticality, task complexity, distractions) and non-technical surgical skills (including situational awareness, decision making, stress resilience, communication, leadership) may directly influence the clinical outcome of the surgery. METHODS: A literature search in PubMed, Scopus and PsycNet databases was conducted for relevant scientific publications. The standard PRISMA method was followed to filter the search results, including non-technical skill assessment and mental/cognitive load and workload estimation in RAMIS. Publications related to traditional manual Minimally Invasive Surgery were excluded, and also the usability studies on the surgical tools were not assessed. RESULTS: 50 relevant publications were identified for non-technical skill assessment and mental load and workload estimation in the domain of RAMIS. The identified assessment techniques ranged from self-rating questionnaires and expert ratings to autonomous techniques, citing their most important benefits and disadvantages. CONCLUSIONS: Despite the systematic research, only a limited number of articles was found, indicating that non-technical skill and mental load assessment in RAMIS is not a well-studied area. Workload assessment and soft skill measurement do not constitute part of the regular clinical training and practice yet. Meanwhile, the importance of the research domain is clear based on the publicly available surgical error statistics. Questionnaires and expert-rating techniques are widely employed in traditional surgical skill assessment; nevertheless, recent technological development in sensors and Internet of Things-type devices show that skill assessment approaches in RAMIS can be much more profound employing automated solutions. Measurements and especially big data type analysis may introduce more objectivity and transparency to this critical domain as well. SIGNIFICANCE: Non-technical skill assessment and mental load evaluation in Robot-Assisted Minimally Invasive Surgery is not a well-studied area yet; while the importance of this domain from the clinical outcome’s point of view is clearly indicated by the available surgical error statistics.},
  issn = {1424-8220},
}

@article{NagyneElek2021a,
  author = {{Nagyn{\'{e}} Elek}, Ren{\'{a}}ta and Haidegger, Tam{\'{a}}s},
  title = {Non-Technical Skill Assessment and Mental Load Evaluation in Robot-Assisted Minimally Invasive Surgery},
  journal = {Sensors},
  year = {2021},
  volume = {21},
  number = {8},
  month = {apr},
  doi = {10.3390/s21082666},
  semanticscholar = {https://www.semanticscholar.org/paper/326ea42ae3b6614b0926c24d83913717f13c16a7},
  research_field = {TR},
  data_type = {ED},
  dvrk_site = {OU},
  abstract = {BACKGROUND: Sensor technologies and data collection practices are changing and improving quality metrics across various domains. Surgical skill assessment in Robot-Assisted Minimally Invasive Surgery (RAMIS) is essential for training and quality assurance. The mental workload on the surgeon (such as time criticality, task complexity, distractions) and non-technical surgical skills (including situational awareness, decision making, stress resilience, communication, leadership) may directly influence the clinical outcome of the surgery. METHODS: A literature search in PubMed, Scopus and PsycNet databases was conducted for relevant scientific publications. The standard PRISMA method was followed to filter the search results, including non-technical skill assessment and mental/cognitive load and workload estimation in RAMIS. Publications related to traditional manual Minimally Invasive Surgery were excluded, and also the usability studies on the surgical tools were not assessed. RESULTS: 50 relevant publications were identified for non-technical skill assessment and mental load and workload estimation in the domain of RAMIS. The identified assessment techniques ranged from self-rating questionnaires and expert ratings to autonomous techniques, citing their most important benefits and disadvantages. CONCLUSIONS: Despite the systematic research, only a limited number of articles was found, indicating that non-technical skill and mental load assessment in RAMIS is not a well-studied area. Workload assessment and soft skill measurement do not constitute part of the regular clinical training and practice yet. Meanwhile, the importance of the research domain is clear based on the publicly available surgical error statistics. Questionnaires and expert-rating techniques are widely employed in traditional surgical skill assessment; nevertheless, recent technological development in sensors and Internet of Things-type devices show that skill assessment approaches in RAMIS can be much more profound employing automated solutions. Measurements and especially big data type analysis may introduce more objectivity and transparency to this critical domain as well. SIGNIFICANCE: Non-technical skill assessment and mental load evaluation in Robot-Assisted Minimally Invasive Surgery is not a well-studied area yet; while the importance of this domain from the clinical outcome’s point of view is clearly indicated by the available surgical error statistics.},
  issn = {1424-8220},
}

@inproceedings{Zhao2021,
  author = {Zhao, Zixu and Jin, Yueming and Lu, Bo and Ng, Chi-Fai and Dou, Qi and Liu, Yun-Hui and Heng, Pheng-Ann},
  title = {One to Many: Adaptive Instrument Segmentation via Meta Learning and Dynamic Online Adaptation in Robotic Surgical Video},
  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2021},
  month = {mar},
  doi = {10.1109/icra48506.2021},
  url = {http://arxiv.org/abs/2103.12988},
  semanticscholar = {https://www.semanticscholar.org/paper/967af3930617d1ce227fb769400b57c63dc71aec},
  research_field = {IM},
  data_type = {RI},
  archiveprefix = {arXiv},
  arxivid = {2103.12988},
  eprint = {2103.12988},
}

@article{Di2021,
  author = {Di, James and Xu, Mingwei and Das, Nikhil and Yip, Michael C.},
  title = {Optimal Multi-Manipulator Arm Placement for Maximal Dexterity during Robotics Surgery},
  journal = {arXiv preprint},
  year = {2021},
  month = {apr},
  doi = {10.1109/ICRA48506.2021.9561570},
  url = {https://arxiv.org/abs/2104.06348},
  semanticscholar = {https://www.semanticscholar.org/paper/f258f65e531564ad74c358405a99843efdc358fe},
  arxiv = {https://arxiv.org/abs/2104.06348},
  research_field = {HW},
  data_type = {KD and ED},
  dvrk_site = {UCSD},
  abstract = {Robot arm placements are oftentimes a limitation in surgical preoperative procedures, relying on trained staff to evaluate and decide on the optimal positions for the arms. Given new and different patient anatomies, it can be challenging to make an informed choice, leading to more frequently colliding arms or limited manipulator workspaces. In this paper, we develop a method to generate the optimal manipulator base positions for the multi-port da Vinci surgical system that minimizes self-collision and environment-collision, and maximizes the surgeon’s reachability inside the patient. Scoring functions are defined for each criterion so that they may be optimized over. Since for multi-manipulator setups, a large number of free parameters are available to adjust the base positioning of each arm, a challenge becomes how one can expediently assess possible setups. We thus also propose methods that perform fast queries of each measure with the use of a proxy collision-checker. We then develop an optimization method to determine the optimal position using the scoring functions. We evaluate the optimality of the base positions for the robot arms on canonical trajectories, and show that the solution yielded by the optimization program can satisfy each criterion. The metrics and optimization strategy are generalizable to other surgical robotic platforms so that patient-side manipulator positioning may be optimized and solved.},
  archiveprefix = {arXiv},
  arxivid = {2104.06348},
  eprint = {2104.06348},
}

@article{Abdelaal2021,
  author = {Abdelaal, Alaa Eldin and Liu, Jordan and Hong, Nancy and Hager, Gregory D. and Salcudean, Septimiu E.},
  title = {Parallelism in Autonomous Robotic Surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {2},
  month = {apr},
  doi = {10.1109/LRA.2021.3060402},
  semanticscholar = {https://www.semanticscholar.org/paper/ba6b797b53310d8502512d19865322cc3be714d4},
  research_field = {AU},
  data_type = {RI and KD and ED},
  dvrk_site = {UBC},
  abstract = {Robots can perform multiple tasks in parallel. This work is about leveraging this capability in automating multilateral surgical subtasks. In particular, we explore, in a simulation study, the benefits of considering this parallelism capability in developing execution models for autonomous robotic surgery. We apply our work to two surgical subtask categories: (i) coupled-motion subtasks, where multiple robot arms share the same resources to perform the subtask, and (ii) decoupled-motion subtasks, where each robot arm executes its part of the task independently from the others. We propose and develop parallel execution models for the surgical debridement subtask, a representative of the first category, and the multi-throw suturing subtask, a representative of the second one. Comparing these parallel execution models to the state-of-the-art ones shows significant reductions in the subtasks completion time by at least 40%. In 20 trials, our results show that our proposed model for the surgical debridement subtask, that uses hierarchical concurrent state machines, provides a parallel execution framework that is efficient while greatly reducing collisions between the arms compared to a naive parallel execution model without coordination. We also show how applying parallelism can lead to execution models that go beyond the normal practice of human surgeons. We finally propose the notion of “automation for surgical manual execution” where we argue that autonomous robotic surgery research can be used as a tool for surgeons to discover novel manual execution models that can significantly improve their surgical practice.},
  issn = {2377-3766},
}

@article{sharon2021rate,
  author = {Sharon, Yarden and Jarc, Anthony M. and Lendvay, Thomas S. and Nisky, Ilana},
  title = {Rate of Orientation Change as a New Metric for Robot-Assisted and Open Surgical Skill Evaluation},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2021},
  volume = {3},
  number = {2},
  pages = {414-425},
  doi = {10.1109/TMRB.2021.3073209},
  semanticscholar = {https://www.semanticscholar.org/paper/ffbeb4ffd52539d5c8a8e6ce9dae794da1672e63},
  arxiv = {https://arxiv.org/abs/1709.09452},
  research_field = {TR},
  data_type = {RI and KD},
  dvrk_site = {BGUN},
  abstract = {Surgeons’ technical skill directly impacts patient outcomes. To date, the angular motion of the instruments has been largely overlooked in objective skill evaluation. To fill this gap, we have developed metrics for surgical skill evaluation that are based on the orientation of surgical instruments. We tested our new metrics on two datasets with different conditions: (1) a dataset of experienced robotic surgeons and nonmedical users performing needle-driving on a dry lab model, and (2) a small dataset of suturing movements performed by surgeons training on a porcine model. We evaluated the performance of our new metrics (angular displacement and the rate of orientation change) alongside the performances of classical metrics (task time and path length). We calculated each metric on different segments of the movement. Our results highlighted the importance of segmentation rather than calculating the metrics on the entire movement. Our new metric, the rate of orientation change, showed statistically significant differences between experienced surgeons and nonmedical users / novice surgeons, which were consistent with the classical task time metric. The rate of orientation change captures technical aspects that are taught during surgeons’ training, and together with classical metrics can lead to a more comprehensive discrimination of skills.},
}

@article{Sharon2021,
  author = {Sharon, Yarden and Jarc, Anthony M. and Lendvay, Thomas S. and Nisky, Ilana},
  title = {Rate of Orientation Change as a New Metric for Robot-Assisted and Open Surgical Skill Evaluation},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2021},
  doi = {10.1109/TMRB.2021.3073209},
  semanticscholar = {https://www.semanticscholar.org/paper/ffbeb4ffd52539d5c8a8e6ce9dae794da1672e63},
  arxiv = {https://arxiv.org/abs/1709.09452},
  research_field = {},
  data_type = {},
  dvrk_site = {BGUN},
  abstract = {Surgeons’ technical skill directly impacts patient outcomes. To date, the angular motion of the instruments has been largely overlooked in objective skill evaluation. To fill this gap, we have developed metrics for surgical skill evaluation that are based on the orientation of surgical instruments. We tested our new metrics on two datasets with different conditions: (1) a dataset of experienced robotic surgeons and nonmedical users performing needle-driving on a dry lab model, and (2) a small dataset of suturing movements performed by surgeons training on a porcine model. We evaluated the performance of our new metrics (angular displacement and the rate of orientation change) alongside the performances of classical metrics (task time and path length). We calculated each metric on different segments of the movement. Our results highlighted the importance of segmentation rather than calculating the metrics on the entire movement. Our new metric, the rate of orientation change, showed statistically significant differences between experienced surgeons and nonmedical users / novice surgeons, which were consistent with the classical task time metric. The rate of orientation change captures technical aspects that are taught during surgeons’ training, and together with classical metrics can lead to a more comprehensive discrimination of skills.},
  issn = {2576-3202},
}

@article{Thananjeyan2021,
  author = {Thananjeyan, Brijen and Balakrishna, Ashwin and Nair, Suraj and Luo, Michael and Srinivasan, Krishnan and Hwang, Minho and Gonzalez, Joseph E. and Ibarz, Julian and Finn, Chelsea and Goldberg, Ken},
  title = {Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {3},
  month = {jul},
  doi = {10.1109/LRA.2021.3070252},
  semanticscholar = {https://www.semanticscholar.org/paper/431dc05ac25510de6264084434254cca877f9ab3},
  arxiv = {https://arxiv.org/abs/2010.15920},
  research_field = {},
  data_type = {},
  dvrk_site = {UCB},
  abstract = {Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.},
  issn = {2377-3766},
}

@inproceedings{WuISMR2021a,
  author = {Wu, Jie Ying and Yilmaz, Nural and Tumerdem, Ugur and Kazanzides, Peter},
  title = {Robot Force Estimation with Learned Intraoperative Correction},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  year = {2021},
  doi = {10.1109/ISMR48346.2021.9661568},
  semanticscholar = {https://www.semanticscholar.org/paper/b15f228283837b5de87e3733ed3504c13b0af592},
  abstract = {Measurement of environment interaction forces during robotic minimally-invasive surgery would enable haptic feedback to the surgeon, thereby solving one long-standing limitation. Estimating this force from existing sensor data avoids the challenge of retrofitting systems with force sensors, but is difficult due to mechanical effects such as friction and compliance in the robot mechanism. We have previously shown that neural networks can be trained to estimate the internal robot joint torques, thereby enabling estimation of external forces on the da Vinci Research Kit (dVRK). In this work, we extend the method to estimate external Cartesian forces and torques, and also present a two-step approach to adapt to the specific surgical setup by compensating for forces due to the interactions between the instrument shaft and cannula seal and between the trocar and patient body. Experiments show that this approach provides estimates of external forces and torques within a mean root-mean-square error (RMSE) of 1.8N and 0.1Nm, respectively. Furthermore, the two-step approach can add as little as 5 minutes to the surgery setup time, with about 4 minutes to collect intraoperative training data and 1 minute to train the second-step network.},
  keywords = {dvrk, machine-learning},
  date = {2021-11-01},
  pubstate = {published},
  tppubtype = {inproceedings},
  urldate = {2021-11-01},
}

@article{Richter2021,
  author = {Richter, Florian and Lu, Jingpei and Orosco, Ryan K. and Yip, Michael C.},
  title = {Robotic Tool Tracking under Partially Visible Kinematic Chain: A Unified Approach},
  journal = {arXiv preprint},
  year = {2021},
  month = {feb},
  doi = {10.1109/tro.2021.3111441},
  url = {http://arxiv.org/abs/2102.06235},
  semanticscholar = {https://www.semanticscholar.org/paper/76882ba81760f9c40bd1ad0195ad7ecb1f2f361d},
  arxiv = {https://arxiv.org/abs/2102.06235},
  research_field = {IM},
  data_type = {RI and KD and ED},
  dvrk_site = {UCSD},
  abstract = {Anytime a robot manipulator is controlled via visual feedback, the transformation between the robot and camera frame must be known. However, in the case where cameras can only capture a portion of the robot manipulator in order to better perceive the environment being interacted with, there is greater sensitivity to errors in calibration of the base-to-camera transform. A secondary source of uncertainty during robotic control are inaccuracies in joint angle measurements which can be caused by biases in positioning and complex transmission effects such as backlash and cable stretch. In this work, we bring together these two sets of unknown parameters into a unified problem formulation when the kinematic chain is partially visible in the camera view. We prove that these parameters are nonidentifiable implying that explicit estimation of them is infeasible. To overcome this, we derive a smaller set of parameters we call lumped error since it lumps together the errors of calibration and joint angle measurements. A particle filter method is presented and tested in simulation and on two real world robots to estimate the lumped error and show the efficiency of this parameter reduction.},
  archiveprefix = {arXiv},
  arxivid = {2102.06235},
  eprint = {2102.06235},
}

@article{Barragan2021SACHETSSC,
  author = {Juan Antonio Barragan and Daniela Chanci and Denny Yu and J. Wachs},
  title = {SACHETS: Semi-Autonomous Cognitive Hybrid Emergency Teleoperated Suction},
  journal = {2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN)},
  booktitle = {IEEE International Symposium on Robot and Human Interactive Communication},
  year = {2021},
  pages = {1243-1248},
  semanticscholar = {https://www.semanticscholar.org/paper/f2063842143232152d8404b56ada4f66b955ec80},
}

@inproceedings{Hasan2021,
  author = {Hasan, S. M. Kamrul and Simon, Richard A. and Linte, Cristian A.},
  title = {Segmentation and removal of surgical instruments for background scene visualization from endoscopic/laparoscopic video},
  booktitle = {Medical Imaging 2021: Image-Guided Procedures, Robotic Interventions, and Modeling},
  year = {2021},
  month = {feb},
  publisher = {SPIE},
  doi = {10.1117/12.2580668},
  semanticscholar = {https://www.semanticscholar.org/paper/6eb535a14bf9e57ff5b7cf5b10348ae1921830da},
  research_field = {},
  data_type = {},
  abstract = {Surgical tool segmentation is becoming imperative to provide detailed information during intra-operative execution. These tools can obscure surgeons' dexterity control due to narrow working space and visual field-of-view, which increases the risk of complications resulting from tissue injuries (e.g. tissue scars and tears). This paper demonstrates a novel application of segmenting and removing surgical instruments from laparoscopic/endoscopic video using digital inpainting algorithms. To segment the surgical instruments, we use a modified U-Net architecture (U-NetPlus) composed of a pre-trained VGG11 or VGG16 encoder and redesigned decoder. The decoder is modified by replacing the transposed convolution operation with an up-sampling operation based on nearest-neighbor (NN) interpolation. This modification removes the artifacts generated by the transposed convolution, and, furthermore, these new interpolation weights require no learning for the upsampling operation. The tool removal algorithms use the tool segmentation mask and either the instrument-free reference frames or previous instrument-containing frames to fill-in (i.e., inpaint) the instrument segmentation mask with the background tissue underneath. We have demonstrated the performance of the proposed surgical tool segmentation/removal algorithms on a robotic instrument dataset from the MICCAI 2015 EndoVis Challenge. We also showed successful performance of the tool removal algorithm from synthetically generated surgical instruments-containing videos obtained by embedding a moving surgical tool into surgical tool-free videos. Our application successfully segments and removes the surgical tool to unveil the background tissue view otherwise obstructed by the tool, producing visually comparable results to the ground truth.},
  editor = {Linte, Cristian A. and Siewerdsen, Jeffrey H.},
  isbn = {9781510640252},
}

@inproceedings{xu2021surrol,
  author = {Xu, Jiaqi and Li, Bin and Lu, Bo and Liu, Yun-Hui and Dou, Qi and Heng, Pheng-Ann},
  title = {Surrol: An open-source reinforcement learning centered and dvrk compatible platform for surgical robot learning},
  booktitle = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2021},
  pages = {1821--1828},
  doi = {10.1109/IROS51168.2021.9635867},
  ieeexplore = {https://ieeexplore.ieee.org/abstract/document/9635867},
  semanticscholar = {https://www.semanticscholar.org/paper/6f4ca4a3fd6071787083d067cf420e468e930d62},
  arxiv = {https://arxiv.org/abs/2108.13035},
  research_field = {AU and SS and TR},
  abstract = {Autonomous surgical execution relieves tedious routines and surgeon’s fatigue. Recent learning-based methods, especially reinforcement learning (RL) based methods, achieve promising performance for dexterous manipulation, which usually requires the simulation to collect data efficiently and reduce the hardware cost. The existing learning-based simulation platforms for medical robots suffer from limited scenarios and simplified physical interactions, which degrades the real-world performance of learned policies. In this work, we designed SurRoL, an RL-centered simulation platform for surgical robot learning compatible with the da Vinci Research Kit (dVRK). The designed SurRoL integrates a user-friendly RL library for algorithm development and a real-time physics engine, which is able to support more PSM/ECM scenarios and more realistic physical interactions. Ten learning-based surgical tasks are built in the platform, which are common in the real autonomous surgical execution. We evaluate SurRoL using RL algorithms in simulation, provide in-depth analysis, deploy the trained policies on the real dVRK, and show that our SurRoL achieves better transferability in the real world.},
  organization = {IEEE},
}

@article{meli2021unsupervised,
  author = {Meli, Daniele and Fiorini, Paolo},
  title = {Unsupervised identification of surgical robotic actions from small non-homogeneous datasets},
  journal = {IEEE Robotics and Automation Letters},
  year = {2021},
  volume = {6},
  number = {4},
  pages = {8205--8212},
  publisher = {IEEE},
  doi = {10.1109/LRA.2021.3104880},
  semanticscholar = {https://www.semanticscholar.org/paper/a1e7eef4baf3a26d7128618c03238860309d242e},
  arxiv = {https://arxiv.org/abs/2105.08488},
  research_field = {TR},
  data_type = {RI and KD},
  abstract = {Robot-assisted surgery is an established clinical practice. The automatic identification of surgical actions is needed for a range of applications, including performance assessment of trainees and surgical process modeling for autonomous execution and monitoring. However, supervised action identification is not feasible, due to the burden of manually annotating recordings of potentially complex and long surgical executions. Moreover, often few example executions of a surgical procedure can be recorded. This letter proposes a novel fast algorithm for unsupervised identification of surgical actions in a standard surgical training task, the ring transfer, executed with da Vinci Research Kit. Exploiting kinematic and semantic visual features automatically extracted from a very limited dataset of executions, we are able to significantly outperform state-of-the-art results on a dataset of non-expert executions (58% vs. 24% F1-score), and improve performance in the presence of noise, short actions and non-homogeneous workflows, i.e. non repetitive action sequences.},
}

@article{Özgüner2021VisuallyGN,
  author = {Orhan Özgüner and Thomas Shkurti and Su Lu and W. Newman and M. C. Çavuşoğlu},
  title = {Visually Guided Needle Driving and Pull for Autonomous Suturing},
  journal = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
  booktitle = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
  year = {2021},
  pages = {242-248},
  doi = {10.1109/case49439.2021},
  semanticscholar = {https://www.semanticscholar.org/paper/13d72b70a8970800ebb7f261e07d50a711fff24a},
}

@article{Black2020,
  author = {Black, David G and Hosseinabadi, Amir H Hadi and Salcudean, Septimiu E},
  title = {6-DOF Force Sensing for the Master Tool Manipulator of the da Vinci Surgical System},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2970944},
  semanticscholar = {https://www.semanticscholar.org/paper/dba4e60dc28bc9ce99428b026a0b71f3ea5e3141},
  research_field = {},
  data_type = {},
  dvrk_site = {UBC},
  abstract = {We integrated a force/torque sensor into the wrist of the Master Tool Manipulator (MTM) of the da Vinci Standard Surgical system. The added sensor can be used to monitor the surgeon interaction forces and to improve the haptic experience. The proposed mechanical design is expected to have little effect on the surgeon's operative experience and is simple and inexpensive to implement. We also developed a software package that allows for seamless integration of the force sensor into the da Vinci Research Kit (dVRK) and the Robot Operating System (ROS). The complete mechanical and electrical modifications, as well as the software packages are discussed. Two example applications of impedance control at the MTM and joystick control of the PSM are presented to demonstrate the successful integration of the sensor into the MTM and the interface to the dVRK.},
  issn = {2377-3766},
}

@article{Wu2020a,
  author = {Wu, Gloria C Y and Podolsky, Dale J and Looi, Thomas and Kahrs, Lueder A and Drake, James M and Forrest, Christopher R},
  title = {A 3 mm Wristed Instrument for the da Vinci Robot: Setup, Characterization, and Phantom Tests for Cleft Palate Repair},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/TMRB.2020.2977737},
  semanticscholar = {https://www.semanticscholar.org/paper/f208bae8d3e5a626986da2012388524473da71bc},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {SKCH},
  abstract = {Cleft palate is a congenital defect that affects approximately 1 in 800 births worldwide. A robotic approach for cleft palate repair is desired due to the potential ergonomic, vision and accessibility improvements in the small workspace of the oral cavity. This paper presents a 3 mm, 4-degree-of-freedom instrument for use with the widely available da Vinci system. The pin-jointed wrist design features cable guide channels in place of pulleys, significantly reducing the size of the wrist. Additionally, a novel cam mechanism minimizes any potential cable slack resulting from the lack of pulleys. The accuracy of the novel wrist was found to be 1.4° for motion in the same direction and the slack reducing capability of the cam mechanism was verified. Despite the 11.9° of hysteresis, there was no noticeable impact to teleoperation as the instrument was used to successfully perform suturing with the da Vinci Research Kit. Furthermore, contact between the tools and the oral aperture during surgery is significantly lower for the 3 mm instrument when compared to current 8 mm instruments. In summary, the novel instrument has the potential to better facilitate the adoption of a robotic approach.},
  issn = {2576-3202},
}

@article{Wang2020,
  author = {Wang, Ziheng and Kasman, Michael and Martinez, Marco and Rege, Robert and Zeh, Herbert and Scott, Danny and Fey, Ann Majewicz},
  title = {A Comparative Human-Centric Analysis of Virtual Reality and Dry Lab Training Tasks on the da Vinci Surgical Platform},
  journal = {Journal of Medical Robotics Research},
  year = {2020},
  publisher = {World Scientific Publishing Company},
  doi = {10.1142/s2424905x19420078},
  semanticscholar = {https://www.semanticscholar.org/paper/bdbef3bb025ab0fcd5b69ee6d1a6e1ee9515f8f0},
  research_field = {},
  data_type = {},
  dvrk_site = {UTD},
  abstract = {There is a growing, widespread trend of adopting robot-assisted minimally invasive surgery (RMIS) in clinical care. Dry lab robot training and virtual reality simulation are commonly used to train surgical residents; however, it is unclear whether both types of training are equivalent or can be interchangeable and still achieve the same results in terms of training outcomes. In this paper, we take the first step in comparing the effects of physical and simulated surgical training tasks on human operator kinematics and physiological response to provide a richer understanding of exactly how the user interacts with the actual or simulated surgical robot. Four subjects, with expertise levels ranging from novice to expert surgeon, were recruited to perform three surgical tasks — Continuous Suture, Pick and Place, Tubes, with three repetitions — on two training platforms: (1) the da Vinci Si Skills Simulator and (2) da Vinci S robot, in a randomized order. We collected physiological response and kinematic movement data through body-worn sensors for a total of 72 individual experimental trials. A range of expertise was chosen for this experiment to wash out inherent differences based on expertise and only focus on inherent differences between the virtual reality and dry lab platforms. Our results show significant differences ([Formula: see text]-[Formula: see text]) between tasks done on the simulator and surgical robot. Specifically, robotic tasks resulted in significantly higher muscle activation and path length, and significantly lower economy of volume. The individual tasks also had significant differences in various kinematic and physiological metrics, leading to significant interaction effects between the task type and training platform. These results indicate that the presence of the robotic system may make surgical training tasks more difficult for the human operator. Thus, the potentially detrimental effects of virtual reality training alone are an important topic for future investigation.},
  issn = {2424-905X},
}

@inproceedings{Hashempour2020a,
  author = {Hashempour, Hamidreza and Nazari, Kiyanoush and Zhong, Fangxun and Esfahani, Amir Masoud Ghalamzan},
  title = {A data-set of piercing needle through deformable objects for deep learning from demonstrations},
  booktitle = {arXiv},
  year = {2020},
  doi = {10.5860/choice.189890},
  url = {https://arxiv.org/abs/2012.02458},
  semanticscholar = {https://www.semanticscholar.org/paper/f4327b978dec52f16b089c222c43543f8ecf4717},
  research_field = {AU},
  data_type = {KD and DD and SD and ED},
  abstract = {The innovative preprint repository, arXiv, was created in the early 1990s to improve access to scientific research. arXiv contains millions of Open Access articles in physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems science, and economics. All articles are available for free download on the open web. Often research findings are available on arXiv before they are published in a peer-reviewed journal. arXiv relies on a collaborative support business model where institutions that most heavily utilize arXiv contribute financially. Support also comes from Cornell University and the Simons Foundation.},
  archiveprefix = {arXiv},
  arxivid = {2012.02458},
  eprint = {2012.02458},
  issn = {23318422},
}

@inproceedings{Tran2020,
  author = {Tran, Nam and Wu, Jie Ying and Deguet, Anton and Kazanzides, Peter},
  title = {A Deep Learning Approach to Intrinsic Force Sensing on the da Vinci Surgical Robot},
  booktitle = {IEEE Intl. Conf. on Robotic Computing},
  year = {2020},
  pages = {25-32},
  doi = {10.1109/IRC.2020.00011},
  semanticscholar = {https://www.semanticscholar.org/paper/a2fc15e7937f14e53a6c1077d5b0a10cdb234b9c},
  abstract = {In robot-assisted minimally-invasive surgery (RAMIS), force estimation remains a challenging issue. We seek to estimate external forces based on available measurements from the joint encoders and motor currents. To this end, we propose a deep learning approach for end-to-end force estimation on the da Vinci Surgical System that is trained using data collected by both moving an instrument in free space and by palpating a tissue phantom that has an embedded force sensor for ground truth. The trained neural network provides reasonable force estimates (within about 1N to 2N precision given a full range of 10N) and is generalizable to other regions of the robot workspace. We further show that our proposed system can provide useful haptic feedback in a pilot study to differentiate stiffness in various tissue phantoms.},
  keywords = {dvrk, machine-learning},
  date = {2020-11-01},
  pubstate = {published},
  tppubtype = {inproceedings},
}

@article{Lu2020,
  author = {Lu, Bo and Chen, Wei and Jin, Yue-Ming and Zhang, Dandan and Dou, Qi and Chu, Henry K. and Heng, Pheng-Ann and Liu, Yun-Hui},
  title = {A Learning-Driven Framework with Spatial Optimization For Surgical Suture Thread Reconstruction and Autonomous Grasping Under Multiple Topologies and Environmental Noises},
  journal = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS45743.2020.9341445},
  semanticscholar = {https://www.semanticscholar.org/paper/efb94e389706c6307d42e70e33599771c9283f0c},
  arxiv = {https://arxiv.org/abs/2007.00920},
  research_field = {AU},
  data_type = {RI and KD and SD},
  dvrk_site = {CUHK},
  abstract = {Surgical knot tying is one of the most fundamental and important procedures in surgery, and a high-quality knot can significantly benefit the postoperative recovery of the patient. However, a longtime operation may easily cause fatigue to surgeons, especially during the tedious wound closure task. In this paper, we present a vision-based method to automate the suture thread grasping, which is a sub-task in surgical knot tying and an intermediate step between the stitching and looping manipulations. To achieve this goal, the acquisition of a suture’s three-dimensional (3D) information is critical. Towards this objective, we adopt a transfer-learning strategy first to fine-tune a pre-trained model by learning the information from large legacy surgical data and images obtained by the onsite equipment. Thus, a robust suture segmentation can be achieved regardless of inherent environment noises. We further leverage a searching strategy with termination policies for a suture’s sequence inference based on the analysis of multiple topologies. Exact results of the pixel-level sequence along a suture can be obtained, and they can be further applied for a 3D shape reconstruction using our optimized shortest path approach. The grasping point considering the suturing criterion can be ultimately acquired. Experiments regarding the suture 2D segmentation and ordering sequence inference under environmental noises were extensively evaluated. Results related to the automated grasping operation were demonstrated by simulations in V-REP and by robot experiments using Universal Robot (UR) together with the da Vinci Research Kit (dVRK) adopting our learning-driven framework.},
}

@article{Brancadoro2020,
  author = {Brancadoro, Margherita and Dimitri, Mattia and Boushaki, Mohamed Nassim and Staderini, Fabio and Sinibaldi, Edoardo and Capineri, Lorenzo and Cianchi, Fabio and {Biffi Gentili}, Guido and Menciassi, Arianna},
  title = {A novel microwave tool for robotic liver resection in minimally invasive surgery},
  journal = {Minimally Invasive Therapy & Allied Technologies},
  year = {2020},
  publisher = {Taylor & Francis},
  doi = {10.1080/13645706.2020.1749083},
  semanticscholar = {https://www.semanticscholar.org/paper/9365d4896bf62f8f21292c356e6c0ccf1fe37906},
  research_field = {HW},
  data_type = {KD and SD and ED},
  dvrk_site = {SSSA},
  abstract = {Abstract Introduction During the last two decades, many surgical procedures have evolved from open surgery to minimally invasive surgery (MIS). This limited invasiveness has motivated the development of robotic assistance platforms to obtain better surgical outcomes. Nowadays, the da Vinci robot is a commercial tele-robotic platform widely used for different surgical applications. Material and methods In this work, the da Vinci Research Kit (dVRK), namely the research version of the da Vinci, is used to manipulate a novel microwave device in a teleoperation scenario. The dVRK provides an open source platform, so that the novel microwave tool, dedicated to prevention bleeding during hepatic resection surgery, is mechanically integrated on the slave side, while the software interface is adapted in order to correctly control tool pose. Tool integration is validated through in-vitro and ex-vivo tests performed by expert surgeons, meanwhile the coagulative efficacy of the developed tool in a perfused liver model was proved in in-vivo tests. Results and conclusions An innovative microwave tool for liver robotic resection has been realized and integrated into a surgical robot. The tool can be easily operated through the dVRK without limiting the intuitive and friendly use, and thus easily reaching the hemostasis of vessels.},
  issn = {1364-5706},
}

@inproceedings{Alambeigi2020,
  author = {Alambeigi, Farshid and Wang, Zerui and Liu, Yun-Hui and Taylor, Russell H. and Armand, Mehran},
  title = {A Versatile Data-Driven Framework for Model-Independent Control of Continuum Manipulators Interacting With Obstructed Environments With Unknown Geometry and Stiffness},
  booktitle = {arXiv preprint},
  year = {2020},
  doi = {10.1145/3613904.3642858},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  arxiv = {https://arxiv.org/abs/2310.09985},
  research_field = {HW},
  data_type = {RI and KD and DD},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@article{Zhang2020a,
  author = {Zhang, Dandan and Liu, Jindong and Gao, Anzhu and Yang, Guang-Zhong},
  title = {An Ergonomic Shared Workspace Analysis Framework for the Optimal Placement of a Compact Master Control Console},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2974428},
  semanticscholar = {https://www.semanticscholar.org/paper/e984a36b39cf76efe10122d38442766517cbcbad},
  research_field = {},
  data_type = {},
  dvrk_site = {ICL},
  abstract = {Master-Slave control is commonly used for Robot-Assisted Minimally Invasive Surgery (RAMIS). The configuration, as well as the placement of the master manipulators, can influence the remote control performance. An ergonomic shared workspace analysis framework is proposed in this letter. Combined with the workspace of the master manipulators and the human arms, the human-robot interaction workspace can be generated. The optimal master robot placement can be determined based on three criteria: 1) interaction workspace volume, 2) interaction workspace quality, and 3) intuitiveness for slave robot control. Experimental verification of the platform is conducted on a da Vinci Research Kit (dVRK). An in-house compact master manipulator (Hamlyn CRM) is used as the master robot and the da Vinci robot is used as the slave robot. Comparisons are made between with and without using design optimization to validate the effectiveness of the ergonomic shared workspace analysis technique. Results indicate that the proposed ergonomic shared workspace analysis can improve the performance of teleoperation in terms of task completion time and the number of clutching required during operation.},
  issn = {2377-3766},
}

@article{Caccianiga2020,
  author = {Caccianiga, Guido and Mariani, Andrea and {De Momi}, Elena and Cantarero, Gabriela and Brown, Jeremy D},
  title = {An Evaluation of Inanimate and Virtual Reality Training for Psychomotor Skill Development in Robot-Assisted Minimally Invasive Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/TMRB.2020.2990692},
  semanticscholar = {https://www.semanticscholar.org/paper/afbb60612267eddc0e81a7fbb633bc099744eae6},
  research_field = {TR},
  data_type = {KD and ED},
  dvrk_site = {JHU},
  abstract = {Robot-assisted minimally invasive surgery (RAMIS) is gaining widespread adoption in many surgical specialties, despite the lack of a standardized training curriculum. Current training approaches rely heavily on virtual reality simulators, in particular for basic psychomotor and visuomotor skill development. It is not clear, however, whether training in virtual reality is equivalent to inanimate model training. In this manuscript, we seek to compare virtual reality training to inanimate model training, with regard to skill learning and skill transfer. Using a custom-developed needle-driving training task with inanimate and virtual analogs, we investigated the extent to which N=18 participants improved their skill on a given platform post-training, and transferred that skill to the opposite platform. Results indicate that the two approaches are not equivalent, with more salient skill transfer after inanimate training than virtual training. These findings support the claim that training with real physical models is the gold standard, and suggest more inanimate model training be incorporated into training curricula for early psychomotor skill development.},
  issn = {2576-3202},
}

@article{Mariani2020a,
  author = {Mariani, Andrea and Colaci, Giorgia and {Da Col}, Tommaso and Sanna, Nicole and Vendrame, Eleonora and Menciassi, Arianna and {De Momi}, Elena},
  title = {An Experimental Comparison Towards Autonomous Camera Navigation to Optimize Training in Robot Assisted Surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2965067},
  semanticscholar = {https://www.semanticscholar.org/paper/3c6bff93a4542c116174b429df3977eca1913267},
  research_field = {TR},
  data_type = {KD and SD},
  dvrk_site = {POLIMI},
  abstract = {Robot-Assisted Surgery enhances vision and it can restore depth perception, but it introduces the need for learning how to tele-operatively control both the surgical tools and the endoscope. Together with the complexity of selecting the optimal viewpoint to carry out the procedure, this requires distinct training. This work proposes an autonomous camera navigation during the initial stages of training in order to optimize the learning of these skills. A user study involving 26 novice participants was carried out using the master console of the da Vinci Research Kit and a virtual reality training environment. The subjects were randomly divided into two groups: the control group that manually controlled the camera as in the current practice and the experimental group that underwent the autonomous navigation. After training, the time-accuracy metrics of the users who underwent autonomous camera navigation were significantly higher with respect to the control group. Additionally, autonomous camera navigation seemed to be capable to provide an imprinting about endoscope management.},
  issn = {2377-3766},
}

@article{Fontanelli2020,
  author = {Fontanelli, Giuseppe Andrea and Buonocore, Luca Rosario and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},
  title = {An External Force Sensing System for Minimally Invasive Robotic Surgery},
  journal = {IEEE/ASME Transactions on Mechatronics},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/TMECH.2020.2979027},
  semanticscholar = {https://www.semanticscholar.org/paper/1963a7371ab606afa050269828cda11cb1790eb4},
  research_field = {},
  data_type = {},
  dvrk_site = {UNFII},
  abstract = {Minimally invasive robotic surgery (MIRS) has revolutionized surgical procedures. However, compared to classic laparoscopy, the surgeon must rely only on visual perception because of the lack of force feedback. In this article, a new noninvasive force feedback system is proposed and evaluated. Extending the work by Fontanelli (2017), where preliminary results were presented, a solution based on a novel force sensor placed in the terminal part of the trocar is shown in detail. With respect to the state of the art, our system allows measuring the interaction forces between the surgical instrument and the environment inside the patient's body without any changes to the instrument structure and with full adaptability to different robotic platforms and surgical tools. Using a commercial force-torque sensor as ground truth, the static and dynamic characterization of the sensor is provided together with an extensive experimental validation. Finally, a simple and intuitive application of the proposed sensing system in a realistic surgical scenario is presented.},
  issn = {1083-4435},
}

@inproceedings{Munawar2020,
  author = {Munawar, Adnan and Srishankar, Nishan and Fischer, Gregory S},
  title = {An Open-Source Framework for Rapid Development of Interactive Soft-Body Simulations for Real-Time Training},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2020},
  doi = {10.1109/icra40945.2020},
  semanticscholar = {https://www.semanticscholar.org/paper/a88c357bf3045b5d438ee972f5bf8c284291c7f0},
  research_field = {TR},
  data_type = {DD and ED},
  isbn = {9781728173955},
}

@inproceedings{Li2020c,
  author = {Li, Zhaoshuo and Gordon, Alex and Looi, Thomas and Drake, James and Forrest, Christopher and Taylor, Russell H.},
  title = {Anatomical Mesh-Based Virtual Fixtures for Surgical Robots},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {},
  data_type = {},
}

@article{Richter2020,
  author = {Richter, Florian and Shen, Shihao and Liu, Fei and Huang, Jingbin and Funk, Emily K. and Orosco, Ryan K. and Yip, Michael C.},
  title = {Autonomous Robotic Suction to Clear the Surgical Field for Hemostasis using Image-based Blood Flow Detection},
  journal = {arXiv preprint},
  year = {2020},
  doi = {10.1109/LRA.2021.3056057},
  semanticscholar = {https://www.semanticscholar.org/paper/57a0705beed92d510701b2d7c8032111709897d2},
  arxiv = {https://arxiv.org/abs/2010.08441},
  research_field = {AU},
  data_type = {KD},
  dvrk_site = {UCSD},
  abstract = {Autonomous robotic surgery has seen significant progression over the last decade with the aims of reducing surgeon fatigue, improving procedural consistency, and perhaps one day take over surgery itself. However, automation has not been applied to the critical surgical task of controlling tissue and blood vessel bleeding–known as hemostasis. The task of hemostasis covers a spectrum of bleeding sources and a range of blood velocity, trajectory, and volume. In an extreme case, an un-controlled blood vessel fills the surgical field with flowing blood. In this work, we present the first, automated solution for hemostasis through development of a novel probabilistic blood flow detection algorithm and a trajectory generation technique that guides autonomous suction tools towards pooling blood. The blood flow detection algorithm is tested in both simulated scenes and in a real-life trauma scenario involving a hemorrhage that occurred during thyroidectomy. The complete solution is tested in a physical lab setting with the da Vinci Research Kit (dVRK) and a simulated surgical cavity for blood to flow through. The results show that our automated solution has accurate detection, a fast reaction time, and effective removal of the flowing blood. Therefore, the proposed methods are powerful tools to clearing the surgical field which can be followed by either a surgeon or future robotic automation developments to close the vessel rupture.},
}

@inproceedings{Ginesi2020,
  author = {Ginesi, Michele and Meli, Daniele and Roberti, Andrea and Sansonetto, Nicola and Fiorini, Paolo},
  title = {Autonomous task planning and situation awareness in robotic surgery},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {},
  data_type = {},
}

@inproceedings{Attanasio2020,
  author = {Attanasio, Aleks and Scaglioni, Bruno and Leonetti, Matteo and Frangi, Alejandro F. and Cross, William and Biyani, Chandra Shekhar and Valdastri, Pietro},
  title = {Autonomous Tissue Retraction in Robotic Assisted Minimally Invasive Surgery - A Feasibility Study},
  booktitle = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {4},
  doi = {10.1109/LRA.2020.3013914},
  semanticscholar = {https://www.semanticscholar.org/paper/d591e9d71641e8c1e74c5d48354dc4403932aa99},
  research_field = {AU},
  data_type = {RI and KD and SD},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Zhan2020,
  author = {Zhan, Jian and Cartucho, Joao and Giannarou, Stamatia},
  title = {Autonomous Tissue Scanning under Free-Form Motion for Intraoperative Tissue Characterisation},
  journal = {arXiv preprint},
  year = {2020},
  doi = {10.1109/ICRA40945.2020.9197294},
  semanticscholar = {https://www.semanticscholar.org/paper/84b35a7c6a35bbd284a0de4aa414244f1f161b4e},
  arxiv = {https://arxiv.org/abs/2005.05050},
  research_field = {},
  data_type = {},
  dvrk_site = {ICL},
  abstract = {In Minimally Invasive Surgery (MIS), tissue scanning with imaging probes is required for subsurface visualisation to characterise the state of the tissue. However, scanning of large tissue surfaces in the presence of motion is a challenging task for the surgeon. Recently, robot-assisted local tissue scanning has been investigated for motion stabilisation of imaging probes to facilitate the capturing of good quality images and reduce the surgeon’s cognitive load. Nonetheless, these approaches require the tissue surface to be static or translating with periodic motion. To eliminate these assumptions, we propose a visual servoing framework for autonomous tissue scanning, able to deal with free-form tissue motion. The 3D structure of the surgical scene is recovered, and a feature-based method is proposed to estimate the motion of the tissue in real-time. The desired scanning trajectory is manually defined on a reference frame and continuously updated using projective geometry to follow the tissue motion and control the movement of the robotic arm. The advantage of the proposed method is that it does not require the learning of the tissue motion prior to scanning and can deal with free-form motion. We deployed this framework on the da Vinci®surgical robot using the da Vinci Research Kit (dVRK) for Ultrasound tissue scanning. Our framework can be easily extended to other probe-based imaging modalities.},
}

@article{Ozguner2020,
  author = {Ozguner, Orhan and Shkurti, Thomas and Huang, Siqi and Hao, Ran and Jackson, Russell C. and Newman, Wyatt S. and Cavusoglu, M. Cenk},
  title = {Camera-Robot Calibration for the Da Vinci Robotic Surgery System},
  journal = {IEEE Transactions on Automation Science and Engineering},
  year = {2020},
  volume = {17},
  number = {4},
  doi = {10.1109/TASE.2020.2986503},
  semanticscholar = {https://www.semanticscholar.org/paper/b703b865ad8090966883596da80b2198f913c88c},
  research_field = {},
  data_type = {},
  dvrk_site = {CWRU},
  abstract = {The development of autonomous or semiautonomous surgical robots stands to improve the performance of existing teleoperated equipment but requires fine hand-eye calibration between the free-moving endoscopic camera and patient-side manipulator arms (PSMs). A novel method of solving this problem for the da Vinci robotic surgical system and kinematically similar systems is presented. First, a series of image-processing and optical-tracking operations are performed to compute the coordinate transformation between the endoscopic camera view frame and an optical-tracking marker permanently affixed to the camera body. Then, the kinematic properties of the PSM are exploited to compute the coordinate transformation between the kinematic base frame of the PSM and an optical marker permanently affixed thereto. Using these transformations, it is then possible to compute the spatial relationship between the PSM and the endoscopic camera using only one tracker snapshot of the two markers. The effectiveness of this calibration is demonstrated by successfully guiding the PSM end-effector to points of interest identified through the camera. Additional tests on a surgical task, namely, grasping a surgical needle, are also performed to validate the proposed method. The resulting visually guided robot positioning accuracy is better than the earlier hand-eye calibration results reported in the literature for the da Vinci system while supporting the intraoperative update of the calibration and requiring only devices that are already commonly used in the surgical environment. Note to Practitioners—The problem of hand-eye calibration for the da Vinci robotic surgical system and kinematically similar systems is addressed in this article. Existing approaches have insufficient accuracy to automate low-level surgical subtasks and often require external patterns or subjective human intervention, none of which are applicable to practical robotic minimally invasive surgery (RMIS) scenarios. This article breaks down the calibration procedure into systematic steps to reduce error accumulation. Most of the time-consuming steps are performed offline, allowing them to be retained between movements. Each time the passive joints of the manipulator or the endoscope move, all that needs to be done is to refresh the transformation between the fixed markers. This key idea enables intraoperative updates of the hand-eye calibration to be performed online without sacrificing precision. The calibration method presented here demonstrates that the achieved accuracy is sufficient for automating basic surgical manipulation tasks, such as grasping a suturing needle. The hand-eye calibration will be incorporated into a visually guided manipulation framework to perform high-precision autonomous surgical tasks.},
  issn = {1545-5955},
}

@inproceedings{Su2020a,
  author = {Su, Yun-Hsuan and Munawar, Adnan and Deguet, Anton and Lewis, Andrew and Lindgren, Kyle and Li, Yangming and Taylor, Russell H. and Fischer, Gregory S. and Hannaford, Blake and Kazanzides, Peter},
  title = {Collaborative Robotics Toolkit (CRTK): Open Software Framework for Surgical Robotics Research},
  booktitle = {2020 Fourth IEEE International Conference on Robotic Computing (IRC)},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/IRC.2020.00014},
  semanticscholar = {https://www.semanticscholar.org/paper/54551156fdb5ff1806491cc1a1369899ca2390b5},
  research_field = {HW},
  data_type = {SD},
  abstract = {Robot-assisted minimally invasive surgery has made a substantial impact in operating rooms over the past few decades with their high dexterity, small tool size, and impact on adoption of minimally invasive techniques. In recent years, intelligence and different levels of surgical robot autonomy have emerged thanks to the medical robotics endeavors at numerous academic institutions and leading surgical robot companies. To accelerate interaction within the research community and prevent repeated development, we propose the Collaborative Robotics Toolkit (CRTK), a common API for the RAVEN-II and da Vinci Research Kit (dVRK) - two open surgical robot platforms installed at more than 40 institutions worldwide. CRTK has broadened to include other robots and devices, including simulated robotic systems and industrial robots. This common API is a community software infrastructure for research and education in cutting edge human-robot collaborative areas such as semi-autonomous teleoperation and medical robotics. This paper presents the concepts, design details and the integration of CRTK with physical robot systems and simulation platforms.},
}

@inproceedings{Varier2020,
  author = {Varier, Vignesh Manoj and Rajamani, Dhruv Kool and Goldfarb, Nathaniel and Tavakkolmoghaddam, Farid and Munawar, Adnan and Fischer, Gregory S},
  title = {Collaborative Suturing: A Reinforcement Learning Approach to Automate Hand-off Task in Suturing for Surgical Robots},
  booktitle = {2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/RO-MAN47096.2020.9223543},
  semanticscholar = {https://www.semanticscholar.org/paper/ea46d2f21b3bb2e43549b2e6bd210b8e5b162a99},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {Over the past decade, Robot-Assisted Surgeries (RAS), have become more prevalent in facilitating successful operations. Of the various types of RAS, the domain of collaborative surgery has gained traction in medical research. Prominent examples include providing haptic feedback to sense tissue consistency, and automating sub-tasks during surgery such as cutting or needle hand-off - pulling and reorienting the needle after insertion during suturing. By fragmenting suturing into automated and manual tasks the surgeon could essentially control the process with one hand and also circumvent workspace restrictions imposed by the control interface present at the surgeon's side during the operation. This paper presents an exploration of a discrete reinforcement learning-based approach to automate the needle hand-off task. Users were asked to perform a simple running suture using the da Vinci Research Kit. The user trajectory was learnt by generating a sparse reward function and deriving an optimal policy using Q-learning. Trajectories obtained from three learnt policies were compared to the user defined trajectory. The results showed a root-mean-square error of [0.0044mm, 0.0027mm, 0.0020mm] in ℝ3. Additional trajectories from varying initial positions were produced from a single policy to simulate repeated passes of the hand-off task.},
  isbn = {978-1-7281-6075-7},
}

@article{Orosco2020,
  author = {Orosco, Ryan K and Lurie, Benjamin and Matsuzaki, Tokio and Funk, Emily K and Divi, Vasu and Holsinger, F Christopher and Hong, Steven and Richter, Florian and Das, Nikhil and Yip, Michael},
  title = {Compensatory motion scaling for time ‑ delayed robotic surgery},
  journal = {Surgical Endoscopy},
  year = {2020},
  publisher = {Springer US},
  doi = {10.1007/s00464-020-07681-7},
  semanticscholar = {https://www.semanticscholar.org/paper/30a554a02e651a215c1f816b32efa9538d9731ff},
  research_field = {HW},
  data_type = {RI and KD and DD},
  dvrk_site = {UCSD},
  isbn = {0046402007681},
  issn = {1432-2218},
}

@inproceedings{Seita2020a,
  author = {Seita, Daniel and Ganapathi, Aditya and Hoque, Ryan and Hwang, Minho and Cen, Edward and Tanwani, Ajay Kumar and Balakrishna, Ashwin and Thananjeyan, Brijen and Ichnowski, Jeffrey and Jamali, Nawid and Yamane, Katsu and Iba, Soshi and Canny, John and Goldberg, Ken},
  title = {Deep Imitation Learning of Sequential Fabric Smoothing From an Algorithmic Supervisor},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {AU},
  data_type = {RI and KD},
}

@article{Eslamian2020,
  author = {Eslamian, Shahab and Reisner, Luke A and Pandya, Abhilash K},
  title = {Development and evaluation of an autonomous camera control algorithm on the da Vinci Surgical System},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2020},
  volume = {16},
  number = {2},
  publisher = {Wiley Online Library},
  doi = {10.1002/rcs.2036},
  semanticscholar = {https://www.semanticscholar.org/paper/a0f43c66bf5f795a1d01f82e0c76513bf6650b16},
  research_field = {AU},
  data_type = {RI and KD},
  dvrk_site = {WSU},
  abstract = {Manual control of the camera arm in telerobotic surgical systems requires the surgeon to repeatedly interrupt the flow of the surgery. During surgery, there are instances when one or even both tools can drift out of the field of view. These issues may lead to increased workload and potential errors.},
  issn = {1478-5951},
}

@article{Hwang2020,
  author = {Hwang, Minho and Thananjeyan, Brijen and Paradis, Samuel and Seita, Daniel and Ichnowski, Jeffrey and Fer, Danyal and Low, Thomas and Goldberg, Ken},
  title = {Efficiently Calibrating Cable-Driven Surgical Robots With RGBD Fiducial Sensing and Recurrent Neural Networks},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {4},
  doi = {10.1109/LRA.2020.3010746},
  semanticscholar = {https://www.semanticscholar.org/paper/1e6bee245e605a039aa6d2d80f1477b61fcbaec7},
  research_field = {},
  data_type = {},
  dvrk_site = {UBC},
  abstract = {Automation of surgical subtasks using cable-driven robotic surgical assistants (RSAs) such as Intuitive Surgical's da Vinci Research Kit (dVRK) is challenging due to imprecision in control from cable-related effects such as cable stretching and hysteresis. We propose a novel approach to efficiently calibrate such robots by placing a 3D printed fiducial coordinate frames on the arm and end-effector that is tracked using RGBD sensing. To measure the coupling and history-dependent effects between joints, we analyze data from sampled trajectories and consider 13 approaches to modeling. These models include linear regression and LSTM recurrent neural networks, each with varying temporal window length to provide compensatory feedback. With the proposed method, data collection of 1800 samples takes 31 minutes and model training takes under 1 minute. Results on a test set of reference trajectories suggest that the trained model can reduce the mean tracking error of physical robot from 2.96 mm to 0.65 mm. Results on the execution of open-loop trajectories of the FLS peg transfer surgeon training task suggest that the best model increases success rate from 39.4% to 96.7%, producing performance comparable to that of an expert surgical resident. Supplementary materials, including code and 3D-printable models, are available at https://sites.google.com/berkeley.edu/surgical-calibration.},
  issn = {2377-3766},
}

@incollection{Penza2020,
  author = {Penza, Veronica and Moccia, Sara and {De Momi}, Elena and Mattos, Leonardo S},
  title = {Enhanced Vision to Improve Safety in Robotic Surgery},
  booktitle = {Handbook of Robotic and Image-Guided Surgery},
  year = {2020},
  pages = {223--237},
  publisher = {Elsevier},
  doi = {10.1016/c2017-0-01316-2},
  semanticscholar = {https://www.semanticscholar.org/paper/88f3cb632a00f51ca00e441ecb892ba6b98402fa},
  research_field = {},
  data_type = {},
}

@inproceedings{Qian2020,
  author = {Qian, Long and Song, Chengzhi and Jiang, Yiwei and Luo, Qi and Ma, Xin and Chiu, Philip Waiyan},
  title = {FlexiVision : Teleporting the Surgeon ' s Eyes via Robotic Flexible Endoscope and Head-Mounted Display},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {AU},
  data_type = {KD and SD and ED},
}

@article{Zhang2020,
  author = {Zhang, Dandan and Liu, Jindong and Zhang, Lin and Yang, Guang-Zhong},
  title = {Hamlyn CRM: a compact master manipulator for surgical robot remote control},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2020},
  volume = {15},
  number = {3},
  publisher = {Springer},
  doi = {10.1007/s11548-019-02112-y},
  semanticscholar = {https://www.semanticscholar.org/paper/46ffc8aab655fa9c27b471a89fb5d479f7f3a84e},
  research_field = {HW},
  data_type = {KD and DD and ED},
  dvrk_site = {ICL},
  abstract = {Purpose Compact master manipulators have inherent advantages, since they can have practical deployment within the general surgical environments easily and bring benefits to surgical training. To assess the advantages of compact master manipulators for surgical skills training and the performance of general robot-assisted surgical tasks, Hamlyn Compact Robotic Master (Hamlyn CRM) is built up and evaluated in this paper. Methods A compact structure for the master manipulator is proposed. A novel sensing system is designed while stable real-time motion tracking can be realized by fusing the information from multiple sensors. User studies were conducted based on a ring transfer task and a needle passing task to explore a suitable mapping strategy for the compact master manipulator to control a surgical robot remotely. The overall usability of the Hamlyn CRM is verified based on the da Vinci Research Kit (dVRK). The master manipulators of the dVRK control console are used as the reference Results Motion tracking experiments verified that the proposed system can track the operators’ hand motion precisely. As for the master–slave mapping strategy, user studies proved that the combination of the position relative mapping mode and the orientation absolute mapping mode is suitable for Robot-Assisted Minimally Invasive Surgery (RAMIS), while key parameters for mapping are selected. Conclusion Results indicated that the Hamlyn CRM can serve as a compact master manipulator for surgical training and has potential applications for RAMIS.},
  issn = {1861-6429},
}

@article{Zhong2020,
  author = {Zhong, Fangxun and Wang, Zerui and Chen, Wei and He, Kejing and Wang, Yaqing and Liu, Yun-Hui},
  title = {Hand-Eye Calibration of Surgical Instrument for Robotic Surgery Using Interactive Manipulation},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2967685},
  semanticscholar = {https://www.semanticscholar.org/paper/25b8542c00d4992d82cac797972676a18d8397e7},
  research_field = {IM},
  data_type = {RI and KD and ED},
  dvrk_site = {CUHK},
  abstract = {Conventional robot hand-eye calibration methods are impractical for localizing robotic instruments in minimally-invasive surgeries under intra-corporeal workspace after pre-operative set-up. In this letter, we present a new approach to autonomously calibrate a robotic instrument relative to a monocular camera without recognizing calibration objects or salient features. The algorithm leverages interactive manipulation (IM) of the instrument for tracking its rigid-body motion behavior subject to the remote center-of-motion constraint. An adaptive controller is proposed to regulate the IM-induced instrument trajectory, using visual feedback, within a 3D plane which is observable from both the robot base and the camera. The eye-to-hand orientation and position are then computed via a dual-stage process allowing parameter estimation in low-dimensional spaces. The method does not require the exact knowledge of instrument model or large-scale data collection. Results from simulations and experiments on the da Vinci Research Kit are demonstrated via a laparoscopy resembled set-up using the proposed framework.},
  issn = {2377-3766},
}

@article{Saracino2020,
  author = {Saracino, A. and {Oude Vrielink}, T. J. C. and Menciassi, Arianna and Sinibaldi, E. and Mylonas, G. P.},
  title = {Haptic intracorporeal palpation using a cable-driven parallel robot: a user study},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2020},
  doi = {10.1109/TBME.2020.2987646},
  semanticscholar = {https://www.semanticscholar.org/paper/3cd0d8f6efb2c6f030432663f094950ba004bde1},
  research_field = {},
  data_type = {},
  dvrk_site = {SSSA},
  abstract = {Objective: Intraoperative palpation is a surgical gesture jeopardized by the lack of haptic feedback which affects robotic minimally invasive surgery. Restoring the force reflection in teleoperated systems may improve both surgeons’ performance and procedures’ outcome. Methods: A force-based sensing approach was developed, based on a cable-driven parallel manipulator with anticipated seamless and low-cost integration capabilities in teleoperated robotic surgery. No force sensor on the end-effector is used, but tissue probing forces are estimated from measured cable tensions. A user study involving surgical trainees (n = 22) was conducted to experimentally evaluate the platform in two palpation-based test-cases on silicone phantoms. Two modalities were compared: visual feedback alone and both visual + haptic feedbacks available at the master site. Results: Surgical trainees’ preference for the modality providing both visual and haptic feedback is corroborated by both quantitative and qualitative metrics. Hard nodules detection sensitivity improves (94.35 ± 9.1% vs 76.09 ± 19.15% for visual feedback alone), while also exerting smaller forces (4.13 ± 1.02 N vs 4.82 ± 0.81 N for visual feedback alone) on the phantom tissues. At the same time, the subjective perceived workload decreases. Conclusion: Tissue-probe contact forces are estimated in a low cost and unique way, without the need of force sensors on the end-effector. Haptics demonstrated an improvement in the tumor detection rate, a reduction of the probing forces, and a decrease in the perceived workload for the trainees. Significance: Relevant benefits are demonstrated from the usage of combined cable-driven parallel manipulators and haptics during robotic minimally invasive procedures. The translation of robotic intraoperative palpation to clinical practice could improve the detection and dissection of cancer nodules.},
  issn = {0018-9294},
}

@article{Li2020,
  author = {Li, Zhaoshuo and Shahbazi, Mahya and Patel, Niravkumar and O'Sullivan, Eimear and Zhang, Haojie and Vyas, Khushi and Chalasani, Preetham and Deguet, Anton and Gehlbach, Peter L and Iordachita, Iulian},
  title = {Hybrid Robot-assisted Frameworks for Endomicroscopy Scanning in Retinal Surgeries},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/TMRB.2020.2988312},
  semanticscholar = {https://www.semanticscholar.org/paper/029f5b0626af6f0a0f4a3f080fdc851e5cdfa3a2},
  arxiv = {https://arxiv.org/abs/1909.06852},
  research_field = {},
  data_type = {},
  dvrk_site = {JHU},
  abstract = {High-resolution real-time intraocular imaging of retina at the cellular level is very challenging due to the vulnerable and confined space within the eyeball as well as the limited availability of appropriate modalities. A probe-based confocal laser endomicroscopy (pCLE) system, can be a potential imaging modality for improved diagnosis. The ability to visualize the retina at the cellular level could provide information that may predict surgical outcomes. The adoption of intraocular pCLE scanning is currently limited due to the narrow field of view and the micron-scale range of focus. In the absence of motion compensation, physiological tremors of the surgeons hand and patient movements also contribute to the deterioration of the image quality. Therefore, an image-based hybrid control strategy is proposed to mitigate the above challenges. The proposed hybrid control strategy enables a shared control of the pCLE probe between surgeons and robots to scan the retina precisely, with the absence of hand tremors and with the advantages of an image-based auto-focus algorithm that optimizes the quality of pCLE images. The hybrid control strategy is deployed on two frameworks—cooperative and teleoperated. Better image quality, smoother motion, and reduced workload are all achieved in a statistically significant manner with the hybrid control frameworks.},
  issn = {2576-3202},
}

@article{Roberti2020b,
  author = {Roberti, Andrea and Piccinelli, Nicola and Meli, Daniele and Muradore, Riccardo and Fiorini, Paolo},
  title = {Improving Rigid 3-D Calibration for Robotic Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2020},
  volume = {2},
  doi = {10.1109/TMRB.2020.3033670},
  semanticscholar = {https://www.semanticscholar.org/paper/2584f49ec0ebc1796bdf60a4856b59b15bf0a2fb},
  arxiv = {https://arxiv.org/abs/2007.08427},
  research_field = {IM},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {UV},
  abstract = {Autonomy is the next frontier of research in robotic surgery and its aim is to improve the quality of surgical procedures in the next future. One fundamental requirement for autonomy is advanced perception capability through vision sensors. In this article, we propose a novel calibration technique for a surgical scenario with a da Vinci® Research Kit (dVRK) robot. Camera and robotic arms calibration are necessary to precise position and emulate expert surgeon. The novel calibration technique is tailored for RGB-D cameras. Different tests performed on relevant use cases prove that we significantly improve precision and accuracy with respect to state of the art solutions for similar devices on a surgical-size setups. Moreover, our calibration method can be easily extended to standard surgical endoscope used in real surgical scenario.},
  issn = {2576-3202},
}

@article{Minelli2020,
  author = {Minelli, Marco and Sozzi, Alessio and Rossi, Giacomo De and Ferraguti, Federica and Setti, Francesco and Muradore, Riccardo and Bonf, Marcello and Secchi, Cristian},
  title = {Integrating Model Predictive Control and Dynamic Waypoints Generation for Motion Planning in Surgical Scenario},
  journal = {2020 IEEE International Workshop on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS45743.2020.9341673},
  semanticscholar = {https://www.semanticscholar.org/paper/92e75215524119301f04e92361b8eb40e17040b2},
  research_field = {AU},
  data_type = {RI and KD and ED},
  dvrk_site = {UV},
  abstract = {In this paper we present a novel strategy for motion planning of autonomous robotic arms in Robotic Minimally Invasive Surgery (R-MIS). We consider a scenario where several laparoscopic tools must move and coordinate in a shared environment. The motion planner is based on a Model Predictive Controller (MPC) that predicts the future behavior of the robots and allows to move them avoiding collisions between the tools and satisfying the velocity limitations. In order to avoid the local minima that could affect the MPC, we propose a strategy for driving it through a sequence of waypoints. The proposed control strategy is validated on a realistic surgical scenario.},
  isbn = {9781728162119},
}

@inproceedings{Paradis2020a,
  author = {Paradis, Samuel and Hwang, Minho and Thananjeyan, Brijen and Ichnowski, Jeffrey and Seita, Daniel and Fer, Danyal and Low, Thomas and Gonzalez, Joseph E. and Goldberg, Ken},
  title = {Intermittent Visual Servoing: Efficiently Learning Policies Robust to Instrument Changes for High-precision Surgical Manipulation},
  booktitle = {arXiv preprint},
  year = {2020},
  doi = {10.1145/3613904.3642858},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  arxiv = {https://arxiv.org/abs/2310.09985},
  research_field = {AU},
  data_type = {KD and SD and ED},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@inproceedings{Bombieri2020,
  author = {Bombieri, Marco and Alba, Diego Dall and Ramesh, Sanat and Menegozzo, Giovanni and Schneider, Caitlin and Fiorini, Paolo},
  title = {Joints-Space Metrics for Automatic Robotic Surgical Gestures Classification},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {TR},
  data_type = {RI and KD and ED},
}

@article{Wu2020,
  author = {Wu, Jie Ying and Kazanzides, Peter and Unberath, Mathias},
  title = {Leveraging vision and kinematics data to improve realism of biomechanic soft tissue simulation for robotic surgery},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2020},
  publisher = {Springer},
  doi = {10.1007/s11548-020-02139-6},
  semanticscholar = {https://www.semanticscholar.org/paper/0d452fbd66ba6f76a1242258d0a04f685d0687c2},
  arxiv = {https://arxiv.org/abs/2003.06518},
  research_field = {SS},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {JHU},
  abstract = {Purpose Surgical simulations play an increasingly important role in surgeon education and developing algorithms that enable robots to perform surgical subtasks. To model anatomy, finite element method (FEM) simulations have been held as the gold standard for calculating accurate soft tissue deformation. Unfortunately, their accuracy is highly dependent on the simulation parameters, which can be difficult to obtain. Methods In this work, we investigate how live data acquired during any robotic endoscopic surgical procedure may be used to correct for inaccurate FEM simulation results. Since FEMs are calculated from initial parameters and cannot directly incorporate observations, we propose to add a correction factor that accounts for the discrepancy between simulation and observations. We train a network to predict this correction factor. Results To evaluate our method, we use an open-source da Vinci Surgical System to probe a soft tissue phantom and replay the interaction in simulation. We train the network to correct for the difference between the predicted mesh position and the measured point cloud. This results in 15–30% improvement in the mean distance, demonstrating the effectiveness of our approach across a large range of simulation parameters. Conclusion We show a first step towards a framework that synergistically combines the benefits of model-based simulation and real-time observations. It corrects discrepancies between simulation and the scene that results from inaccurate modeling parameters. This can provide a more accurate simulation environment for surgeons and better data with which to train algorithms.},
  issn = {1861-6410},
}

@incollection{Taylor2020,
  author = {Taylor, Russell H and Kazanzides, Peter and Fischer, Gregory S and Simaan, Nabil},
  title = {Medical robotics and computer-integrated interventional medicine},
  booktitle = {Biomedical Information Technology},
  year = {2020},
  publisher = {Elsevier},
  doi = {10.1117/12.916500},
  semanticscholar = {https://www.semanticscholar.org/paper/aec2473601284a3e8b9dde86539af318fa201ac3},
  research_field = {},
  data_type = {},
  abstract = {Computer-Integrated Interventional Medicine (CIIM) promises to have a profound impact on health care in the next 20 years, much as and for many of the same reasons that the marriage of computers and information processing methods with other technology have had on manufacturing, transportation, and other sectors of our society. Our basic premise is that the steps of creating patient-specific computational models, using these models for planning, registering the models and plans with the actual patient in the operating room, and using this information with appropriate technology to assist in carrying out and monitoring the intervention are best viewed as part of a complete patient-specific intervention process that occurs over many time scales. Further, the information generated in computer-integrated interventions can be captured and analyzed statistically to improve treatment processes. This paper will explore these themes briefly, using examples drawn from our work at the Engineering Research Center for Computer-Integrated Surgical Systems and Technology (CISST ERC).},
}

@inproceedings{Yilmaz2020,
  author = {Yilmaz, Nural and Wu, Jie Ying and Kazanzides, Peter and Tumerdem, Ugur},
  title = {Neural Network based Inverse Dynamics Identification and External Force Estimation on the da Vinci Research Kit},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/ICRA40945.2020.9197445},
  semanticscholar = {https://www.semanticscholar.org/paper/e66026db91fbab60592001e2378e45887c7c6513},
  research_field = {},
  data_type = {},
  abstract = {Most current surgical robotic systems lack the ability to sense tool/tissue interaction forces, which motivates research in methods to estimate these forces from other available measurements, primarily joint torques. These methods require the internal joint torques, due to the robot inverse dynamics, to be subtracted from the measured joint torques. This paper presents the use of neural networks to estimate the inverse dynamics of the da Vinci surgical robot, which enables estimation of the external environment forces. Experiments with motions in free space demonstrate that the neural networks can estimate the internal joint torques within 10% normalized rootmean-square error (NRMSE), which outperforms model-based approaches in the literature. Comparison with an external force sensor shows that the method is able to estimate environment forces within about 10% NRMSE.},
  isbn = {978-1-7281-7395-5},
}

@article{Abdelaal2020,
  author = {Abdelaal, Alaa Eldin and Mathur, Prateek and Salcudean, Septimiu E},
  title = {Robotics In Vivo: A Perspective on Human–Robot Interaction in Surgical Robotics},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  year = {2020},
  volume = {3},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-control-091219-013437},
  semanticscholar = {https://www.semanticscholar.org/paper/1211f289d081e173384a568520df293d3c73fdba},
  research_field = {RE},
  data_type = {},
  dvrk_site = {UBC},
  abstract = {This article reviews recent work on surgical robots that have been used or tested in vivo, focusing on aspects related to human–robot interaction. We present the general design requirements that should be considered when developing such robots, including the clinical requirements and the technologies needed to satisfy them. We also discuss the human aspects related to the design of these robots, considering the challenges facing surgeons when using robots in the operating room, and the safety issues of such systems. We then survey recent work in seven different surgical settings: urology and gynecology, orthopedic surgery, cardiac surgery, head and neck surgery, neurosurgery, radiotherapy, and bronchoscopy. We conclude with the open problems and recommendations on how to move forward in this research area.},
  issn = {2573-5144},
}

@article{Thananjeyan2020,
  author = {Thananjeyan, Brijen and Balakrishna, Ashwin and Rosolia, Ugo and Li, Felix and McAllister, Rowan and Gonzalez, Joseph E and Levine, Sergey and Borrelli, Francesco and Goldberg, Ken},
  title = {Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2976272},
  semanticscholar = {https://www.semanticscholar.org/paper/51fe965c579a689d1dc466f2313227a07eb22126},
  research_field = {AU},
  data_type = {RI and KD and ED},
  dvrk_site = {UCB},
  abstract = {Reinforcement learning (RL) for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations (SAVED), which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than <inline-formula><tex-math notation="LaTeX">$\text\{5\}\%$</tex-math></inline-formula> of the time while SAVED has a success rate of over <inline-formula><tex-math notation="LaTeX">$\text\{75\}\%$</tex-math></inline-formula> in the first 50 training iterations. Code and supplementary material is available at <uri>https://tinyurl.com/saved-rl</uri>.},
  issn = {2377-3766},
}

@inproceedings{Col2020,
  author = {Col, Tommaso Da and Mariani, Andrea and Deguet, Anton and Menciassi, Arianna and Kazanzides, Peter and Momi, Elena De},
  title = {SCAN : System for Camera Autonomous Navigation in Robotic-Assisted Surgery},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {},
  data_type = {},
}

@article{Mariani2020,
  author = {Mariani, Andrea and Pellegrini, Edoardo and {De Momi}, Elena},
  title = {Skill-oriented and Performance-driven Adaptive Curricula for Training in Robot-Assisted Surgery using Simulators: a Feasibility Study},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/TBME.2020.3011867},
  semanticscholar = {https://www.semanticscholar.org/paper/fa4a74bd7f1370b4f435a77444a83a10c130346d},
  research_field = {},
  data_type = {},
  dvrk_site = {POLIMI},
  abstract = {Objective: Virtual Reality (VR) simulators represent a remarkable educational opportunity in order to acquire and refine surgical practical skills. Nevertheless, there exists no consensus regarding a standard curriculum of simulation-based training. This study introduces an automatic, adaptive curriculum where the training session is real-time scheduled on the basis of the trainee's performances. Methods: An experimental study using the master console of the da Vinci Research Kit (Intuitive Surgical Inc., Sunnyvale, US) was carried out to test this approach. Tasks involving fundamental skills of robotic surgery were designed and simulated in VR. Twelve participants without medical background along with twelve medical residents were randomly and equally divided into two groups: a control group, self-managing the training session, and an experimental group, undergoing the proposed adaptive training. Results: The performances of the experimental users were significantly better with respect to the ones of the control group after training (non-medical: p < 0.01; medical: p = 0.02). This trend was analogous in the non-medical and medical populations and no significant difference was identified between these two classes (even in the baseline assessment). Conclusion: The analysis of the learning of the involved surgical skills highlighted how the proposed adaptive training managed to better identify and compensate for the trainee's gaps. The absence of initial difference between the non-medical and medical users underlines that robotic surgical devices require specific training before clinical practice. Significance: This feasibility study could pave the way towards the improvement of simulation-based training curricula.},
}

@article{Huan2020,
  author = {Huan, Yu and Tamadon, Izadyar and Scatena, Cristian and Cela, Vito and Naccarato, Antonio Giuseppe},
  title = {Soft Graspers for Safe and Effective Tissue Clutching in Minimally Invasive Surgery},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2020},
  volume = {9294},
  doi = {10.1109/TBME.2020.2996965},
  semanticscholar = {https://www.semanticscholar.org/paper/9942b975ec6d8fd9c13e4bc7833005e4a64d128c},
  research_field = {},
  data_type = {},
  dvrk_site = {SSSA},
  abstract = {Objective: Surgical graspers must be safe, not to damage tissue, and effective, to establish a stable contact for operation. For conventional rigid graspers, these requirements are conflicting and tissue damage is often induced. We thus proposed novel soft graspers, based on morphing jaws that increase contact area with clutching force. Methods: We introduced two soft jaw concepts: DJ and CJ. They were designed (using analytical and numerical models) and prototyped (10 mm diameter, 10 mm span). Corresponding graspers were obtained by integrating the jaws into a conventional tool used in the dVRK surgical robotics platform. Morphing performance was experimentally characterized. Jaw-tissue interaction was quantitatively assessed through damage indicators obtained from ex vivo tests and histological analysis, also comparing DJ, CJ and dVRK rigid jaws. Soft graspers were demonstrated through ex vivo tests on dVRK. Ex vivo tests and related analysis were devised/performed with medical doctors. Results: Design goal was achieved for both soft jaws: by morphing, contact area exceeded by 20–30% the maximum area allowed by encumbrance specifications to rigid jaws. Experimental characterization was in good agreement with model predictions (error ≈ 4%). Damage indicators showed differences amongst DJ, CJ and dVRK jaws (ANOVA p-value  =  0.0005): damage was one order of magnitude lower for soft graspers (each pairwise comparison was statistically significant). Conclusion: We proposed and demonstrated soft graspers potentially less harmful to tissue than conventional graspers. Significance: Beyond minimally invasive surgery, the proposed concepts and design methodology can foster the development of graspers for soft robotics.},
}

@inproceedings{Tagliabue2020,
  author = {Tagliabue, Eleonora and Pore, Ameya and Alba, Diego Dall and Magnabosco, Enrico and Piccinelli, Marco and Fiorini, Paolo},
  title = {Soft Tissue Simulation Environment to Learn Manipulation Tasks in Autonomous Robotic Surgery *},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  year = {2020},
  doi = {10.1109/IROS60139.2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  research_field = {AU},
  data_type = {RI and KD and DD and SD and ED},
}

@article{Li2020a,
  author = {Li, Yang and Richter, Florian and Lu, Jingpei and Funk, Emily K and Orosco, Ryan K and Zhu, Jianke and Yip, Michael C},
  title = {SuPer: A Surgical Perception Framework for Endoscopic Tissue Manipulation with Surgical Robotics},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2970659},
  semanticscholar = {https://www.semanticscholar.org/paper/4f885053ad0cd16ddcd6d65192caaebae411173f},
  arxiv = {https://arxiv.org/abs/1909.05405},
  research_field = {},
  data_type = {},
  dvrk_site = {UCSD},
  abstract = {Traditional control and task automation have been successfully demonstrated in a variety of structured, controlled environments through the use of highly specialized modeled robotic systems in conjunction with multiple sensors. However, the application of autonomy in endoscopic surgery is very challenging, particularly in soft tissue work, due to the lack of high-quality images and the unpredictable, constantly deforming environment. In this letter, we propose a novel surgical perception framework, SuPer, for surgical robotic control. This framework continuously collects 3D geometric information that allows for mapping a deformable surgical field while tracking rigid instruments within the field. To achieve this, a model-based tracker is employed to localize the surgical tool with a kinematic prior in conjunction with a model-free tracker to reconstruct the deformable environment and provide an estimated point cloud as a mapping of the environment. The proposed framework was implemented on the da Vinci Surgical System in real-time with an end-effector controller where the target configurations are set and regulated through the framework. Our proposed framework successfully completed soft tissue manipulation tasks with high accuracy. The demonstration of this novel framework is promising for the future of surgical autonomy. In addition, we provide our dataset for further surgical research.**Website: https://www.sites.google.com/ucsd.edu/super-framework.},
  issn = {2377-3766},
}

@article{Hwang2020a,
  author = {Hwang, Minho and Thananjeyan, Brijen and Seita, Daniel and Ichnowski, Jeffrey and Paradis, Samuel and Fer, Danyal and Low, Thomas and Goldberg, Ken},
  title = {Superhuman Surgical Peg Transfer Using Depth-Sensing and Deep Recurrent Neural Networks},
  journal = {arXiv preprint},
  year = {2020},
  semanticscholar = {https://www.semanticscholar.org/paper/5d58c24b6a1d7a093415121dcf245a3559f243b8},
  research_field = {},
  data_type = {},
  dvrk_site = {UBC},
}

@article{Bahar2020,
  author = {Bahar, Lidor and Sharon, Yarden and Nisky, Ilana},
  title = {Surgeon-Centered Analysis of Robot-Assisted Needle Driving Under Different Force Feedback Conditions},
  journal = {Frontiers in Neurorobotics},
  year = {2020},
  volume = {13},
  publisher = {Frontiers},
  doi = {10.3389/fnbot.2019.00108},
  semanticscholar = {https://www.semanticscholar.org/paper/dc6493ff295ee6c6f9a646ec2e4ea7c063e81e5b},
  research_field = {TR},
  data_type = {KD and DD and SD},
  dvrk_site = {BGUN},
  abstract = {Robotic assisted minimally invasive surgery (RAMIS) systems present many advantages to the surgeon and patient over open and standard laparoscopic surgery. However, haptic feedback, which is crucial for the success of many surgical procedures, is still an open challenge in RAMIS. Understanding the way that haptic feedback affects performance and learning can be useful in the development of haptic feedback algorithms and teleoperation control systems. In this study, we examined the performance and learning of inexperienced participants under different haptic feedback conditions in a task of surgical needle driving via a soft homogeneous deformable object—an artificial tissue. We designed an experimental setup to characterize their movement trajectories and the forces that they applied on the artificial tissue. Participants first performed the task in an open condition, with a standard surgical needle holder, followed by teleoperation in one of three feedback conditions: (1) no haptic feedback, (2) haptic feedback based on position exchange, and (3) haptic feedback based on direct recording from a force sensor, and then again with the open needle holder. To quantify the effect of different force feedback conditions on the quality of needle driving, we developed novel metrics that assess the kinematics of needle driving and the tissue interaction forces, and we combined our novel metrics with classical metrics. We analyzed the final teleoperated performance in each condition, the improvement during teleoperation, and the aftereffect of teleoperation on the performance when using the open needle driver. We found that there is no significant difference in the final performance and in the aftereffect between the 3 conditions. Only the two conditions with force feedback presented statistically significant improvement during teleoperation in several of the metrics, but when we compared directly between the improvements in the three different feedback conditions none of the effects reached statistical significance. We discuss possible explanations for the relative similarity in performance. We conclude that we developed several new metrics for the quality of surgical needle driving, but even with these detailed metrics, the advantage of state of the art force feedback methods to tasks that require interaction with homogeneous soft tissue is questionable.},
  issn = {1662-5218},
}

@incollection{Colleoni2020,
  author = {Colleoni, Emanuele and Edwards, Philip and Stoyanov, Danail},
  title = {Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention - MICCAI},
  year = {2020},
  publisher = {Medical Image Computing and Computer Assisted Intervention},
  doi = {10.1007/978-3-030-59716-0_67},
  semanticscholar = {https://www.semanticscholar.org/paper/04f42c6607d97d0e685cf33fd1d4635554eddfd1},
  arxiv = {https://arxiv.org/abs/2007.09107},
  research_field = {IM},
  data_type = {RI and KD and SD},
  abstract = {Semantic tool segmentation in surgical videos is important for surgical scene understanding and computer-assisted interventions as well as for the development of robotic automation. The problem is challenging because different illumination conditions, bleeding, smoke and occlusions can reduce algorithm robustness. At present labelled data for training deep learning models is still lacking for semantic surgical instrument segmentation and in this paper we show that it may be possible to use robot kinematic data coupled with laparoscopic images to alleviate the labelling problem. We propose a new deep learning based model for parallel processing of both laparoscopic and simulation images for robust segmentation of surgical tools. Due to the lack of laparoscopic frames annotated with both segmentation ground truth and kinematic information a new custom dataset was generated using the da Vinci Research Kit (dVRK) and is made available.},
}

@incollection{Lasso2020,
  author = {Lasso, Andras and Kazanzides, Peter},
  title = {System integration},
  booktitle = {Handbook of Medical Image Computing and Computer Assisted Intervention},
  year = {2020},
  publisher = {Elsevier},
  doi = {10.1016/c2017-0-04608-6},
  semanticscholar = {https://www.semanticscholar.org/paper/2e9c9324cfaf0d7a6fa90611deeed1fa32629040},
  research_field = {},
  data_type = {},
}

@article{Chua2020,
  author = {Chua, Zonghe and Jarc, Anthony M. and Wren, Sherry and Nisky, Ilana and Okamura, Allison M.},
  title = {Task Dynamics of Prior Training Influence Visual Force Estimation Ability During Teleoperation},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2020},
  doi = {10.1109/TMRB.2020.3023005},
  semanticscholar = {https://www.semanticscholar.org/paper/2158ac5446a898563fa984e8f60554671f2abf4a},
  arxiv = {https://arxiv.org/abs/2004.13226},
  research_field = {TR},
  data_type = {RI and KD and DD and ED},
  dvrk_site = {SU},
  abstract = {The lack of haptic feedback in teleoperation is a potential barrier to safe handling of soft materials, yet in Robot-assisted Minimally Invasive Surgery (RMIS), haptic feedback is often unavailable. Due to its availability in open and laparoscopic surgery, surgeons with such experience potentially possess learned models of tissue stiffness that might promote good force estimation abilities during RMIS. To test if prior haptic experience leads to improved force estimation ability in teleoperation, 33 naive participants were assigned to one of three training conditions: manual manipulation, teleoperation with force feedback, or teleoperation without force feedback, and learned to tension a silicone sample to a set of forces. They were then asked to perform the tension task, and a previously unencountered palpation task, to a different set of forces under teleoperation without force feedback. Compared to the teleoperation groups, the manual group had higher force error in the tension task outside the range of forces they had trained on, but showed better speed-accuracy functions in the palpation task at low force levels. This suggests that the dynamics of the training modality affect force estimation ability during teleoperation, with the prior haptic experience accessible if formed under the same dynamics as the task.},
  issn = {2576-3202},
}

@article{Leporini2020,
  author = {Leporini, Alice and Oleari, Elettra and Landolfo, Carmela and Sanna, Alberto and Larcher, Alessandro and Gandaglia, Giorgio and Fossati, Nicola and Muttin, Fabio and Capitanio, Umberto and Montorsi, Francesco and Salonia, Andrea and Minelli, Marco and Ferraguti, Federica and Secchi, Cristian and Farsoni, Saverio and Sozzi, Alessio and Bonfe, Marcello and Sayols, Narcis and Hernansanz, Albert and Casals, Alicia and Hertle, Sabine and Cuzzolin, Fabio and Dennison, Andrew and Melzer, Andreas and Kronreif, Gernot and Siracusano, Salvatore and Falezza, Fabio and Setti, Francesco and Muradore, Riccardo},
  title = {Technical and Functional Validation of a Teleoperated Multirobots Platform for Minimally Invasive Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2020},
  volume = {2},
  number = {2},
  doi = {10.1109/TMRB.2020.2990286},
  semanticscholar = {https://www.semanticscholar.org/paper/42644e0ea548b08b4aa66a56e6cffaf2d4077e64},
  research_field = {HW},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {UV},
  abstract = {Nowadays Robotic assisted Minimally Invasive Surgeries (R-MIS) are the elective procedures for treating highly accurate and scarcely invasive pathologies, thanks to their ability to empower surgeons’ dexterity and skills. The research on new Multi-Robots Surgery (MRS) platform is cardinal to the development of a new SARAS surgical robotic platform, which aims at carrying out autonomously the assistants tasks during R-MIS procedures. In this work, we will present the SARAS MRS platform validation protocol, framed in order to assess: (i) its technical performances in purely dexterity exercises, and (ii) its functional performances. The results obtained show a prototype able to put the users in the condition of accomplishing the tasks requested (both dexterity- and surgical-related), even with reasonably lower performances respect to the industrial standard. The main aspects on which further improvements are needed result to be the stability of the end effectors, the depth perception and the vision systems, to be enriched with dedicated virtual fixtures. The SARAS’ aim is to reduce the main surgeon’s workload through the automation of assistive tasks which would benefit both surgeons and patients by facilitating the surgery and reducing the operation time.},
  issn = {2576-3202},
}

@article{Liu2020a,
  author = {Liu, Huan and Selvaggio, Mario and Ferrentino, Pasquale and Moccia, Rocco and Pirozzi, Salvatore and Bracale, Umberto and Ficuciello, Fanny},
  title = {The MUSHA Hand II: A Multi-Functional Hand for Robot-Assisted Laparoscopic Surgery},
  journal = {IEEE/ASME Transactions on Mechatronics},
  year = {2020},
  doi = {10.1109/TMECH.2020.3022782},
  semanticscholar = {https://www.semanticscholar.org/paper/af53cb2e2e264546fa0d168ea524a0d06802c8a2},
  research_field = {},
  data_type = {},
  dvrk_site = {UNFII},
  abstract = {Although substantial progresses have been made in robot-assisted laparoscopic surgery, the graspers for existing surgical systems generally remain nonsensorized forceps design with limited functions. This article presents the design, development, and preliminary evaluation of the MUSHA Hand II, a multifunctional hand with force sensors for robot-assisted laparoscopic surgery. The proposed hand has three snake-like underactuated fingers that can be folded into a $\phi$12 mm cylindrical form. Each finger has a three-axis force sensor, to provide force information. After been deployed into an abdominal cavity, the hand can be configured to either grasper mode, retractor mode, or palpation mode for different tasks. Underactuated finger design enhances the adaptivity in grasping and the compliance in interaction with the environment. In addition, fingertip force sensors can be utilized for palpation to obtain a real-time stiffness map of organs. Using the da Vinci Research Kit as a robotic testbed, the functionality of the hand has been demonstrated and experiments have been conducted, including robotic palpation and organ manipulation. The results suggest that the hand can effectively enhance the functionality of a robotic surgical system and overcome the limits on force sensing introduced by the use of robots in laparoscopic surgery.},
  issn = {1083-4435},
}

@article{Wang2020a,
  author = {Wang, Chongyun and Komninos, Charalampos and Andersen, Stephanie and D'Ettorre, Claudia and Dwyer, George and Maneas, Efthymios and Edwards, Philip and Desjardins, Adrien and Stilli, Agostino and Stoyanov, Danail},
  title = {Ultrasound 3D reconstruction of malignant masses in robotic-assisted partial nephrectomy using the PAF rail system: a comparison study.},
  journal = {International journal of computer assisted radiology and surgery},
  year = {2020},
  publisher = {Springer},
  doi = {10.1007/s11548-020-02149-4},
  semanticscholar = {https://www.semanticscholar.org/paper/5ee158e176091938b84854839c7156cae98599ba},
  research_field = {HW},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {UCL},
  abstract = {Purpose In robotic-assisted partial nephrectomy (RAPN), the use of intraoperative ultrasound (IOUS) helps to localise and outline the tumours as well as the blood vessels within the kidney. The aim of this work is to evaluate the use of the pneumatically attachable flexible (PAF) rail system for US 3D reconstruction of malignant masses in RAPN. The PAF rail system is a novel device developed and previously presented by the authors to enable track-guided US scanning. Methods We present a comparison study between US 3D reconstruction of masses based on: the da Vinci Surgical System kinematics, single- and stereo-camera tracking of visual markers embedded on the probe. An US-realistic kidney phantom embedding a mass is used for testing. A new design for the US probe attachment to enhance the performance of the kinematic approach is presented. A feature extraction algorithm is proposed to detect the margins of the targeted mass in US images. Results To evaluate the performance of the investigated approaches the resulting 3D reconstructions have been compared to a CT scan of the phantom. The data collected indicates that single camera reconstruction outperformed the other approaches, reconstructing with a sub-millimetre accuracy the targeted mass. Conclusions This work demonstrates that the PAF rail system provides a reliable platform to enable accurate US 3D reconstruction of masses in RAPN procedures. The proposed system has also the potential to be employed in other surgical procedures such as hepatectomy or laparoscopic liver resection.},
  issn = {1861-6429},
}

@article{Jo2020,
  author = {Jo, Yeeun and Kim, Yoon Jae and Cho, Minwoo and Lee, Chiwon and Kim, Myungjoon and Moon, Hye-Min and Kim, Sungwan},
  title = {Virtual Reality-based Control of Robotic Endoscope in Laparoscopic Surgery},
  journal = {International Journal of Control, Automation and Systems},
  year = {2020},
  volume = {18},
  number = {1},
  publisher = {Springer},
  doi = {10.1007/s12555-019-0244-9},
  semanticscholar = {https://www.semanticscholar.org/paper/75d936fd8c83fdb1fca2dafb93c8ac3d0c05707c},
  research_field = {HW},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {SNU},
  issn = {1598-6446},
}

@article{Moccia2020,
  author = {Moccia, Rocco and Iacono, Cristina and Siciliano, Bruno and Ficuciello, Fanny},
  title = {Vision-based dynamic virtual fixtures for tools collision avoidance in robotic surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2969941},
  semanticscholar = {https://www.semanticscholar.org/paper/4524caf974eb70df7bec1eda98bb71c7ca6cb56d},
  research_field = {HW},
  data_type = {RI and KD and DD},
  dvrk_site = {UNFII},
  abstract = {In robot-aided surgery, during the execution of typical bimanual procedures such as dissection, surgical tools can collide and create serious damage to the robot or tissues. The da Vinci robot is one of the most advanced and certainly the most widespread robotic system dedicated to minimally invasive surgery. Although the procedures performed by da Vinci-like surgical robots are teleoperated, potential collisions between surgical tools are a very sensitive issue declared by surgeons. Shared control techniques based on Virtual Fixtures (VF) can be an effective way to help the surgeon prevent tools collision. This letter presents a surgical tools collision avoidance method that uses Forbidden Region Virtual Fixtures. Tool clashing is avoided by rendering a repulsive force to the surgeon. To ensure the correct definition of the VF, a marker-less tool tracking method, using deep neural network architecture for tool segmentation, is adopted. The use of direct kinematics for tools collision avoidance is affected by tools position error introduced by robot component elasticity during tools interaction with the environment. On the other hand, kinematics information can help in case of occlusions of the camera. Therefore, this work proposes an Extended Kalman Filter (EKF) for pose estimation which ensures a more robust application of VF on the tool, coupling vision and kinematics information. The entire pipeline is tested in different tasks using the da Vinci Research Kit system.},
  issn = {2377-3766},
}

@article{Ma2020,
  author = {Ma, Xin and Song, Chengzhi and Chiu, Philip Waiyan and Li, Zheng},
  title = {Visual Servo of a 6-DOF Robotic Stereo Flexible Endoscope Based on da Vincix Research Kit (dVRK) System},
  journal = {IEEE Robotics and Automation Letters},
  year = {2020},
  volume = {5},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2020.2965863},
  semanticscholar = {https://www.semanticscholar.org/paper/be0cdde41dc1e65998f356992fd884e7ff92bf6e},
  research_field = {HW},
  data_type = {RI and KD and ED},
  dvrk_site = {CUHK},
  abstract = {Endoscopes play an important role in minimally invasive surgery (MIS). Due to the advantages of less occupied motion space and enhanced safety, flexible endoscopes are drawing more and more attention. However, the structure of the flexible section makes it difficult for surgeons to manually rotate and guide the view of endoscopes. To solve these problems, we developed a 6-DOF robotic stereo flexible endoscope (RSFE) based on the da Vinci Research Kit (dVRK). Then an image-based endoscope guidance method with depth information is proposed for the RSFE. With this method, the view and insertion depth of the RSFE can be adjusted by tracking the surgical instruments automatically. Additionally, an image-based view rotation control method is proposed, with which the rotation of the view can be controlled by tracking two surgical instruments. The experimental results show that the proposed methods control the direction and rotation of the view of the flexible endoscope faster than the manual control method. Lastly, an ex vivo experiment is performed to demonstrate the feasibility of the proposed control method and system.},
  issn = {2377-3766},
}

@inproceedings{Molnar2020,
  author = {Molnar, Cecilia and Nagy, Tamas D. and Elek, Renata Nagyne and Haidegger, Tamas},
  title = {Visual servoing-based camera control for the da Vinci Surgical System},
  booktitle = {2020 IEEE 18th International Symposium on Intelligent Systems and Informatics (SISY)},
  year = {2020},
  publisher = {IEEE},
  doi = {10.1109/SISY50555.2020.9217086},
  semanticscholar = {https://www.semanticscholar.org/paper/bcb04ef6c47aa6eab1fa7aedd2435028778fe02e},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {Minimally Invasive Surgery (MIS)–which is a very beneficial technique to the patient but can be challenging to the surgeon–includes endoscopic camera handling by an assistant (traditional MIS) or a robotic arm under the control of the operator (Robot-Assisted MIS, RAMIS). Since in the case of RAMIS the endoscopic image is the sole sensory input, it is essential to keep the surgical tools in the field-of-view of the camera for patient safety reasons. Based on the endoscopic images, the movement of the endoscope holder arm can be automated by visual servoing techniques, which can reduce the risk of medical error. In this paper, we propose a marker-based visual servoing technique for automated camera positioning in the case of RAMIS. The method was validated on the research-enhanced da Vinci Surgical System. The implemented method is available at: https://github.com/ABC-iRobotics/irob-saf/tree/visua1_servoing},
  isbn = {978-1-7281-7352-8},
}

@misc{Hoque2020,
  author = {Hoque, Ryan and Seita, Daniel and Balakrishna, Ashwin and Ganapathi, Aditya and Tanwani, Ajay Kumar and Jamali, Nawid and Yamane, Katsu and Iba, Soshi and Goldberg, Ken},
  title = {VisuoSpatial foresight for multi-step, multi-task fabric manipulation},
  booktitle = {Robotics: Science and Systems (RSS)},
  year = {2020},
  doi = {10.15607/rss.2020.xvi.034},
  semanticscholar = {https://www.semanticscholar.org/paper/369aea426e4602ab0963da50c1ee88aa08754fdf},
  arxiv = {https://arxiv.org/abs/2003.09044},
  research_field = {},
  data_type = {},
  abstract = {Robotic fabric manipulation has applications in home robotics, textiles, senior care and surgery. Existing fabric manipulation techniques, however, are designed for specific tasks, making it difficult to generalize across different but related tasks. We extend the Visual Foresight framework to learn fabric dynamics that can be efficiently reused to accomplish different fabric manipulation tasks with a single goal-conditioned policy. We introduce VisuoSpatial Foresight (VSF), which builds on prior work by learning visual dynamics on domain randomized RGB images and depth maps simultaneously and completely in simulation. We experimentally evaluate VSF on multi-step fabric smoothing and folding tasks against 5 baseline methods in simulation and on the da Vinci Research Kit (dVRK) surgical robot without any demonstrations at train or test time. Furthermore, we find that leveraging depth significantly improves performance. RGBD data yields an 80% improvement in fabric folding success rate over pure RGB data. Code, data, videos, and supplementary material are available at this https URL.},
  archiveprefix = {arXiv},
  arxivid = {2003.09044},
  eprint = {2003.09044},
  issn = {23318422},
}

@article{Chrysilla2019,
  author = {Chrysilla, Grace and Eusman, Nickolas and Deguet, Anton and Kazanzides, Peter},
  title = {A Compliance Model to Improve the Accuracy of the da Vinci Research Kit (dVRK)},
  journal = {Acta Polytechnica Hungarica},
  year = {2019},
  volume = {16},
  number = {8},
  doi = {10.12700/aph.16.8.2019.8.4},
  semanticscholar = {https://www.semanticscholar.org/paper/922b22f7dda3b65e5b68148f10644ff24914cb06},
  research_field = {HW},
  data_type = {KD and DD and SD},
  dvrk_site = {JHU},
  abstract = {: The da Vinci surgical robot is widely used for minimally-invasive surgery. It inserts multiple articulated instruments through small incisions into the patient. The robot system contains encoders to measure joint displacements which, when combined with the kinematic model of the robot, measures the instrument position and orientation. But, the accuracy of these measurements is affected by non-kinematic errors, such as bending of the instruments due to applied loads. We develop a compliance model that relates displacement of the first two joints of the da Vinci Patient Side Manipulator (PSM) to lateral forces applied to the instrument shaft. This model enables us to compensate for these errors based on the measured joint efforts, which are derived from the measured motor currents. We perform experiments with the open-source da Vinci Research Kit (dVRK) to estimate the model parameters and to evaluate the accuracy improvement that results from application of this model. Preliminary results indicate that the model-based correction can reduce instrument position error due to externally-applied forces.},
}

@article{Wang2019c,
  author = {Wang, Yan and Gondokaryono, Radian and Munawar, Adnan and Fischer, Gregory Scott},
  title = {A Convex Optimization-based Dynamic Model Identification Package for the da Vinci Research Kit},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  doi = {10.1109/LRA.2019.2927947},
  semanticscholar = {https://www.semanticscholar.org/paper/71d6271db34dde1616d2c5a1b0d1c26fba51fa81},
  arxiv = {https://arxiv.org/abs/1902.10875},
  research_field = {SS},
  data_type = {DD},
  dvrk_site = {WPI},
  abstract = {The da Vinci Research Kit (dVRK) is a teleoperated surgical robotic system. For dynamic simulations and model-based control, the dynamic model of the dVRK is required. We present an open-source dynamic model identification package for the dVRK, capable of modeling the parallelograms, springs, counterweight, and tendon couplings, which are inherent to the dVRK. A convex optimization-based method is used to identify the dynamic parameters of the dVRK subject to physical consistency. Experimental results show the effectiveness of the modeling and the robustness of the package. Although this software package is originally developed for the dVRK, it is feasible to apply it on other similar robots.},
  issn = {2377-3766},
}

@article{Nagy2019,
  author = {Nagy, Tam{\'{a}}s D{\'{a}}niel and Haidegger, Tam{\'{a}}s},
  title = {A DVRK-based framework for surgical subtask automation},
  journal = {Acta Polytechnica Hungarica},
  year = {2019},
  pages = {61--78},
  publisher = {{\'{O}}buda University},
  doi = {10.12700/aph.16.8.2019.8.5},
  semanticscholar = {https://www.semanticscholar.org/paper/d64175e3bbf4073d1a23a31c59c37f1feb5b9612},
  research_field = {AU},
  data_type = {RI and KD and SD},
  dvrk_site = {OU},
  abstract = {: Robotic assistance is becoming a standard in Minimally Invasive Surgery. Despite its clinical beneﬁts and technical potential, surgeons still have to perform manually a number of monotonous and time-consuming surgical subtasks, like knot-tying or blunt dissection. Many believe that the next bold step in the advancement of robotic surgery is the automation of such subtasks. Partial automation can reduce the cognitive load on surgeons, and support them in paying more attention to the critical elements of the surgical workﬂow. Our aim was to develop a software framework to ease and hasten the automation of surgical subtasks. This framework was built alongside the Da Vinci Research Kit (DVRK), while it can be ported onto other robotic platforms, since it is based on the Robot Operating System (ROS). The software includes both stereo vision-based and hierarchical motion planning, with a wide palette of often used surgical gestures—such as grasping, cutting or soft tissue manipulation—as building blocks to support the high-level implementation of autonomous surgical subtask execution routines. This open-source surgical automation framework—named irob-saf —is available at https://github.com/ABC-iRobotics/irob-saf .},
  issn = {1785-8860},
}

@inproceedings{Ginesi2019,
  author = {Ginesi, Michele and Meli, Daniele and Nakawala, Hirenkumar and Roberti, Andrea and Fiorini, Paolo},
  title = {A knowledge-based framework for task automation in surgery},
  booktitle = {2019 19th International Conference on Advanced Robotics (ICAR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icar46387.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/8037d8cc267f6ad7129de2ea6f71513a69c0bc29},
  research_field = {},
  data_type = {},
  isbn = {1728124670},
}

@inproceedings{Setti2019,
  author = {Setti, Francesco and Oleari, Elettra and Leporini, Alice and Trojaniello, Diana and Sanna, Alberto and Capitanio, Umberto and Montorsi, Francesco and Salonia, Andrea and Muradore, Riccardo},
  title = {A Multirobots Teleoperated Platform for Artificial Intelligence Training Data Collection in Minimally Invasive Surgery},
  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/ISMR.2019.8710209},
  semanticscholar = {https://www.semanticscholar.org/paper/2ce4415da0f510e2468ada95b88a2f96fd557cc4},
  research_field = {AU},
  data_type = {RI and KD and DD},
  abstract = {Dexterity and perception capabilities of surgical robots may soon be improved by cognitive functions that can support surgeons in decision making and performance monitoring, and enhance the impact of automation within the operating rooms. Nowadays, the basic elements of autonomy in robotic surgery are still not well understood and their mutual interaction is unexplored. Current classification of autonomy encompasses six basic levels: Level 0: no autonomy; Level 1: robot assistance; Level 2: task autonomy; Level 3: conditional autonomy; Level 4: high autonomy. Level 5: full autonomy. The practical meaning of each level and the necessary technologies to move from one level to the next are the subject of intense debate and development. In this paper, we discuss the first outcomes of the European funded project Smart Autonomous Robotic Assistant Surgeon (SARAS). SARAS will develop a cognitive architecture able to make decisions based on pre-operative knowledge and on scene understanding via advanced machine learning algorithms. To reach this ambitious goal that allows us to reach Level 1 and 2, it is of paramount importance to collect reliable data to train the algorithms. We will present the experimental setup to collect the data for a complex surgical procedure (Robotic Assisted Radical Prostatectomy) on very sophisticated manikins (i.e. phantoms of the inflated human abdomen). The SARAS platform allows the main surgeon and the assistant to teleoperate two independent two-arm robots. The data acquired with this platform (videos, kinematics, audio) will be used in our project and will be released (with annotations) for research purposes.},
  isbn = {978-1-5386-7825-1},
}

@inproceedings{Ferro2019a,
  author = {Ferro, Marco and Brunori, Damiano and Magistri, Federico and Saiella, Lorenzo and Selvaggio, Mario and Fontanelli, Giuseppe Andrea},
  title = {A Portable da Vinci Simulator in Virtual Reality},
  booktitle = {2019 Third IEEE International Conference on Robotic Computing (IRC)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/IRC.2019.00093},
  semanticscholar = {https://www.semanticscholar.org/paper/84f62b19c9095754a0e64db3a05718298c20ae41},
  research_field = {},
  data_type = {},
  abstract = {Research activity in Minimally Invasive Robotic Surgery (MIRS) has gained a considerable momentum in the last years, due to the availability of reliable and clinically relevant research platforms like the da Vinci Research Kit (dVRK). However, despite the wide sharing of the dVRK in the research community, access to the platform remains limited because of high maintenance costs and difficulty in replacing components. In this work we complete a robotic simulator of the dVRK, previously developed by our group, with cheap haptic interfaces and an Oculus Rift to replicate and extend the functionalities of the Master console. The complete system represents an efficient, safe and low-cost tool, useful to design and validate new surgical instruments and control strategies, as well as provide an easyto-access educational tool to students.},
  isbn = {978-1-5386-9245-5},
}

@inproceedings{Munawar2019,
  author = {Munawar, Adnan and Wang, Yan and Gondokaryono, Radian and Fischer, Gregory S},
  title = {A real-time dynamic simulator and an associated front-end representation format for simulating complex robots and environments},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2019},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90},
  research_field = {},
  data_type = {},
  isbn = {1728140048},
}

@article{Lin2019,
  author = {Lin, Hongbin and Hui, Chiu-Wai Vincent and Wang, Yan and Deguet, Anton and Kazanzides, Peter and Au, K W Samuel},
  title = {A Reliable Gravity Compensation Control Strategy for dVRK Robotic Arms With Nonlinear Disturbance Forces},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  volume = {4},
  number = {4},
  publisher = {IEEE},
  doi = {10.1109/LRA.2019.2927953},
  semanticscholar = {https://www.semanticscholar.org/paper/16913ea312e4307d409031f6f64c04cdaebf3499},
  arxiv = {https://arxiv.org/abs/2001.06156},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {CUHK},
  abstract = {External disturbance forces caused by nonlinear springy electrical cables in the master tool manipulator (MTM) of the da Vinci Research Kit (dVRK) limits the usage of the existing gravity compensation methods. Significant motion drifts at the MTM tip are often observed when the MTM is located far from its identification trajectory, preventing the usage of these methods for the entire workspace reliably. In this letter, we propose a general and systematic framework to address the problems of the gravity compensation for the MTM of the dVRK. Particularly, high-order polynomial models were used to capture the highly nonlinear disturbance forces and integrated with the multi-step least square estimation framework. This method allows us to identify the parameters of both the gravitational and disturbance forces for each link sequentially, preventing residual error passing among the links of the MTM with uneven mass distribution. A corresponding gravity compensation controller was developed to compensate the gravitational and disturbance forces. The method was validated with extensive experiments in the majority of the manipulator's workspace, showing significant performance enhancements over existing methods. Finally, a deliverable software package in MATLAB and C++ was integrated with dVRK and published in the dVRK community for open-source research and development.},
  issn = {2377-3766},
}

@article{Qian2019a,
  author = {Qian, Long and Wu, Jie Ying and DiMaio, Simon and Navab, Nassir and Kazanzides, Peter},
  title = {A Review of Augmented Reality in Robotic-Assisted Surgery},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/TMRB.2019.2957061},
  semanticscholar = {https://www.semanticscholar.org/paper/760105262d700ad2b0dfca8430cb4c8ee97f7ae6},
  research_field = {},
  data_type = {},
  dvrk_site = {JHU},
  abstract = {Augmented reality (AR) and robotic-assisted surgery (RAS) are both rapidly evolving technologies in recent years. RAS systems, such as the da Vinci Surgical System, aim to improve surgical precision and dexterity, as well as access to minimally-invasive procedures, while AR provides an advanced interface to enhance user perception. Combining the features of both, AR-integrated RAS has become an appealing concept with increased interest among the academic community. In this paper, we review the existing literature about AR-integrated RAS. We discuss the hardware components, application paradigms and clinical relevance of the literature. The concept of AR-integrated RAS has been shown to be feasible for various procedures. Encouraging preliminary results include reduced sight diversion and improved situation awareness. Special techniques, e.g., activation-on-demand, are taken into consideration to address visual clutter of the AR interface and ensure that the system is fail-safe. Although AR-integrated RAS is not yet mature, we believe that if the current trend of development continues, it will soon demonstrate its clinical value.},
  issn = {2576-3202},
}

@article{Song2019,
  author = {Song, Chengzhi and Ma, Xin and Xia, Xianfeng and Chiu, Philip Wai Yan and Chong, Charing Ching Ning and Li, Zheng},
  title = {A robotic flexible endoscope with shared autonomy: a study of mockup cholecystectomy},
  journal = {Surgical endoscopy},
  year = {2019},
  publisher = {Springer},
  doi = {10.1007/s00464-019-07241-8},
  semanticscholar = {https://www.semanticscholar.org/paper/d41d9f68bd45e963dfb506be08fefc0d63e8dc95},
  research_field = {},
  data_type = {},
  dvrk_site = {CUHK},
  issn = {1432-2218},
}

@article{Pandya2019a,
  author = {Pandya, Abhilash and Eslamian, Shahab and Ying, Hao and Nokleby, Matthew and Reisner, Luke A.},
  title = {A Robotic Recording and Playback Platform for Training Surgeons and Learning Autonomous Behaviors Using the da Vinci Surgical System},
  journal = {Robotics},
  year = {2019},
  volume = {8},
  number = {1},
  doi = {10.3390/robotics8010009},
  semanticscholar = {https://www.semanticscholar.org/paper/197a57d17f2c2f19b72bd93aac6ca8f6bd2f8f09},
  research_field = {TR},
  data_type = {RI and KD and DD and SD},
  dvrk_site = {WSU},
  abstract = {This paper describes a recording and playback system developed using a da Vinci Standard Surgical System and research kit. The system records stereo laparoscopic videos, robot arm joint angles, and surgeon–console interactions in a synchronized manner. A user can then, on-demand and at adjustable speeds, watch stereo videos and feel recorded movements on the hand controllers of entire procedures or sub procedures. Currently, there is no reported comprehensive ability to capture expert surgeon movements and insights and reproduce them on hardware directly. This system has important applications in several areas: (1) training of surgeons, (2) collection of learning data for the development of advanced control algorithms and intelligent autonomous behaviors, and (3) use as a “black box” for retrospective error analysis. We show a prototype of such an immersive system on a clinically-relevant platform along with its recording and playback fidelity. Lastly, we convey possible research avenues to create better systems for training and assisting robotic surgeons.},
  issn = {2218-6581},
}

@inproceedings{Marinho2019,
  author = {Marinho, Murilo M and Adorno, Bruno V and Harada, Kanako and Deie, Kyoichi and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H and Mitsuishi, Mamoru},
  title = {A unified framework for the teleoperation of surgical robots in constrained workspaces},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {HW},
  data_type = {RI and KD and DD and SD and ED},
  isbn = {153866027X},
}

@article{Avinash2019b,
  author = {Avinash, Apeksha and Abdelaal, Alaa Eldin and Mathur, Prateek and Salcudean, Septimiu E.},
  title = {A “pickup” stereoscopic camera with visual-motor aligned control for the da Vinci surgical system: a preliminary study},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2019},
  volume = {14},
  number = {7},
  doi = {10.1007/s11548-019-01955-9},
  semanticscholar = {https://www.semanticscholar.org/paper/0521167e315f4426987cceba8c8396c4faaf800f},
  research_field = {},
  data_type = {},
  dvrk_site = {UBC},
  issn = {1861-6410},
}

@inproceedings{Banach2019,
  author = {Banach, Artur and Leibrandt, Konrad and Grammatikopoulou, Maria and Yang, Guang-Zhong},
  title = {Active contraints for tool-shaft collision avoidance in minimally invasive surgery},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {HW},
  data_type = {RI and DD and SD},
  isbn = {153866027X},
}

@article{Li2019,
  author = {Li, Weibing and Song, Chengzhi and Li, Zheng},
  title = {An Accelerated Recurrent Neural Network for Visual Servo Control of a Robotic Flexible Endoscope with Joint Limit Constraint},
  journal = {IEEE Transactions on Industrial Electronics},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/TIE.2019.2959481},
  semanticscholar = {https://www.semanticscholar.org/paper/d7c57e7af91d5b46c7cda8308ed4a54dca35afb2},
  research_field = {},
  data_type = {},
  dvrk_site = {CUHK},
  abstract = {In this article, a recurrent neural network (RNN) is accelerated and applied to visual servo control of a physically constrained robotic flexible endoscope. The robotic endoscope consists of a patient side manipulator of the da Vinci Research Kit platform and a flexible endoscope working as an end-effector. To automate the robotic endoscope, kinematic modeling for visual servoing is conducted, leading to a quadratic programming (QP) control framework incorporating kinematic and physical constraints of the robotic endoscope. To solve the QP problem and realize the vision-based control, an RNN accelerated to finite-time convergence by a sign-bipower activation function (SBPAF) is proposed. The finite-time convergence of the RNN is theoretically proved in the sense of Lyapunov, showing that the SBPAF activated RNN exhibits a faster convergence speed as compared with its predecessor. To validate the efficacy of the RNN model and the control framework, simulations are performed using a simulated flexible endoscope in the robot operating system. Physical experiment is then further performed to verify the feasibility of the RNN model and the control framework. Both simulation and experimental results demonstrate that the proposed RNN solution is effective to achieve visual servoing and handle physical limits of the robotic endoscope simultaneously.},
  issn = {0278-0046},
}

@article{Gondokaryono2019,
  author = {Gondokaryono, Radian A and Agrawal, Ankur and Munawar, Adnan and Nycz, Christopher J and Fischer, Gregory S},
  title = {An Approach to Modeling Closed-Loop Kinematic Chain Mechanisms, Applied to Simulations of the da Vinci Surgical System},
  journal = {Acta Polytechnica Hungarica},
  year = {2019},
  volume = {16},
  number = {8},
  doi = {10.12700/aph.16.8.2019.8.3},
  semanticscholar = {https://www.semanticscholar.org/paper/4439636218c25289c1e736563eb1c936ee07561e},
  research_field = {SS},
  data_type = {KD and DD},
  dvrk_site = {WPI},
  abstract = {Open-sourced kinematic models of the da Vinci Surgical System have previously been developed using serial chains for forward and inverse kinematics. However, these models do not describe the motion of every link in the closed-loop mechanism of the da Vinci manipulators; knowing the kinematics of all components in motion is essential for the foundation of modeling the system dynamics and implementing representative simulations. This paper proposes a modeling method of the closed-loop kinematics, using the existing da Vinci kinematics and an optical motion capture link length calibration. Resulting link lengths and DH parameters are presented and used as the basis for ROS-based simulation models. The models were simulated in RViz visualization simulation and Gazebo dynamics simulation. Additionally, the closed-loop kinematic chain was verified by comparing the remote center of motion location of simulation with the hardware. Furthermore, the dynamic simulation resulted in satisfactory joint stability and performance. All models and simulations are provided as an open-source package.},
}

@inproceedings{Munawar2019a,
  author = {Munawar, Adnan and Fischer, Gregory S},
  title = {An Asynchronous Multi-Body Simulation Framework for Real-Time Dynamics, Haptics and Learning with Application to Surgical Robots},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2019},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90},
  research_field = {AU},
  data_type = {KD and DD},
  isbn = {1728140048},
}

@inproceedings{Qian2019b,
  author = {Qian, Long and Deguet, Anton and Wang, Zerui and Liu, Yun-Hui and Kazanzides, Peter},
  title = {Augmented reality assisted instrument insertion and tool manipulation for the first assistant in robotic surgery},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {HW},
  data_type = {RI and KD and DD and SD and ED},
  isbn = {153866027X},
}

@inproceedings{Richter2019b,
  author = {Richter, Florian and Zhang, Yifei and Zhi, Yuheng and Orosco, Ryan K and Yip, Michael C},
  title = {Augmented reality predictive displays to help mitigate the effects of delayed telesurgery},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {},
  data_type = {},
  isbn = {153866027X},
}

@inproceedings{Sundaresan2019,
  author = {Sundaresan, Priya and Thananjeyan, Brijen and Chiu, Johnathan and Fer, Danyal and Goldberg, Ken},
  title = {Automated Extraction of Surgical Needles from Tissue Phantoms},
  booktitle = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/case41385.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/59acaaa887009df13c9a31715cab6aa26b4c1f4b},
  research_field = {},
  data_type = {},
  isbn = {1728103568},
}

@article{Ma2019b,
  author = {Ma, Xin and Song, Chengzhi and Chiu, Philip Waiyan and Li, Zheng},
  title = {Autonomous Flexible Endoscope for Minimally Invasive Surgery With Enhanced Safety},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  volume = {4},
  number = {3},
  doi = {10.1109/LRA.2019.2895273},
  semanticscholar = {https://www.semanticscholar.org/paper/96fcfac950605112f51ae230f0cfe3448dc1d8f1},
  research_field = {AU},
  data_type = {RI and KD and SD},
  dvrk_site = {CUHK},
  abstract = {Automation in robotic surgery has become an increasingly attractive topic. Although full automation remains fictional, task autonomy and conditional autonomy are highly achievable. Apart from the performance of task fulfillment, one major concern in robotic surgery is safety. In this paper, we present a flexible endoscope that can help to guide the minimally invasive surgical operations automatically. It is developed based on the tendon-driven continuum mechanism and is integrated with the da Vinci research kit. In total, the proposed flexible endoscope has six degree-of-freedoms. Visual servoing is adopted to automatically track the surgical instruments. During the tracking, optimal control method is used to minimize the motion and space occupation of the flexible endoscope, which will improve the safety of both the robot system and the assistants nearby. Compared with the existing rigid endoscope, both the experimental results and the user study results show that the proposed flexible endoscope has advantages of being safer and less space occupation without reducing its comfort level.},
  issn = {2377-3766},
}

@inproceedings{Shin2019b,
  author = {Shin, Changyeob and Ferguson, Peter Walker and Pedram, Sahba Aghajani and Ma, Ji and Dutson, Erik P. and Rosen, Jacob},
  title = {Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control},
  booktitle = {2019 IEEE International Conference On Robotics and Automation (ICRA)},
  year = {2019},
  doi = {10.1109/ICRA.2019.8794159},
  semanticscholar = {https://www.semanticscholar.org/paper/d9ad92812d61709a9bf35b09078361d8bffd3f7a},
  arxiv = {https://arxiv.org/abs/1902.01459},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {Tissue manipulation is a frequently used fundamental subtask of any surgical procedures, and in some cases it may require the involvement of a surgeon’s assistant. The complex dynamics of soft tissue as an unstructured environment is one of the main challenges in any attempt to automate the manipulation of it via a surgical robotic system. Two AI learning based model predictive control algorithms using vision strategies are proposed and studied: (1) reinforcement learning and (2) learning from demonstration. Comparison of the performance of these AI algorithms in a simulation setting indicated that the learning from demonstration algorithm can boost the learning policy by initializing the predicted dynamics with given demonstrations. Furthermore, the learning from demonstration algorithm is implemented on a Raven IV surgical robotic system and successfully demonstrated feasibility of the proposed algorithm using an experimental approach. This study is part of a profound vision in which the role of a surgeon will be redefined as a pure decision maker whereas the vast majority of the manipulation will be conducted autonomously by a surgical robotic system. A supplementary video can be found at: http://bionics.seas.ucla.edu/research/surgeryproject17.html},
}

@article{Haidegger2019a,
  author = {Haidegger, Tamas},
  title = {Autonomy for Surgical Robots: Concepts and Paradigms},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  year = {2019},
  volume = {1},
  number = {2},
  doi = {10.1109/TMRB.2019.2913282},
  semanticscholar = {https://www.semanticscholar.org/paper/0af00e43658fe5dacaec577a943619351b8ad230},
  research_field = {},
  data_type = {},
  dvrk_site = {OU},
  abstract = {Robot-assisted and computer-integrated surgery provides innovative, minimally invasive solutions to heal complex injuries and diseases. The dominant portion of these surgical interventions has been performed with master–slave teleoperation systems, which are not capable of autonomous task execution or cognitive decision making. Much of the most advanced technologies foundered on the drawing boards or at the research labs for a long time, partially due to the fact that the surgical domain is resistant to the introduction of new hazards via the increased complexity of novel solutions. It has been seen with similar heavily regulated areas that internationally accepted standards can facilitate the adoption of new technologies in a safe manner. This paper reviews the existing autonomous capabilities of surgical robots, and investigates the major barriers of development presented by the lack of autonomy benchmarks and standards. The emerging safety standard environment is presented, as a key enabling factor to the commercialization of autonomous surgical robots. A practical scale is introduced to assess the level of autonomy of current and future surgical robots. Regarding the forthcoming robotic platforms, it is crucial to improve the transparency of the regulatory environment, streamline the standardization framework, and increase the social acceptance.},
  issn = {2576-3202},
}

@article{Ficuciello2019a,
  author = {Ficuciello, Fanny and Tamburrini, Guglielmo and Arezzo, Alberto and Villani, Luigi and Siciliano, Bruno},
  title = {Autonomy in surgical robots and its meaningful human control},
  journal = {Paladyn, Journal of Behavioral Robotics},
  year = {2019},
  volume = {10},
  number = {1},
  doi = {10.1515/pjbr-2019-0002},
  semanticscholar = {https://www.semanticscholar.org/paper/285dd7840ee2b5f80edb8ae551527f49e858652f},
  research_field = {},
  data_type = {},
  dvrk_site = {UNFII},
  abstract = {Abstract This article focuses on ethical issues raised by increasing levels of autonomy for surgical robots. These ethical issues are explored mainly by reference to state-ofart case studies and imminent advances in Minimally Invasive Surgery (MIS) and Microsurgery. In both area, surgicalworkspace is limited and the required precision is high. For this reason, increasing levels of robotic autonomy can make a significant difference there, and ethically justified control sharing between humans and robots must be introduced. In particular, from a responsibility and accountability perspective suitable policies for theMeaningfulHuman Control (MHC) of increasingly autonomous surgical robots are proposed. It is highlighted how MHC should be modulated in accordance with various levels of autonomy for MIS and Microsurgery robots. Moreover, finer MHC distinctions are introduced to deal with contextual conditions concerning e.g. soft or rigid anatomical environments.},
  issn = {2081-4836},
}

@inproceedings{Mikic2019,
  author = {Mikic, Marko and Francis, Peter and Looi, Thomas and Gerstle, J Ted and Drake, James},
  title = {Bone Conduction Headphones for Force Feedback in Robotic Surgery},
  booktitle = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/embc43219.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/1473f421ade8ac1b0f9db1e4195d7707515478ae},
  research_field = {},
  data_type = {},
  isbn = {1538613115},
}

@inproceedings{Mariani2019a,
  author = {Mariani, Andrea and Colaci, Giorgia and Sanna, Nicole and Vendrame, Vendrame and Menciassi, Arianna and {De Momi}, Elena},
  title = {Comparing Users Performances in a Virtual Reality Surgical Task under Different Camera Control Modalities: a Pilot Study Towards the Introduction of an Autonomous Camera},
  booktitle = {Proceeding of Joint Workshop on Computer/Robot Assisted Surgery (CRAS)},
  year = {2019},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2},
  research_field = {},
  data_type = {},
}

@article{Colleoni2019a,
  author = {Colleoni, Emanuele and Moccia, Sara and Du, Xiaofei and {De Momi}, Elena and Stoyanov, Danail},
  title = {Deep Learning Based Robotic Tool Detection and Articulation Estimation With Spatio-Temporal Layers},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  volume = {4},
  number = {3},
  doi = {10.1109/LRA.2019.2917163},
  semanticscholar = {https://www.semanticscholar.org/paper/0a566ec0eabf0e9ce602d757f8e98da80d0fce69},
  research_field = {AU},
  data_type = {RI and KD and SD},
  dvrk_site = {UCL},
  abstract = {Surgical-tool joint detection from laparoscopic images is an important but challenging task in computer-assisted minimally invasive surgery. Illumination levels, variations in background and the different number of tools in the field of view, all pose difficulties to algorithm and model training. Yet, such challenges could be potentially tackled by exploiting the temporal information in laparoscopic videos to avoid per frame handling of the problem. In this letter, we propose a novel encoder–decoder architecture for surgical instrument joint detection and localization that uses three-dimensional convolutional layers to exploit spatio-temporal features from laparoscopic videos. When tested on benchmark and custom-built datasets, a median Dice similarity coefficient of 85.1% with an interquartile range of 4.6% highlights performance better than the state of the art based on single-frame processing. Alongside novelty of the network architecture, the idea for inclusion of temporal information appears to be particularly useful when processing images with unseen backgrounds during the training phase, which indicates that spatio-temporal features for joint detection help to generalize the solution.},
  issn = {2377-3766},
}

@inproceedings{Cheng2019a,
  author = {Cheng, Zhuoqi and Dall'Alba, Diego and Caldwell, Darwin G and Fiorini, Paolo and Mattos, Leonardo S},
  title = {Design and Integration of Electrical Bio-Impedance Sensing in a Bipolar Forceps for Soft Tissue Identification: A Feasibility Study},
  booktitle = {International Conference on Electrical Bioimpedance},
  year = {2019},
  publisher = {Springer},
  doi = {10.1088/1742-6596/224/1/011001},
  semanticscholar = {https://www.semanticscholar.org/paper/08cb7275459e899d0c64ee4d8eee8b78591275a9},
  research_field = {HW},
  data_type = {RI and KD and SD},
  abstract = {The XIVth International Conference on Electrical Bioimpedance, held in conjunction with the 11th Conference on Biomedical Applications of EIT (ICEBI & EIT 2010), took place from 4–8 April 2010 in the Reitz Union of the University of Florida, in Gainesville, USA. This was the first time since its inception in 1969 that the ICEBI was held in the United States. As in the last three conferences (Graz 2007, Gdansk 2004 and Oslo 2001) the ICEBI was combined with the Conference on Biomedical Applications of EIT – a mutually beneficial approach for those interested in the biophysics of tissue electrical properties and those developing imaging methods and measurement systems based thereon. This year's conference was particularly notable for the many papers presented on hybrid and emerging imaging techniques such as Electric Property Tomography (EPT), Magneto Acoustic Tomography using Magnetic Induction (MAT-MI) and Magnetic Resonance Electrical Impedance Tomography (MREIT); sessions on Cell Scale Impedance, Cardiac Impedance and Imaging Neural Activity. About 180 scientists from all over the world attended, including keynote speakers on topics of fundamental electromagnetic principles (Jaakko Malmivuo), Electrical Source and Impedance Imaging (Bin He), Bioimpedance applications in Nephrology, (Nathan Levin), and Lung EIT (Gerhard Wolf). The papers in this volume are peer-reviewed four-page works selected from over 150 presented in oral and poster sessions at the conference. The complete program is available from the conference website.},
}

@article{Cheng2019b,
  author = {Cheng, Zhuoqi and Dall'Alba, Diego and Foti, Simone and Mariani, Andrea and Chupin, Thibaud and Caldwell, Darwin G. and Ferrigno, Giancarlo and {De Momi}, Elena and Mattos, Leonardo S. and Fiorini, Paolo},
  title = {Design and Integration of Electrical Bio-impedance Sensing in Surgical Robotic Tools for Tissue Identification and Display},
  journal = {Frontiers in Robotics and AI},
  year = {2019},
  volume = {6},
  doi = {10.3389/frobt.2019.00055},
  semanticscholar = {https://www.semanticscholar.org/paper/f9c1c0c2ac503358cae7db3ad6ba639f7d20211e},
  research_field = {HW},
  data_type = {RI and KD and SD},
  dvrk_site = {UV},
  abstract = {The integration of intra-operative sensors into surgical robots is a hot research topic since this can significantly facilitate complex surgical procedures by enhancing surgical awareness with real-time tissue information. However, currently available intra-operative sensing technologies are mainly based on image processing and force feedback, which normally require heavy computation or complicated hardware modifications of existing surgical tools. This paper presents the design and integration of electrical bio-impedance sensing into a commercial surgical robot tool, leading to the creation of a novel smart instrument that allows the identification of tissues by simply touching them. In addition, an advanced user interface is designed to provide guidance during the use of the system and to allow augmented-reality visualization of the tissue identification results. The proposed system imposes minor hardware modifications to an existing surgical tool, but adds the capability to provide a wealth of data about the tissue being manipulated. This has great potential to allow the surgeon (or an autonomous robotic system) to better understand the surgical environment. To evaluate the system, a series of ex-vivo experiments were conducted. The experimental results demonstrate that the proposed sensing system can successfully identify different tissue types with 100% classification accuracy. In addition, the user interface was shown to effectively and intuitively guide the user to measure the electrical impedance of the target tissue, presenting the identification results as augmented-reality markers for simple and immediate recognition.},
  issn = {2296-9144},
}

@inproceedings{Zhang2019d,
  author = {Zhang, Dandan and Liu, Jindong and Zhang, Lin and Yang, Guang-Zhong},
  title = {Design and verification of a portable master manipulator based on an effective workspace analysis framework},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2019},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90},
  research_field = {HW},
  data_type = {RI and SD and ED},
  isbn = {1728140048},
}

@inproceedings{Dimitri2019,
  author = {Dimitri, M and Gentili, G B and Staderini, F and Brancadoro, M and Menciassi, A and Coratti, A and Cianchi, F and Corvi, A and Capineri, L},
  title = {Developrnent of a Robotic Surgical System of Thermal Ablation and Microwave Coagulation},
  booktitle = {2019 PhotonIcs & Electromagnetics Research Symposium-Spring (PIERS-Spring)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/piers-spring46901.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/6c007bbfd098515b358a57d27972c3d510b1e26f},
  research_field = {},
  data_type = {},
  isbn = {172813403X},
}

@article{Zhong2019,
  author = {Zhong, Fangxun and Wang, Yaqing and Wang, Zerui and Liu, Yun-Hui},
  title = {Dual-Arm Robotic Needle Insertion With Active Tissue Deformation for Autonomous Suturing},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  volume = {4},
  number = {3},
  publisher = {IEEE},
  doi = {10.1109/LRA.2019.2913082},
  semanticscholar = {https://www.semanticscholar.org/paper/a6bd27cee22069d3ccf56c3162cbbe78b2360c91},
  research_field = {},
  data_type = {},
  dvrk_site = {CUHK},
  abstract = {A major issue for needle insertion into soft tissue during suturing is the induced tissue deformation that hinders the minimization of tip-target positioning error. In this letter, we present a new robot control framework to solve target deviation by integrating active deformation control. We characterize the motion behavior of the desired target under needle-tissue interaction by introducing the needle-induced deformation matrix. Note that the modeling does not require the exact knowledge of tissue or needle insertion properties. The unknown parameters are online updated during the insertion procedure by an adaptive estimator via sensor-based measurement. A closed-loop controller is then proposed for dual-arm robotic execution upon image guidance. The dual-arm control aims to regulate a feature vector concerning the tip-target alignment to ensure target reachability. The feasibility of the proposed algorithm is studied via simulations and experiments on different biological tissues to simulate robotic minimally-invasive suturing using the da Vinci Research Kit as the control platform.},
  issn = {2377-3766},
}

@inproceedings{Qian2019,
  author = {Qian, Long and Deguet, Anton and Kazanzides, Peter},
  title = {dVRK-XR: Mixed Reality Extension for da Vinci Research Kit},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2019},
  publisher = {The Hamlyn Centre, Faculty of Engineering, Imperial College London},
  doi = {10.31256/HSMR2019.47},
  semanticscholar = {https://www.semanticscholar.org/paper/011be270619c0a58ced1a641ac9c3ecfe650933a},
  research_field = {},
  data_type = {},
  abstract = {INTRODUCTION The da Vinci Research Kit (dVRK) has become a widely adopted research platform for surgical robotics research since its initial public release in 2012. To date, it has been installed at 35 institutes worldwide. In the past decade, research interest in mixed reality (including augmented reality and virtual reality) has increased among both the robotics and medical communities [1], due to the improved usability of mixed reality headsets and the much reduced hardware cost. In this paper, we describe dVRK-XR (https://github.com/jhu-dvrk/dvrkxr), an extension to the dVRK open source package that facilitates the integration of mixed reality in surgical robotics research.},
}

@inproceedings{Pique2019a,
  author = {Piqu{\'{e}}, Francesco and Boushaki, Mohamed N and Brancadoro, Margherita and {De Momi}, Elena and Menciassi, Arianna},
  title = {Dynamic modeling of the da Vinci research kit arm for the estimation of interaction wrench},
  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/ismr44918.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/8aa5230692467f704ac8decfeaf514dc93ba09f2},
  research_field = {},
  data_type = {},
  isbn = {153867825X},
}

@article{Sozzi2019,
  author = {Sozzi, Alessio and Bonf{\`{e}}, Marcello and Farsoni, Saverio and {De Rossi}, Giacomo and Muradore, Riccardo},
  title = {Dynamic Motion Planning for Autonomous Assistive Surgical Robots},
  journal = {Electronics},
  year = {2019},
  volume = {8},
  number = {9},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/electronics8090957},
  semanticscholar = {https://www.semanticscholar.org/paper/e094bd8275dcec62865a473975ae00e6a76b080e},
  research_field = {AU},
  data_type = {RI and KD and DD and ED},
  dvrk_site = {UV},
  abstract = {The paper addresses the problem of the generation of collision-free trajectories for a robotic manipulator, operating in a scenario in which obstacles may be moving at non-negligible velocities. In particular, the paper aims to present a trajectory generation solution that is fully executable in real-time and that can reactively adapt to both dynamic changes of the environment and fast reconfiguration of the robotic task. The proposed motion planner extends the method based on a dynamical system to cope with the peculiar kinematics of surgical robots for laparoscopic operations, the mechanical constraint being enforced by the fixed point of insertion into the abdomen of the patient the most challenging aspect. The paper includes a validation of the trajectory generator in both simulated and experimental scenarios.},
}

@inproceedings{DiFlumeri2019,
  author = {{Di Flumeri}, Gianluca and Aric{\`{o}}, Pietro and Borghini, Gianluca and Sciaraffa, Nicolina and Ronca, Vincenzo and Vozzi, Alessia and Storti, Silvia Francesca and Menegaz, Gloria and Fiorini, Paolo and Babiloni, Fabio},
  title = {EEG-Based Workload Index as a Taxonomic Tool to Evaluate the Similarity of Different Robot-Assisted Surgery Systems},
  booktitle = {International Symposium on Human Mental Workload: Models and Applications},
  year = {2019},
  publisher = {Springer},
  doi = {10.1007/978-3-030-91408-0},
  semanticscholar = {https://www.semanticscholar.org/paper/5b4bc2edf19576e6979d1497e49aeef8c7a82726},
  research_field = {TR},
  data_type = {RI and KD and ED},
}

@inproceedings{Nagy2019a,
  author = {Nagy, Tam{\'{a}}s D and Ukhrenkov, Nikita and Drexler, Daniel A and Tak{\'{a}}cs, {\'{A}}rp{\'{a}}d and Haidegger, Tam{\'{a}}s},
  title = {Enabling quantitative analysis of situation awareness: system architecture for autonomous vehicle handover studies},
  booktitle = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/smc43495.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/ead4dcd0293d0ccdf7649082cd4ec151e0c9660d},
  research_field = {HW},
  data_type = {RI and KD and DD and ED},
  isbn = {1728145694},
}

@inproceedings{Pryor2019,
  author = {Pryor, Will and Vagvolgyi, Balazs P and Gallagher, William J and Deguet, Anton and Leonard, Simon and Whitcomb, Louis L and Kazanzides, Peter},
  title = {Experimental Evaluation of Teleoperation Interfaces for Cutting of Satellite Insulation},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {HW},
  data_type = {RI and KD and DD and SD and ED},
  isbn = {153866027X},
}

@article{Wu2019,
  author = {Wu, Chuhao and Cha, Jackie and Sulek, Jay and Zhou, Tian and Sundaram, Chandru P and Wachs, Juan and Yu, Denny},
  title = {Eye-Tracking Metrics Predict Perceived Workload in Robotic Surgical Skills Training},
  journal = {Human Factors},
  year = {2019},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  doi = {10.1177/0018720819874544},
  semanticscholar = {https://www.semanticscholar.org/paper/a3fb9cb50ee91a2d5cb8ee3f1ace5203ce8204b6},
  research_field = {},
  data_type = {},
  dvrk_site = {PU},
  abstract = {Objective The aim of this study is to assess the relationship between eye-tracking measures and perceived workload in robotic surgical tasks. Background Robotic techniques provide improved dexterity, stereoscopic vision, and ergonomic control system over laparoscopic surgery, but the complexity of the interfaces and operations may pose new challenges to surgeons and compromise patient safety. Limited studies have objectively quantified workload and its impact on performance in robotic surgery. Although not yet implemented in robotic surgery, minimally intrusive and continuous eye-tracking metrics have been shown to be sensitive to changes in workload in other domains. Methods Eight surgical trainees participated in 15 robotic skills simulation sessions. In each session, participants performed up to 12 simulated exercises. Correlation and mixed-effects analyses were conducted to explore the relationships between eye-tracking metrics and perceived workload. Machine learning classifiers were used to determine the sensitivity of differentiating between low and high workload with eye-tracking features. Results Gaze entropy increased as perceived workload increased, with a correlation of .51. Pupil diameter and gaze entropy distinguished differences in workload between task difficulty levels, and both metrics increased as task level difficulty increased. The classification model using eye-tracking features achieved an accuracy of 84.7% in predicting workload levels. Conclusion Eye-tracking measures can detect perceived workload during robotic tasks. They can potentially be used to identify task contributors to high workload and provide measures for robotic surgery training. Application Workload assessment can be used for real-time monitoring of workload in robotic surgical training and provide assessments for performance and learning.},
  issn = {0018-7208},
}

@article{Saracino2019a,
  author = {Saracino, Arianna and Deguet, Anton and Staderini, Fabio and Boushaki, Mohamed Nassim and Cianchi, Fabio and Menciassi, Arianna and Sinibaldi, Edoardo},
  title = {Haptic feedback in the da Vinci Research Kit (dVRK): A user study based on grasping, palpation, and incision tasks},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2019},
  volume = {15},
  number = {4},
  doi = {10.1002/rcs.1999},
  semanticscholar = {https://www.semanticscholar.org/paper/17526c58ac9e6ac4289a53e28aaa777fb7400dd0},
  research_field = {},
  data_type = {},
  dvrk_site = {SSSA},
  abstract = {It was suggested that the lack of haptic feedback, formerly considered a limitation for the da Vinci robotic system, does not affect robotic surgeons because of training and compensation based on visual feedback. However, conclusive studies are still missing, and the interest in force reflection is rising again.},
  issn = {1478-5951},
}

@inproceedings{Selvaggio2019a,
  author = {Selvaggio, Mario and {Ghalamzan Esfahani}, Amir and Moccia, Rocco and Ficuciello, Fanny and Siciliano, Bruno},
  title = {Haptic-guided shared control for needle grasping optimization in minimally invasive robotic surgery},
  booktitle = {IEEE/RSJ International Conference Intelligent Robotic System},
  year = {2019},
  doi = {10.1109/IROS.1992.594560},
  semanticscholar = {https://www.semanticscholar.org/paper/44a5c3422c6de7559c5ec10bcf93f04c2d301693},
  research_field = {HW},
  data_type = {KD and DD},
}

@article{Hong2019c,
  author = {Hong, Nhayoung and Kim, Myungjoon and Lee, Chiwon and Kim, Sungwan},
  title = {Head-mounted interface for intuitive vision control and continuous surgical operation in a surgical robot system},
  journal = {Medical & Biological Engineering & Computing},
  year = {2019},
  volume = {57},
  number = {3},
  pages = {=},
  doi = {10.1007/s11517-018-1902-4},
  semanticscholar = {https://www.semanticscholar.org/paper/aa5abb28646fe8d4325c8c9bf84d986f20800c2d},
  research_field = {HW},
  data_type = {RI and KD and SD},
  dvrk_site = {SNU},
  issn = {0140-0118},
}

@article{Rau2019,
  author = {Rau, Anita and Edwards, P J Eddie and Ahmad, Omer F and Riordan, Paul and Janatka, Mirek and Lovat, Laurence B and Stoyanov, Danail},
  title = {Implicit domain adaptation with conditional generative adversarial networks for depth prediction in endoscopy},
  journal = {International journal of computer assisted radiology and surgery},
  year = {2019},
  volume = {14},
  number = {7},
  publisher = {Springer},
  doi = {10.1007/s11548-019-01962-w},
  semanticscholar = {https://www.semanticscholar.org/paper/5069f827e5a165330aef63707f973e68bd96f37d},
  research_field = {},
  data_type = {},
  dvrk_site = {UCL},
  abstract = {PurposeColorectal cancer is the third most common cancer worldwide, and early therapeutic treatment of precancerous tissue during colonoscopy is crucial for better prognosis and can be curative. Navigation within the colon and comprehensive inspection of the endoluminal tissue are key to successful colonoscopy but can vary with the skill and experience of the endoscopist. Computer-assisted interventions in colonoscopy can provide better support tools for mapping the colon to ensure complete examination and for automatically detecting abnormal tissue regions.MethodsWe train the conditional generative adversarial network pix2pix, to transform monocular endoscopic images to depth, which can be a building block in a navigational pipeline or be used to measure the size of polyps during colonoscopy. To overcome the lack of labelled training data in endoscopy, we propose to use simulation environments and to additionally train the generator and discriminator of the model on unlabelled real video frames in order to adapt to real colonoscopy environments.ResultsWe report promising results on synthetic, phantom and real datasets and show that generative models outperform discriminative models when predicting depth from colonoscopy images, in terms of both accuracy and robustness towards changes in domains.ConclusionsTraining the discriminator and generator of the model on real images, we show that our model performs implicit domain adaptation, which is a key step towards bridging the gap between synthetic and real data. Importantly, we demonstrate the feasibility of training a single model to predict depth from both synthetic and real images without the need for explicit, unsupervised transformer networks mapping between the domains of synthetic and real data.},
  issn = {1861-6410},
}

@article{OSullivan2019a,
  author = {O'Sullivan, Shane and Nevejans, Nathalie and Allen, Colin and Blyth, Andrew and Leonard, Simon and Pagallo, Ugo and Holzinger, Katharina and Holzinger, Andreas and Sajid, Mohammed Imran and Ashrafian, Hutan},
  title = {Legal, regulatory, and ethical frameworks for development of standards in artificial intelligence (AI) and autonomous robotic surgery},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2019},
  volume = {15},
  number = {1},
  doi = {10.1002/rcs.1968},
  semanticscholar = {https://www.semanticscholar.org/paper/1683a3a8d4be1c72bf086251ffc02263674e9d53},
  research_field = {RE},
  data_type = {},
  dvrk_site = {ICL},
  abstract = {This paper aims to move the debate forward regarding the potential for artificial intelligence (AI) and autonomous robotic surgery with a particular focus on ethics, regulation and legal aspects (such as civil law, international law, tort law, liability, medical malpractice, privacy and product/device legislation, among other aspects).},
  issn = {14785951},
}

@inproceedings{Wang2019b,
  author = {Wang, Congcong and Mohammed, Ahmed Kedir and Cheikh, Faouzi Alaya and Beghdadi, Azeddine and Elle, Ole Jacob},
  title = {Multiscale deep desmoking for laparoscopic surgery},
  booktitle = {Medical Imaging 2019: Image Processing},
  year = {2019},
  publisher = {International Society for Optics and Photonics},
  doi = {10.1117/12.2532364},
  semanticscholar = {https://www.semanticscholar.org/paper/0e5b822eb7e7d3858e4ab6676bee9f73aa652d1d},
  research_field = {IM},
  data_type = {RI},
}

@article{Richter2019a,
  author = {Richter, Florian and Orosco, Ryan K and Yip, Michael C},
  title = {Open-sourced reinforcement learning environments for surgical robotics},
  journal = {arXiv preprint arXiv:1903.02090},
  year = {2019},
  url = {https://arxiv.org/abs/1903.02090},
  semanticscholar = {https://www.semanticscholar.org/paper/8c247678b0f652b9c8f01d200767b6b5c5199cc0},
  arxiv = {https://arxiv.org/abs/1903.02090},
  research_field = {},
  data_type = {},
  dvrk_site = {UCSD},
  abstract = {Reinforcement Learning (RL) is a machine learning framework for artificially intelligent systems to solve a variety of complex problems. Recent years has seen a surge of successes solving challenging games and smaller domain problems, including simple though non-specific robotic manipulation and grasping tasks. Rapid successes in RL have come in part due to the strong collaborative effort by the RL community to work on common, open-sourced environment simulators such as OpenAI's Gym that allow for expedited development and valid comparisons between different, state-of-art strategies. In this paper, we aim to bridge the RL and the surgical robotics communities by presenting the first open-sourced reinforcement learning environments for surgical robotics, called dVRL. Through the proposed RL environment, which are functionally equivalent to Gym, we show that it is easy to prototype and implement state-of-art RL algorithms on surgical robotics problems that aim to introduce autonomous robotic precision and accuracy to assisting, collaborative, or repetitive tasks during surgery. Learned policies are furthermore successfully transferable to a real robot. Finally, combining dVRL with the over 40+ international network of da Vinci Surgical Research Kits in active use at academic institutions, we see dVRL as enabling the broad surgical robotics community to fully leverage the newest strategies in reinforcement learning, and for reinforcement learning scientists with no knowledge of surgical robotics to test and develop new algorithms that can solve the real-world, high-impact challenges in autonomous surgery.},
}

@inproceedings{Thananjeyan2019c,
  author = {Thananjeyan, Brijen and Tanwani, Ajay and Ji, Jessica and Fer, Danyal and Patel, Vatsal and Krishnan, Sanjay and Goldberg, Ken},
  title = {Optimizing Robot-Assisted Surgery Suture Plans to Avoid Joint Limits and Singularities},
  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/ISMR.2019.8710194},
  semanticscholar = {https://www.semanticscholar.org/paper/27930fb5246613500e56b21f010ec04262099463},
  research_field = {},
  data_type = {},
  abstract = {Laparoscopic robots such as the da Vinci Research Kit encounter joint limits and singularities during procedures, leading to errors and prolonged operating times. We propose the Circle Suture Placement Problem to optimize the location and direction of four evenly-spaced stay sutures on surgical mesh for robot-assisted hernia surgery. We present an algorithm for this problem that runs in 0.4 seconds on a desktop equipped with commodity hardware. Simulated results integrating data from expert surgeon demonstrations suggest that optimizing over both suture position and direction increases dexterity reward by 11%-57% over baseline algorithms that optimize over either suture position or direction only.},
  isbn = {978-1-5386-7825-1},
}

@article{Stilli2019b,
  author = {Stilli, Agostino and Dimitrakakis, Emmanouil and D'Ettorre, Claudia and Tran, Maxine and Stoyanov, Danail},
  title = {Pneumatically Attachable Flexible Rails for Track-Guided Ultrasound Scanning in Robotic-Assisted Partial Nephrectomy—A Preliminary Design Study},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  volume = {4},
  number = {2},
  doi = {10.1109/LRA.2019.2894499},
  semanticscholar = {https://www.semanticscholar.org/paper/9176c34c53547893b05fcbc14ad093685f699f23},
  research_field = {HW},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {UCL},
  abstract = {Robotic-assisted partial nephrectomy is a surgical operation in which part of a kidney is removed typically because of the presence of a mass. Pre-operative and intraoperative imaging techniques are used to identify and outline the target mass, thus the margins of the resection area on the kidney surface. Drop-in ultrasound probes are used to acquire intraoperative images: the probe is inserted through a trocar port, grasped with a robotic-assisted laparoscopic gripper and swiped on the kidney surface. Multiple swipes are performed to define the resection area. This is marked swipe by swipe using an electrocautery tool. During this procedure the probe often requires repositioning because of slippage from the target organ surface. Furthermore, the localization can be inaccurate when the target mass is in locations particularly hard to reach, and thus kidney repositioning could be required. A highly skilled surgeon is typically required to successfully perform this pre-operatory procedure. We propose a novel approach for the navigation of drop-in ultrasound probes: the use of pneumatically attachable flexible rails to enable swift, effortless, and accurate track-guided scanning of the kidney. The proposed system attaches on the kidney side surface with the use of a series of bio-inspired vacuum suckers. In this letter, the design of the proposed system and its use in robotic-assisted partial nephrectomy are presented for the first time.},
  issn = {2377-3766},
}

@inproceedings{Zhang2019b,
  author = {Zhang, Ada and Guo, Liheng and Jarc, Anthony M.},
  title = {Prediction of task-based, surgeon efficiency metrics during robotic-assisted minimally invasive surgery},
  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/ISMR.2019.8710177},
  semanticscholar = {https://www.semanticscholar.org/paper/59dafb5aaa0a2e9660180540ef2f6d4ce5c3fac6},
  research_field = {},
  data_type = {},
  abstract = {We present a machine learning algorithm for predicting surgeon performance on future tasks given performance on previous tasks during robotic-assisted surgery. Performance is estimated via six heuristics, called metrics, which are derived from da Vinci Surgical System data streams and which have previously demonstrated significance in differentiating expertise. We use a boosted ensemble of regression trees, which learns a regression from 193 system data features to one of the six target metrics. Predictions from our algorithm are evaluated on whether they achieve significantly lower error than two intelligent guesses: (1) guessing a surgeon performs the future task exactly as how she performed the previous task (GUESS_PREV) and (2) guessing a surgeon will have average performance on the future task (GUESS_MEAN). We show that our algorithm can predict metrics of future tasks, significantly $(p\lt 0.05)$ outperforming GUESS_PREV on 19 out of 36 metrics and task pairs and GUESS_MEAN on 8 out of 36 metrics and task pairs. This work serves as a first step towards pre-operative performance estimation, where a surgeon can proactively improve performance rather than relying solely on post-operative feedback},
  isbn = {978-1-5386-7825-1},
}

@article{Nagy2019b,
  author = {Nagy, Tam{\'{a}}s D and Haidegger, Tam{\'{a}}s},
  title = {Recent Advances in Robot-Assisted Surgery: Soft Tissue Contact Identification},
  journal = {2019 IEEE 13th International Symposium on Applied Computational Intelligence and Informatics ( SACI)},
  year = {2019},
  doi = {10.1109/SACI46893.2019.9111599},
  semanticscholar = {https://www.semanticscholar.org/paper/64215bc2dd771c6810bcee78624de2487bac17c5},
  research_field = {},
  data_type = {},
  dvrk_site = {OU},
  abstract = {Robot-Assisted Minimally Invasive Surgery (RAMIS) is becoming standard-of-care in western medicine. RAMIS offers better patient outcome compared to traditional open surgery, however, the surgeons’ ability to identify the tissues with the sense of touch is missing from most robotic systems. Regarding haptic feedback, the most promising diagnostic technique is probably palpation; a physical contact examination method through which information can be gathered about the underlying structures by gently pressing with the fingers. In open surgery, palpation is widely used to identify blood vessels, tendons or even tumors; and the knowledge on the exact location of such elements is often crucial with respect to the outcome of the intervention. This paper presents a review of the actual research directions in the field of palpation in RAMIS.},
}

@article{Dardona2019,
  author = {Dardona, Tareq and Eslamian, Shahab and Reisner, Luke A and Pandya, Abhilash},
  title = {Remote presence: Development and usability evaluation of a head-mounted display for camera control on the da vinci surgical system},
  journal = {Robotics},
  year = {2019},
  volume = {8},
  number = {2},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/ROBOTICS8020031},
  semanticscholar = {https://www.semanticscholar.org/paper/d531c046c4319465e1265f297343879d55765a25},
  research_field = {HW},
  data_type = {RI and KD and DD and ED},
  dvrk_site = {WSU},
  abstract = {This paper describes the development of a new method to control the camera arm of a surgical robot and create a better sense of remote presence for the surgeon. The current surgical systems are entirely controlled by the surgeon, using hand controllers and foot pedals to manipulate either the instrument or the camera arms. The surgeon must pause the operation to move the camera arm to obtain a desired view and then resume the operation. The camera and tools cannot be moved simultaneously, leading to interrupted and unnatural movements. These interruptions can lead to medical errors and extended operation times. In our system, the surgeon controls the camera arm by his natural head movements while being immersed in a 3D-stereo view of the scene with a head-mounted display (HMD). The novel approach enables the camera arm to be maneuvered based on sensors of the HMD. We implemented this method on a da Vinci Standard Surgical System using the HTC Vive headset along with the Unity engine and the Robot Operating System framework. This paper includes the result of a subjective six-participant usability study that compares the workload of the traditional clutched camera control method against the HMD-based control. Initial results indicate that the system is usable, stable, and has a lower physical and mental workload when using the HMD control method.},
}

@article{Nakawala2019a,
  author = {Nakawala, Hirenkumar and {De Momi}, Elena and Tzemanaki, Antonia and Dogramadzi, Sanja and Russo, Andrea and Catellani, Michele and Bianchi, Roberto and {De Cobelli}, Ottavio and Sideridis, Aristotelis and Papacostas, Emmanuel},
  title = {Requirements elicitation for robotic and computer-assisted minimally invasive surgery},
  journal = {International Journal of Advanced Robotic Systems},
  year = {2019},
  volume = {16},
  number = {4},
  publisher = {SAGE Publications Sage UK: London, England},
  doi = {10.1177/1729881419865805},
  semanticscholar = {https://www.semanticscholar.org/paper/ad1562ff37728723d22ecc72e0158bd7cb04dd59},
  research_field = {RE},
  data_type = {},
  dvrk_site = {POLIMI},
  abstract = {The robotic surgical systems and computer-assisted technologies market has seen impressive growth over the last decades, but uptake by end-users is still scarce. The purpose of this article is to provide a comprehensive and informed list of the end-user requirements for the development of new generation robot- and computer-assisted surgical systems and the methodology for eliciting them. The requirements were elicited, in the frame of the EU project SMARTsurg, by conducting interviews on use cases of chosen urology, cardiovascular and orthopaedics procedures, tailored to provide clinical foundations for scientific and technical developments. The structured interviews resulted in detailed requirement specifications which are ranked according to their priorities. Paradigmatic surgical scenarios support the use cases.},
  issn = {1729-8814},
}

@article{NagyneElek2019,
  author = {{Nagyn{\'{e}} Elek}, Ren{\'{a}}ta and Haidegger, Tam{\'{a}}s},
  title = {Robot-Assisted Minimally Invasive Surgical Skill Assessment—Manual and Automated Platforms},
  journal = {Acta Polytechnica Hungarica},
  year = {2019},
  volume = {16},
  number = {8},
  publisher = {{\'{O}}buda University},
  doi = {10.12700/aph.16.8.2019.8.9},
  semanticscholar = {https://www.semanticscholar.org/paper/0925224abe277b9b368988d6f5fc2cf1ac3bd9b3},
  research_field = {RE},
  data_type = {},
  dvrk_site = {OU},
  abstract = {The practice of Robot-Assisted Minimally Invasive Surgery (RAMIS) requires extensive skills from the human surgeons due to the special input device control, such as moving the surgical instruments, use of buttons, knobs, foot pedals and so. The global popularity of RAMIS created the need to objectively assess surgical skills, not just for quality assurance reasons, but for training feedback as well. Nowadays, there is still no routine surgical skill assessment happening during RAMIS training and education in the clinical practice. In this paper, a review of the manual and automated RAMIS skill assessment techniques is provided, focusing on their general applicability, robustness and clinical relevance.},
  issn = {1785-8860},
}

@inproceedings{Schwaner2019,
  author = {Schwaner, Kim Lindberg and Dall'Alba, Diego and Cheng, Zhuoqi and Mattos, Leonardo S and Fiorini, Paolo and Savarimuthu, Thiusius Rajeeth},
  title = {Robotically assisted electrical bio-impedance measurements for soft tissue characterization: a feasibility study},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2019},
  publisher = {The Hamlyn Centre, Faculty of Engineering, Imperial College London},
  doi = {10.31256/hsmr2018},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  research_field = {HW},
  data_type = {RI and KD and ED},
}

@inproceedings{Hamedani2019,
  author = {Hamedani, Mohammad Hossein and Selvaggio, Mario and Rahimkhani, Mahtab and Ficuciello, Fanny and Sadeghian, Hamid and Zekri, Maryam and Sheikholeslam, Farid},
  title = {Robust Dynamic Surface Control of da Vinci Robot Manipulator Considering Uncertainties: A Fuzzy Based Approach},
  booktitle = {2019 7th International Conference on Robotics and Mechatronics (ICRoM)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icrom48714.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/5bcb6fa9e22dc4bd9200fa1acfefd80a6aea0925},
  research_field = {},
  data_type = {},
  isbn = {1728166047},
}

@inproceedings{Ettorre2019b,
  author = {Ettorre, C. D. and Stilli, A and Dwyer, G and Neves, J B and Tran, M and Stoyanov, D},
  title = {Semi-Autonomous Interventional Manipulation using Pneumatically Attachable Flexible Rails},
  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/IROS40897.2019.8967789},
  semanticscholar = {https://www.semanticscholar.org/paper/f60f67bdd453a5b337bc3e3ad9d6eba6d91c6e33},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {During laparoscopic surgery, tissues frequently need to be retracted and mobilized for manipulation or visualisation. State-of-the-art robotic platforms for minimally invasive surgery (MIS) typically rely on rigid tools to interact with soft tissues. Such tools offer a very narrow contact surface thus applying relatively large forces that can lead to tissue damage, posing a risk for the success of the procedure and ultimately for the patient. In this paper, we show how the use of Pneumatically Attachable Flexible (PAF) rail, a vacuum-based soft attachment for laparoscopic applications, can reduce such risk by offering a larger contact surface between the tool and the tissue. Ex vivo experiments are presented investigating the short- and long-term effects of different levels of vacuum pressure on the tissues surface. These experiments aim at evaluating the best trade-off between applied pressure, potential damage, task duration and connection stability. A hybrid control system has been developed to perform and investigate the organ repositioning task using the proposed system. The task is only partially automated allowing the surgeon to be part of the control loop. A gradient-based planning algorithm is integrated with learning from teleoperation algorithm which allows the robot to improve the learned trajectory. The use of Similar Smooth Path Repositioning (SSPR) algorithm is proposed to improve a demonstrated trajectory based on a known cost function. The results obtained show that a smoother trajectory allows to decrease the minimum level of pressure needed to guarantee active suction during PAF positioning and placement.},
  isbn = {9781728140049},
  issn = {21530866},
}

@inproceedings{Li2019a,
  author = {Li, Xiang and Wang, Zerui and Liu, Yun-Hui},
  title = {Sequential Robotic Manipulation for Active Shape Control of Deformable Linear Objects},
  booktitle = {2019 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/rcar47638.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/4a7f50847cc8689ce804e1397f7faec1370f56fc},
  research_field = {AU},
  data_type = {RI and KD and SD and ED},
  isbn = {1728137268},
}

@inproceedings{Mariani2019,
  author = {Mariani, Andrea and Pellegrini, Edoardo and Menciassi, Arianna and {De Momi}, Elena},
  title = {Simulation-based Adaptive Training for Robot-Assisted Surgery: a Feasibility Study on Medical Residents},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2019},
  doi = {10.31256/hsmr2018},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  research_field = {},
  data_type = {},
}

@inproceedings{Menegozzo2019,
  author = {Menegozzo, Giovanni and Dall'Alba, Diego and Zandon{\`{a}}, Chiara and Fiorini, Paolo},
  title = {Surgical gesture recognition with time delay neural network based on kinematic data},
  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/ismr44918.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/8aa5230692467f704ac8decfeaf514dc93ba09f2},
  research_field = {TR},
  data_type = {RI and KD},
  isbn = {153867825X},
}

@article{Krishnan2019,
  author = {Krishnan, Sanjay and Garg, Animesh and Liaw, Richard and Thananjeyan, Brijen and Miller, Lauren and Pokorny, Florian T and Goldberg, Ken},
  title = {SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards},
  journal = {The International Journal of Robotics Research},
  year = {2019},
  volume = {38},
  number = {2-3},
  publisher = {SAGE Publications Sage UK: London, England},
  doi = {10.1177/0278364918784350},
  semanticscholar = {https://www.semanticscholar.org/paper/d720d601d5b835937b96b68d7119b239d45776f0},
  research_field = {AU},
  data_type = {RI and KD and ED},
  dvrk_site = {UCB},
  abstract = {We present sequential windowed inverse reinforcement learning (SWIRL), a policy search algorithm that is a hybrid of exploration and demonstration paradigms for robot learning. We apply unsupervised learning to a small number of initial expert demonstrations to structure future autonomous exploration. SWIRL approximates a long time horizon task as a sequence of local reward functions and subtask transition conditions. Over this approximation, SWIRL applies Q-learning to compute a policy that maximizes rewards. Experiments suggest that SWIRL requires significantly fewer rollouts than pure reinforcement learning and fewer expert demonstrations than behavioral cloning to learn a policy. We evaluate SWIRL in two simulated control tasks, parallel parking and a two-link pendulum. On the parallel parking task, SWIRL achieves the maximum reward on the task with 85% fewer rollouts than Q-learning, and one-eight of demonstrations needed by behavioral cloning. We also consider physical experiments on surgical tensioning and cutting deformable sheets using a da Vinci surgical robot. On the deformable tensioning task, SWIRL achieves a 36% relative improvement in reward compared with a baseline of behavioral cloning with segmentation.},
  issn = {0278-3649},
}

@article{Selvaggio2019,
  author = {Selvaggio, Mario and Fontanelli, Giuseppe Andrea and Marrazzo, Vincenzo Romano and Bracale, Umberto and Irace, Andrea and Breglio, Giovanni and Villani, Luigi and Siciliano, Bruno and Ficuciello, Fanny},
  title = {The MUSHA underactuated hand for robot‐aided minimally invasive surgery},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2019},
  volume = {15},
  number = {3},
  publisher = {Wiley Online Library},
  doi = {10.1002/rcs.1981},
  semanticscholar = {https://www.semanticscholar.org/paper/785b3406bd95d3ff62fc7300e4cb9af56669d4c7},
  research_field = {HW},
  data_type = {RI},
  dvrk_site = {UNFII},
  abstract = {Keyhole surgery is characterized by loss of dexterity of surgeon's movements because of the limited workspace, nonintuitive motor skills of the surgical systems, and loss of tactile sensation that may lead to tissue damage and bad execution of the tasks.},
  issn = {1478-5951},
}

@article{Moradi2019b,
  author = {Moradi, Hamid and Tang, Shuo and Salcudean, Septimiu E.},
  title = {Toward Robot-Assisted Photoacoustic Imaging: Implementation Using the da Vinci Research Kit and Virtual Fixtures},
  journal = {IEEE Robotics and Automation Letters},
  year = {2019},
  volume = {4},
  number = {2},
  doi = {10.1109/LRA.2019.2897168},
  semanticscholar = {https://www.semanticscholar.org/paper/212607b44ecfa372ff14c73c292e262077a9d9a4},
  research_field = {},
  data_type = {},
  dvrk_site = {UBC},
  abstract = {Photoacoustic imaging of the prostate is challenging due to the limited access and limited acoustic windows to the prostate gland. We aim to develop intraoperative prostate photoacoustic imaging using the da Vinci robotic system and a pick-up ultrasound transducer that can be easily picked up and manipulated by the robot. We propose a new approach in which the da Vinci robot is programmed to acquire trajectories in a shared control configuration with “virtual fixtures”; the pick-up transducer is controlled so that it stays parallel to a single axis defined as the tomography axis, and its translation is fixed to a single plane normal to this axis. The surgeon controls the transducer motion on the tissue along this virtual fixture while the laser is fired and photoacoustic data are collected periodically. The RMS errors of the photoacoustic tomography images are 0.06 a.u. This study confirms that intraoperative da Vinci robot-assisted photoacoustic imaging with a pick-up transducer is feasible.},
  issn = {2377-3766},
}

@inproceedings{Tsai2019,
  author = {Tsai, Ya-Yen and Huang, Bidan and Guo, Yao and Yang, Guang-Zhong},
  title = {Transfer Learning for Surgical Task Segmentation},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {},
  data_type = {},
  isbn = {153866027X},
}

@inproceedings{Itzkovich2019,
  author = {Itzkovich, Danit and Sharon, Yarden and Jarc, Anthony and Refaely, Yael and Nisky, Ilana},
  title = {Using Augmentation to Improve the Robustness to Rotation of Deep Learning Segmentation in Robotic-Assisted Surgical Data},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {TR},
  data_type = {RI and KD},
  isbn = {153866027X},
}

@article{Podolsky2019,
  author = {Podolsky, Dale J and Diller, Eric and Fisher, David M and {Wong Riff}, Karen W and Looi, Thomas and Drake, James M and Forrest, Christopher R},
  title = {Utilization of Cable Guide Channels for Compact Articulation Within a Dexterous Three Degrees-of-Freedom Surgical Wrist Design},
  journal = {Journal of Medical Devices},
  year = {2019},
  volume = {13},
  number = {1},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  doi = {10.1115/1.4041591},
  semanticscholar = {https://www.semanticscholar.org/paper/13cf228dabea6e334b99ded2e4c26774c9cac8a5},
  research_field = {},
  data_type = {},
  dvrk_site = {SKCH},
  abstract = {Pin-jointed wrist mechanisms provide compact articulation for surgical robotic applications, but are difficult to miniaturize at scales suitable for small body cavity surgery. Solid surface cable guide channels, which eliminate the need for pulleys and reduce overall length to facilitate miniaturization, were developed within a three-degree-of-freedom cable-driven pin-jointed wrist mechanism. A prototype was 3D printed in steel at 5 mm diameter. Friction generated by the guide channels was experimentally tested to determine increases in cable tension during constant cable velocity conditions. Cable tension increased exponentially from 0 to 37% when the wrist pitched from 0 deg to 90 deg. The shape of the guide channel groove and angle, where the cable exits the channel impacts the magnitude of cable tension. A spring tensioning and cam actuation mechanism were developed to account for changing cable circuit path lengths during wrist pitch. This work shows that pulley-free cable wrist mechanisms can facilitate miniaturization below current feasible sizes while retaining compact articulation at the expense of increases in friction under constant cable velocity conditions.},
  issn = {1932-6181},
}

@inproceedings{Caccianiga2019,
  author = {Caccianiga, Guido and Mariani, Andrea and {De Momi}, Elena and Brown, Jeremy D},
  title = {Virtual Reality Training in Robot-Assisted Surgery: a Novel Experimental Setup for Skill Transfer Evaluation},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2019},
  doi = {10.31256/hsmr2018},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  research_field = {TR},
  data_type = {RI and KD and SD and ED},
}

@inproceedings{Moccia2019,
  author = {Moccia, Rocco and Selvaggio, Mario and Villani, Luigi and Siciliano, Bruno and Ficuciello, Fanny},
  title = {Vision-based virtual fixtures generation for robotic-assisted polyp dissection procedures},
  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2019},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90},
  research_field = {HW},
  data_type = {RI and KD},
  isbn = {1728140048},
}

@inproceedings{VanAmsterdam2019,
  author = {van Amsterdam, Beatrice and Nakawala, Hirenkumar and {De Momi}, Elena and Stoyanov, Danail},
  title = {Weakly Supervised Recognition of Surgical Gestures},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  year = {2019},
  publisher = {IEEE},
  doi = {10.1109/icra39644.2019},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  research_field = {TR},
  data_type = {RI and KD},
  isbn = {153866027X},
}

@article{Nakawala2019,
  author = {Nakawala, Hirenkumar and Bianchi, Roberto and Pescatori, Laura Erica and {De Cobelli}, Ottavio and Ferrigno, Giancarlo and {De Momi}, Elena},
  title = {“Deep-Onto” network for surgical workflow and context recognition},
  journal = {International journal of computer assisted radiology and surgery},
  year = {2019},
  volume = {14},
  number = {4},
  publisher = {Springer},
  doi = {10.1007/s11548-018-1882-8},
  semanticscholar = {https://www.semanticscholar.org/paper/e843c8b159eab14df15473e138ee72927eea0ab4},
  research_field = {},
  data_type = {},
  dvrk_site = {POLIMI},
  issn = {1861-6410},
}

@article{Allan2018a,
  author = {Allan, M. and Ourselin, S. and Hawkes, D. J. and Kelly, J. D. and Stoyanov, D.},
  title = {3-D Pose Estimation of Articulated Instruments in Robotic Minimally Invasive Surgery},
  journal = {IEEE Transactions on Medical Imaging},
  year = {2018},
  volume = {37},
  number = {5},
  doi = {10.1109/TMI.2018.2794439},
  semanticscholar = {https://www.semanticscholar.org/paper/27595732ddb379053a96128d2c8b3f67e0d578ea},
  research_field = {IM},
  data_type = {RI and KD},
  dvrk_site = {UCL},
  abstract = {Estimating the 3-D pose of instruments is an important part of robotic minimally invasive surgery for automation of basic procedures as well as providing safety features, such as virtual fixtures. Image-based methods of 3-D pose estimation provide a non-invasive low cost solution compared with methods that incorporate external tracking systems. In this paper, we extend our recent work in estimating rigid 3-D pose with silhouette and optical flow-based features to incorporate the articulated degrees-of-freedom (DOFs) of robotic instruments within a gradient-based optimization framework. Validation of the technique is provided with a calibrated ex-vivo study from the da Vinci Research Kit (DVRK) robotic system, where we perform quantitative analysis on the errors each DOF of our tracker. Additionally, we perform several detailed comparisons with recently published techniques that combine visual methods with kinematic data acquired from the joint encoders. Our experiments demonstrate that our method is competitively accurate while relying solely on image data.},
  issn = {0278-0062},
}

@inproceedings{Fontanelli2018e,
  author = {Fontanelli, Giuseppe Andrea and Yang, Guang-Zhong and Siciliano, Bruno},
  title = {A Comparison of Assistive Methods for Suturing in MIRS},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/IROS.2018.8593607},
  semanticscholar = {https://www.semanticscholar.org/paper/354efa12f13fb4e78916e1519e4cd7276d7238cc},
  research_field = {AU},
  data_type = {RI and KD and DD and ED},
  abstract = {In Minimally Invasive Robotic Surgery (MIRS) a robot is interposed between the surgeon and the surgical site to increase the precision, dexterity, and to reduce surgeon's effort and cognitive load with respect to the standard laparoscopic interventions. However, the modern robotic systems for MIRS are still based on the traditional telemanipulation paradigm, e.g. the robot behaviour is fully under surgeon's control, and no autonomy or assistance is implemented. In this work, supervised and shared controllers have been developed in a vision-free, human-in-the-Ioop, control framework to help surgeon during a surgical suturing procedure. Experiments conducted on the da Vinci Research Kit robot proves the effectiveness of the method indicating also the guidelines for improving results.},
  isbn = {978-1-5386-8094-0},
}

@article{Chu2018,
  author = {Chu, Xiangyu and Yip, Hoi Wut and Cai, Yuanpei and Chung, Tsz Yin and Moran, Stuart and Au, K W Samuel},
  title = {A Compliant Robotic Instrument With Coupled Tendon Driven Articulated Wrist Control for Organ Retraction},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {4},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2863373},
  semanticscholar = {https://www.semanticscholar.org/paper/5a68b4e773343a997d0faecc576ab707663c6a61},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {CUHK},
  abstract = {Organ retraction is a common and important task in minimally invasive surgery. It is a surgical technique to push aside or manipulate tissues/organs to improve the access and visualization of the surgical sites. Typical manual retractors are rigid and without any articulation. Physicians often struggle with minimizing the force applying onto the tissue while positioning the retractor to maintain optimal exposure. In this letter, we propose a novel compliant robotic organ retractor with coupled tendon driven articulated wrist control to address the aforementioned clinical risks. The compliant retractor exploits the buckling principle of a continuum bending beam mechanism, allowing it to interact with organ safely and smoothly. We also present a new tendon-driven, high torsional strength articulated joint and corresponding kinematics model to address the lack of dexterity issue. For the instrument control, a general algorithm framework is proposed to design the controller gain systematically, while eliminating the “disturbance” leaking caused by the coupled multitendon driven mechanism. Initial prototypes were built and integrated with the state-of-the-art surgical robotic platform, da Vinci Research Kit for basic functional evaluation. Preliminary simulation and experimental results demonstrate the capability of the proposed device for organ retraction. It is our hope that this novel instrument will become a new benchmark for organ retraction in terms of safety, precision, and dexterity.},
  issn = {2377-3766},
}

@inproceedings{Chalasani2018c,
  author = {Chalasani, Preetham and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H},
  title = {A Computational Framework for Complementary Situational Awareness (CSA) in Surgical Assistant Robots},
  booktitle = {2018 Second IEEE International Conference on Robotic Computing (IRC)},
  year = {2018},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/c0d91e78a238998aa19d11d06381716c1b45344a},
  research_field = {},
  data_type = {},
  isbn = {1538646528},
}

@inproceedings{El-Saig2018,
  author = {El-Saig, D{\'{a}}vid and Elek, Ren{\'{a}}ta Nagyn{\'{e}} and Haidegger, Tam{\'{a}}s},
  title = {A Graphical Tool for Parsing and Inspecting Surgical Robotic Datasets},
  booktitle = {2018 IEEE 18th International Symposium on Computational Intelligence and Informatics (CINTI)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/cinti45972.2018},
  semanticscholar = {https://www.semanticscholar.org/paper/1f4d522a1e8425d6d881e041899b63c1eefe3e91},
  research_field = {TR},
  data_type = {ED},
  isbn = {172811117X},
}

@article{Fontanelli2018d,
  author = {Fontanelli, Giuseppe Andrea and Selvaggio, Mario and Buonocore, Luca Rosario and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},
  title = {A New Laparoscopic Tool With In-Hand Rolling Capabilities for Needle Reorientation},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {3},
  doi = {10.1109/LRA.2018.2809443},
  semanticscholar = {https://www.semanticscholar.org/paper/55f43d2f0929d31b26ea113c2a02a94ba9902ce5},
  research_field = {HW},
  data_type = {RI and KD},
  dvrk_site = {UNFII},
  abstract = {In laparoscopic minimally invasive robotic surgery, a teleoperated robot is interposed between the patient and the surgeon. Despite the robot aid, the manipulation capabilities of surgical instruments are far from those of the human hand. In this letter, we want to make a step forward toward robotic solutions that can improve manipulation capabilities of the surgical instruments. A new concept of needle-driver tool is presented, which takes inspiration from the human hand model. The idea is to modify a standard laparoscopic tool by introducing an additional degree of freedom, which allows in-hand reorientation of the suturing needle. A 3D printed prototype has been built to validate the tool design. The improved manipulation capabilities have been assessed quantitatively by evaluating a weighted dexterity index along a single stitch trajectory. Moreover, a comparison between our tool and a standard needle driver has been done in terms of time required for the execution of a complete suturing sequence.},
  issn = {2377-3766},
}

@inproceedings{Song2018,
  author = {Song, Chengzhi and Mok, Ivan Shuenshing and Chiu, Philip Waiyan and Li, Zheng},
  title = {A Novel Tele-operated Flexible Manipulator Based on the da-Vinci Research Kit},
  booktitle = {2018 13th World Congress on Intelligent Control and Automation (WCICA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/WCICA.2018.8630381},
  semanticscholar = {https://www.semanticscholar.org/paper/0ed5264924fde554d6c259898f3b581b01591dff},
  research_field = {},
  data_type = {},
  abstract = {Flexible manipulators is now attracting an increasing interest in the field of minimally invasive surgery (MIS). Compared to rigid instruments, they could work in confined space and go through tortuous passes. The constrained tendon-driven serpentine mechanism (CTSM) is one of the novel flexible mechanisms proposed recently. It could provide a larger workspace and more dexterous distal end manipulation. In this paper, a number of flexible instruments were developed based on the CTSM and tele-operation was achieved by using the the da Vinci Research Kit (DVRK). The developed flexible instruments successfully completed the tasks that are commonly used in MIS training, including peg transfer and knotting.},
  isbn = {1538673460},
}

@inproceedings{Zevallos2018a,
  author = {Zevallos, Nicolas and Rangaprasad, Arun Srivatsan and Salman, Hadi and Li, Lu and Qian, Jianing and Saxena, Saumya and Xu, Mengyun and Patath, Kartik and Choset, Howie},
  title = {A Real-time Augmented Reality Surgical System for Overlaying Stiffness Information.},
  booktitle = {Robotics: Science and Systems},
  year = {2018},
  doi = {10.1609/aimag.v31i1.2237},
  semanticscholar = {https://www.semanticscholar.org/paper/c9c45251b37fe68ea96904762f1127141abdff99},
  research_field = {HW},
  data_type = {RI and KD},
}

@article{Alambeigi2018b,
  author = {Alambeigi, Farshid and Wang, Zerui and Hegeman, Rachel and Liu, Yun-Hui and Armand, Mehran},
  title = {A Robust Data-Driven Approach for Online Learning and Manipulation of Unmodeled 3-D Heterogeneous Compliant Objects},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {4},
  doi = {10.1109/LRA.2018.2863376},
  semanticscholar = {https://www.semanticscholar.org/paper/3e8cda355d9400b04e8ea61da5e11dbb7df231a1},
  research_field = {AU},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {JHU},
  abstract = {We present a generic data-driven method to address the problem of manipulating a three-dimensional (3-D) compliant object (CO) with heterogeneous physical properties in the presence of unknown disturbances. In this study, we do not assume a prior knowledge about the deformation behavior of the CO and type of the disturbance (e.g., internal or external). We also do not impose any constraints on the CO's physical properties (e.g., shape, mass, and stiffness). The proposed optimal iterative algorithm incorporates the provided visual feedback data to simultaneously learn and estimate the deformation behavior of the CO in order to accomplish the desired control objective. To demonstrate the capabilities and robustness of our algorithm, we fabricated two novel heterogeneous compliant phantoms and performed experiments on the da Vinci Research Kit. Experimental results demonstrated the adaptivity, robustness, and accuracy of the proposed method and, therefore, its suitability for a variety of medical and industrial applications involving CO manipulation.},
  issn = {2377-3766},
}

@article{Zhang2018,
  author = {Zhang, Dandan and Xiao, Bo and Huang, Baoru and Zhang, Lin and Liu, Jindong and Yang, Guang-Zhong},
  title = {A self-adaptive motion scaling framework for surgical robot remote control},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {4},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2890200},
  semanticscholar = {https://www.semanticscholar.org/paper/489c54a29d979ddf6dfe9830e0f5b14f334d8ab7},
  research_field = {},
  data_type = {},
  dvrk_site = {ICL},
  abstract = {Master–slave control is a common form of human–robot interaction for robotic surgery. To ensure seamless and intuitive control, a mechanism of self-adaptive motion scaling during teleoperaton is proposed in this letter. The operator can retain precise control when conducting delicate or complex manipulation, while the movement to a remote target is accelerated via adaptive motion scaling. The proposed framework consists of three components: 1) situation awareness, 2) skill level awareness, and 3) task awareness. The self-adaptive motion scaling ratio allows the operators to perform surgical tasks with high efficiency, forgoing the need of frequent clutching and instrument repositioning. The proposed framework has been verified on a da Vinci Research Kit to assess its usability and robustness. An in-house database is constructed for offline model training and parameter estimation, including both the kinematic data obtained from the robot and visual cues captured through the endoscope. Detailed user studies indicate that a suitable motion-scaling ratio can be obtained and adjusted online. The overall performance of the operators in terms of control efficiency and task completion is significantly improved with the proposed framework.},
  issn = {2377-3766},
}

@inproceedings{Zevallos2018b,
  author = {Zevallos, Nicolas and Srivatsan, Rangaprasad Arun and Salman, Hadi and Li, Lu and Qian, Jianing and Saxena, Saumya and Xu, Mengyun and Patath, Kartik and Choset, Howie},
  title = {A surgical system for automatic registration, stiffness mapping and dynamic image overlay},
  booktitle = {2018 International Symposium on Medical Robotics (ISMR)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ISMR.2018.8333310},
  semanticscholar = {https://www.semanticscholar.org/paper/9786b89d7c56dad60e44a3b64bce3ce7aa397398},
  arxiv = {https://arxiv.org/abs/1711.08828},
  research_field = {},
  data_type = {},
  abstract = {In this paper we develop a surgical system using the da Vinci research kit (dVRK) that is capable of autonomously searching for tumors and dynamically displaying the tumor location using augmented reality. Such a system has the potential to quickly reveal the location and shape of tumors and visually overlay that information to reduce the cognitive overload of the surgeon. To the best of our knowledge, our approach is one of the first to incorporate state-of-the-art methods in registration, force sensing and tumor localization into a unified surgical system. First, the preoperative model is registered to the intra-operative scene using a Bingham distribution-based filtering approach. An active level set estimation is then used to find the location and the shape of the tumors. We use a recently developed miniature force sensor to perform the palpation. The estimated stiffness map is then dynamically overlaid onto the registered preoperative model of the organ. We demonstrate the efficacy of our system by performing experiments on phantom prostate models with embedded stiff inclusions.},
  isbn = {978-1-5386-2512-5},
}

@article{Shahbazi2018b,
  author = {Shahbazi, Mahya and Atashzar, Seyed Farokh and Patel, Rajni V},
  title = {A systematic review of multilateral teleoperation systems},
  journal = {IEEE transactions on haptics},
  year = {2018},
  volume = {11},
  number = {3},
  publisher = {IEEE},
  doi = {10.1109/TOH.2018.2818134},
  semanticscholar = {https://www.semanticscholar.org/paper/08ce1195368e8a6b18b0b15721f440dccb5cd7e1},
  research_field = {RE},
  data_type = {},
  dvrk_site = {UWO},
  abstract = {While conventional bilateral Single-Master/Single-Slave (SM/SS) teleoperation systems have received considerable attention during the past several decades, multilateral teleoperation is only recently being studied. Unlike an SM/SS system, which consists of one master-slave set, multilateral teleoperation frameworks involve a minimum of three agents in order to remotely perform a task. This paper presents an overview of multilateral teleoperation systems and classifies the existing state-of-the-art architectures based on topologies, applications, and closed-loop stability analysis. For each category, the review discusses control strategies used for various architectures as well as control challenges (e.g., closed-loop instability as a result of a delay in the communication network) for each methodology.},
  issn = {1939-1412},
}

@inproceedings{Wang2018c,
  author = {Wang, Zerui and Li, Xiang and Navarro-Alarcon, David and Liu, Yun-hui},
  title = {A Unified Controller for Region-reaching and Deforming of Soft Objects},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/IROS.2018.8593543},
  semanticscholar = {https://www.semanticscholar.org/paper/db09007f2b5fae22085e21d8fefbbbad0adbddaa},
  research_field = {},
  data_type = {},
  abstract = {Emerging applications of robotic manipulation of deformable objects have opened up new challenges in robot control. While several control techniques have been developed to manipulate deformable objects, the performance of existing methods is commonly limited by two issues: 1) implicit assumption that the physical contact between the end-effector and the object is always maintained, and 2) requirements of exact parameters of deformation model, which are difficult to obtain. This paper presents a new control scheme for robotic manipulation of deformable objects, which allows the robot to automatically contact then actively deform the deformable object by assessing the status of deformation in real time. Instead of designing multiple controllers and switching among them, the proposed method smoothly and stably integrates two control phases (i.e. region reaching and active deforming) into a single controller. The stability of the closed-loop system is rigorously proved with the consideration of the uncertain deformation model and uncalibrated cameras. Hence, the proposed control scheme enhances the autonomous capability of active deformable object manipulation. Experimental studies are conducted with different initial conditions to demonstrate the performance of the proposed controller.},
  isbn = {978-1-5386-8094-0},
}

@inproceedings{Fontanelli2018f,
  author = {Fontanelli, G. A. and Selvaggio, M. and Ferro, M. and Ficuciello, F. and Vendittelli, M. and Siciliano, B.},
  title = {A V-REP Simulator for the da Vinci Research Kit Robotic Platform},
  booktitle = {2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/BIOROB.2018.8487187},
  semanticscholar = {https://www.semanticscholar.org/paper/1c8d436444369da2bff7c601196456c83a52e2cd},
  research_field = {SS},
  data_type = {KD and DD and SD},
  abstract = {In this work we present a V-REP simulator for the da Vinci Research Kit (dVRK). The simulator contains a full robot kinematic model and integrated sensors. A robot operating system (ROS) interface has been created for easy use and development of common software components. Moreover, several scenes have been implemented to illustrate the performance and potentiality of the developed simulator. Both the simulator and the example scenes are available to the community as an open source software.},
  isbn = {978-1-5386-8183-1},
}

@article{Pachtrachai2018,
  author = {Pachtrachai, Krittin and Vasconcelos, Francisco and Chadebecq, Fran{\c{c}}ois and Allan, Max and Hailes, Stephen and Pawar, Vijay and Stoyanov, Danail},
  title = {Adjoint transformation algorithm for hand–eye calibration with applications in robotic assisted surgery},
  journal = {Annals of biomedical engineering},
  year = {2018},
  volume = {46},
  number = {10},
  publisher = {Springer},
  doi = {10.1007/s10439-018-2097-4},
  semanticscholar = {https://www.semanticscholar.org/paper/56c22000c9c75ff2e33ca079a293ed462d30be88},
  research_field = {IM},
  data_type = {RI and KD},
  dvrk_site = {UCL},
  abstract = {Hand–eye calibration aims at determining the unknown rigid transformation between the coordinate systems of a robot arm and a camera. Existing hand–eye algorithms using closed-form solutions followed by iterative non-linear refinement provide accurate calibration results within a broad range of robotic applications. However, in the context of surgical robotics hand–eye calibration is still a challenging problem due to the required accuracy within the millimetre range, coupled with a large displacement between endoscopic cameras and the robot end-effector. This paper presents a new method for hand–eye calibration based on the adjoint transformation of twist motions that solves the problem iteratively through alternating estimations of rotation and translation. We show that this approach converges to a solution with a higher accuracy than closed form initializations within a broad range of synthetic and real experiments. We also propose a stereo hand–eye formulation that can be used in the context of both our proposed method and previous state-of-the-art closed form solutions. Experiments with real data are conducted with a stereo laparoscope, the KUKA robot arm manipulator, and the da Vinci surgical robot, showing that both our new alternating solution and the explicit representation of stereo camera hand–eye relations contribute to a higher calibration accuracy.},
  issn = {0090-6964},
}

@inproceedings{Foti2018,
  author = {Foti, Simone and Mariani, Andrea and Chupin, Thibaud and Dall'alba, Diego and Cheng, Zhuoqi and Mattos, Leonardo and Caldwell, Darwin and Fiorini, Paolo and {De Momi}, Elena and Ferrigno, Giancarlo},
  title = {Advanced User Interface for Augmented Information Display on Endoscopic Surgical Images},
  booktitle = {Conference on New Technologies for Computer and Robot Assisted Surgery},
  year = {2018},
  doi = {10.1007/s10151-015-1327-0},
  semanticscholar = {https://www.semanticscholar.org/paper/26a4530f1404285e2fa978266b648091f978b5ac},
  research_field = {HW},
  data_type = {RI and KD and ED},
}

@inproceedings{Gordon2018a,
  author = {Gordon, Alex and Looi, Thomas and Drake, James and Forrest, Christopher R.},
  title = {An Ultrasonic Bone Cutting Tool for the da Vinci Research Kit},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2018.8460797},
  semanticscholar = {https://www.semanticscholar.org/paper/f8fb56493a0c18aeef3feb49d713d5f91f7cea0f},
  research_field = {},
  data_type = {},
  isbn = {978-1-5386-3081-5},
}

@article{Qian2018a,
  author = {Qian, Long and Deguet, Anton and Kazanzides, Peter},
  title = {ARssist: augmented reality on a head-mounted display for the first assistant in robotic surgery},
  journal = {IET Healthcare Technology Letters},
  year = {2018},
  volume = {5},
  number = {5},
  pages = {194-200},
  doi = {10.1049/htl.2018.5065},
  semanticscholar = {https://www.semanticscholar.org/paper/bf1419f685e5cb2df513b3de64b4bae8d7b7e807},
  abstract = {In robot-assisted laparoscopic surgery, the first assistant (FA) is responsible for tasks such as robot docking, passing necessary materials, manipulating hand-held instruments, and helping with trocar planning and placement. The performance of the FA is critical for the outcome of the surgery. The authors introduce ARssist, an augmented reality application based on an optical see-through head-mounted display, to help the FA perform these tasks. ARssist offers (i) real-time three-dimensional rendering of the robotic instruments, hand-held instruments, and endoscope based on a hybrid tracking scheme and (ii) real-time stereo endoscopy that is configurable to suit the FA's hand–eye coordination when operating based on endoscopy feedback. ARssist has the potential to help the FA perform his/her task more efficiently, and hence improve the outcome of robot-assisted laparoscopic surgeries.},
  keywords = {arssist, dvrk, hmd},
  date = {2018-10-01},
  pubstate = {published},
  tppubtype = {article},
}

@inproceedings{DEttorre2018b,
  author = {D'Ettorre, C. and Dwyer, G. and Du, X. and Chadebecq, F. and Vasconcelos, F. and {De Momi}, E. and Stoyanov, D.},
  title = {Automated Pick-Up of Suturing Needles for Robotic Surgical Assistance},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2018.8461200},
  semanticscholar = {https://www.semanticscholar.org/paper/a3ff700c2352966565ba8fd9bae4bc6ce9a78905},
  arxiv = {https://arxiv.org/abs/1804.03141},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {Robot-assisted laparoscopic prostatectomy (RALP) is a treatment for prostate cancer that involves complete or nerve sparing removal prostate tissue that contains cancer. After removal the bladder neck is successively sutured directly with the urethra. The procedure is called urethrovesical anastomosis and is one of the most dexterity demanding tasks during RALP. Two suturing instruments and a pair of needles are used in combination to perform a running stitch during urethrovesical anastomosis. While robotic instruments provide enhanced dexterity to perform the anastomosis, it is still highly challenging and difficult to learn. In this paper, we presents a vision-guided needle grasping method for automatically grasping the needle that has been inserted into the patient prior to anastomosis. We aim to automatically grasp the suturing needle in a position that avoids hand-offs and immediately enables the start of suturing. The full grasping process can be broken down into: a needle detection algorithm; an approach phase where the surgical tool moves closer to the needle based on visual feedback; and a grasping phase through path planning based on observed surgical practice. Our experimental results show examples of successful autonomous grasping that has the potential to simplify and decrease the operational time in RALP by assisting a small component of urethrovesical anastomosis.},
  isbn = {978-1-5386-3081-5},
}

@article{Fard2018,
  author = {Fard, Mahtab J and Ameri, Sattar and {Darin Ellis}, R and Chinnam, Ratna B and Pandya, Abhilash K and Klein, Michael D},
  title = {Automated robot‐assisted surgical skill evaluation: Predictive analytics approach},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2018},
  volume = {14},
  number = {1},
  publisher = {Wiley Online Library},
  doi = {10.1002/rcs.1850},
  semanticscholar = {https://www.semanticscholar.org/paper/b99e9bbbd6eb5a34afa980a9ec2b9896aad3c32b},
  research_field = {TR},
  data_type = {SD and ED},
  dvrk_site = {WSU},
  issn = {1478-5951},
}

@article{Alambeigi2018e,
  author = {Alambeigi, Farshid and Wang, Zerui and Hegeman, Rachel and Liu, Yun-Hui and Armand, Mehran},
  title = {Autonomous data-driven manipulation of unknown anisotropic deformable tissues using unmodelled continuum manipulators},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {4},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2888896},
  semanticscholar = {https://www.semanticscholar.org/paper/5319e5ad25af2165fedc880ce2d618e3c481a8a0},
  research_field = {AU},
  data_type = {RI and KD and DD and SD and ED},
  dvrk_site = {JHU},
  abstract = {We present an autonomous manipulation approach for tissues with anisotropic deformation behavior using a continuum manipulator. The key feature of our vision-based study is an online learning and estimation method, which makes its implementation independent of any prior knowledge about the deformation behavior of the tissue and continuum manipulator as well as calibration of the vision system with respect to the robot. This important feature addresses the difficulty of using model-based control approaches in deformation control of a continuum manipulator manipulating an unknown deformable tissue. We evaluated the performance and robustness of our method in three different experiments using the da Vinci Research Kit coupled with a 5 mm instrument that has a 4-degree-of-freedom snake-like wrist. These experiments simulated situations that occur in various surgical schemes and verified the adaptability, learning capability, and accuracy of the proposed method.},
  issn = {2377-3766},
}

@misc{Bodenstedt2018,
  author = {Bodenstedt, Sebastian and Allan, Max and Agustinos, Anthony and Du, Xiaofei and Garcia-Peraza-Herrera, Luis and Kenngott, Hannes and Kurmann, Thomas and M{\"{u}}ller-Stich, Beat and Ourselin, Sebastien and Pakhomov, Daniil},
  title = {Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery},
  booktitle = {arXiv preprint arXiv:1805.02475},
  year = {2018},
  semanticscholar = {https://www.semanticscholar.org/paper/69c48b29fcd88575745f33b5aa1839d496e85f54},
  arxiv = {https://arxiv.org/abs/1805.02475},
  research_field = {},
  data_type = {},
  abstract = {The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) from 2021 through 2025. Given the rapid pace of AI development, the preprint ecosystem has become a critical barometer for real-time scientific shifts, often preceding formal peer-reviewed publication by months or years. By employing a multi-stage data collection and enrichment pipeline in conjunction with LLM-based institution classification, we analyze the evolution of publication volumes, author team sizes, and academic--industry collaboration patterns. Our results reveal an unprecedented surge in publication output following the introduction of ChatGPT, with academic institutions continuing to provide the largest volume of research. However, we observe that academic--industry collaboration is still suppressed, as measured by a Normalized Collaboration Index (NCI) that remains significantly below the random-mixing baseline across all major subfields. These findings highlight a continuing institutional divide and suggest that the capital-intensive nature of generative AI research may be reshaping the boundaries of scientific collaboration.},
}

@inproceedings{Gu2018b,
  author = {Gu, Yun and Hu, Yang and Zhang, Lin and Yang, Jie and Yang, Guang-Zhong},
  title = {Cross-scene suture thread parsing for robot assisted anastomosis based on joint feature learning},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2018},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/ed2b8fc79604936dca0c58062a98ce35128fa313},
  research_field = {},
  data_type = {},
  isbn = {1538680947},
}

@article{Wang2018b,
  author = {Wang, Ziheng and Fey, Ann Majewicz},
  title = {Deep learning with convolutional neural network for objective skill evaluation in robot-assisted surgery},
  journal = {International journal of computer assisted radiology and surgery},
  year = {2018},
  volume = {13},
  number = {12},
  publisher = {Springer},
  doi = {10.1007/s11548-018-1860-1},
  semanticscholar = {https://www.semanticscholar.org/paper/91fed290d898235d4121627e4f4e2c92134b694c},
  arxiv = {https://arxiv.org/abs/1806.05796},
  research_field = {},
  data_type = {},
  dvrk_site = {UTD},
  abstract = {PurposeWith the advent of robot-assisted surgery, the role of data-driven approaches to integrate statistics and machine learning is growing rapidly with prominent interests in objective surgical skill assessment. However, most existing work requires translating robot motion kinematics into intermediate features or gesture segments that are expensive to extract, lack efficiency, and require significant domain-specific knowledge.MethodsWe propose an analytical deep learning framework for skill assessment in surgical training. A deep convolutional neural network is implemented to map multivariate time series data of the motion kinematics to individual skill levels.ResultsWe perform experiments on the public minimally invasive surgical robotic dataset, JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). Our proposed learning model achieved competitive accuracies of 92.5%, 95.4%, and 91.3%, in the standard training tasks: Suturing, Needle-passing, and Knot-tying, respectively. Without the need of engineered features or carefully tuned gesture segmentation, our model can successfully decode skill information from raw motion profiles via end-to-end learning. Meanwhile, the proposed model is able to reliably interpret skills within a 1–3 second window, without needing an observation of entire training trial.ConclusionThis study highlights the potential of deep architectures for efficient online skill assessment in modern surgical training.},
  issn = {1861-6410},
}

@inproceedings{Mariani2018a,
  author = {Mariani, Andrea and Pellegrini, Edoardo and Enayati, Nima and Kazanzides, Peter and Vidotto, Marco and {De Momi}, Elena},
  title = {Design and Evaluation of a Performance-based Adaptive Curriculum for Robotic Surgical Training: a Pilot Study},
  booktitle = {2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/EMBC.2018.8512728},
  semanticscholar = {https://www.semanticscholar.org/paper/0086375564dec6aea1f9bce3f095cf084142bb64},
  research_field = {TR},
  data_type = {KD and SD},
  abstract = {Training with simulation systems has become a primary alternative for learning the fundamental skills of robotic surgery. However, there exists no consensus regarding a standard training curriculum: sessions defined a priori by expert trainers or self-directed by the trainees feature lack of consistency. This study proposes an adaptive approach that structures the curriculum on the basis of an objective assessment of the trainee’s performance. The work comprised an experimental session with 12 participants performing training on virtual reality tasks with the da Vinci Research Kit surgical console. Half of the subjects self-managed their training session, while the others underwent the adaptive training. The final performance of the latter trainees was found to be higher compared to the former (p=0.002), showing how outcome-based, dynamic designs could constitute a promising advance in robotic surgical training.},
  isbn = {978-1-5386-3646-6},
}

@inproceedings{Vantadori2018,
  author = {Vantadori, Luca and Mariani, Andrea and Chupin, Thibaud and {De Momi}, Elena and Ferrigno, Giancarlo},
  title = {Design and evaluation of an intraoperative safety constraints definition and enforcement system for robot-assisted minimally invasive surgery},
  booktitle = {Proceedings of Computer/Robot Assisted Surgery CRAS},
  year = {2018},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2},
  research_field = {HW},
  data_type = {DD and SD},
}

@article{Xue2018,
  author = {Xue, Renfeng and Ren, Bingyin and Huang, Jiaqing and Yan, Zhiyuan and Du, Zhijiang},
  title = {Design and evaluation of FBG-based tension sensor in laparoscope surgical robots},
  journal = {Sensors},
  year = {2018},
  volume = {18},
  number = {7},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/s18072067},
  semanticscholar = {https://www.semanticscholar.org/paper/ed6d088267cdff4d95a54bfe6d321dac26e9984d},
  research_field = {},
  data_type = {},
  dvrk_site = {CWRU},
  abstract = {Due to the narrow space and a harsh chemical environment in the sterilization processes for the end-effector of surgical robots, it is difficult to install and integrate suitable sensors for the purpose of effective and precise force control. This paper presents an innovative tension sensor for estimation of grasping force in our laparoscope surgical robot. The proposed sensor measures the tension of cable using fiber gratings (FBGs) which are pasted in the grooves on the inclined cantilevers of the sensor. By exploiting the stain measurement characteristics of FBGs, the small deformation of the inclined cantilevers caused by the cable tension can be measured. The working principle and the sensor model are analyzed. Based on the sensor model, the dimensions of the sensor are designed and optimized. A dedicated experimental setup is established to calibrate and test the sensor. The results of experiments for estimation the grasping force validate the sensor.},
}

@article{Francis2018a,
  author = {Francis, P. and Eastwood, K. W. and Bodani, V. and Looi, T. and Drake, J. M.},
  title = {Design, Modelling and Teleoperation of a 2 mm Diameter Compliant Instrument for the da Vinci Platform},
  journal = {Annals of Biomedical Engineering},
  year = {2018},
  volume = {46},
  number = {10},
  doi = {10.1007/s10439-018-2036-4},
  semanticscholar = {https://www.semanticscholar.org/paper/b38ca2141dbce93499c30616440421cf95d64932},
  research_field = {},
  data_type = {},
  dvrk_site = {SKCH},
  issn = {0090-6964},
}

@article{Quek2018,
  author = {Quek, Zhan Fan and Provancher, William R and Okamura, Allison M},
  title = {Evaluation of skin deformation tactile feedback for teleoperated surgical tasks},
  journal = {IEEE transactions on haptics},
  year = {2018},
  volume = {12},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/TOH.2018.2873398},
  semanticscholar = {https://www.semanticscholar.org/paper/859fb2550a459f9b7446150be914e526d82baccf},
  research_field = {HW},
  data_type = {KD and DD and ED},
  dvrk_site = {SU},
  abstract = {During interaction with objects using a tool, we experience force and tactile feedback. One form of tactile feedback is local fingerpad skin deformation. In this paper, we provide haptic feedback to users of a teleoperation system through a skin deformation tactile feedback device. The device is able to provide tangential and normal skin deformation in a coupled manner, and is designed so that users can grasp it with a precision grip using multiple fingerpads. By applying skin deformation feedback on multiple fingerpads, the device is able to provide multi-degree-of-freedom interaction force direction and magnitude information to the user. To evaluate the effectiveness of this approach for the performance of teleoperated manipulation tasks, we performed a study in which 20 participants used a teleoperation system to perform one of two manipulation tasks (peg transfer and tube connection) using force feedback, skin deformation feedback, and the combination of both feedback. Results showed that participants are able to use all feedback to improve task performance compared to the case without haptic feedback, although the degree of improvement depended on the nature of the task. The feedback also improved situation awareness, felt consistent with prior experience, and did not affect concentration on the task, as reported by participants.},
  issn = {1939-1412},
}

@article{Sharon2018a,
  author = {Sharon, Yarden and Nisky, Ilana},
  title = {Expertise, Teleoperation, and Task Constraints Affect the Speed–Curvature–Torsion Power Law in RAMIS},
  journal = {Journal of Medical Robotics Research},
  year = {2018},
  volume = {03},
  doi = {10.1142/S2424905X18410088},
  semanticscholar = {https://www.semanticscholar.org/paper/7ea957233110b7762cee3809114810167b9a7d18},
  research_field = {},
  data_type = {},
  dvrk_site = {BGUN},
  abstract = {Quantitative characterization of surgical movements can improve the quality of patient care by informing the development of new training protocols for surgeons, and the design and control of surgical robots. Here, we focus on the relationship between the speed of movement and its geometry that was extensively studied in computational motor control. In three-dimensional movements, this relationship is defined by a family of speed–curvature–torsion power laws, such as the one-sixth power law. We present a novel characterization of open and teleoperated suturing movements using the speed–curvature–torsion power-law analysis. We fitted the gain and the exponents of this power law to suturing movements of participants with different levels of surgical experience in open (using sensorized forceps) and teleoperated (using the da Vinci Research Kit/da Vinci Surgical System) conditions from two different datasets. We found that expertise and teleoperation significantly affected the gain and exponents of the power law, and that there were large differences between different segments of movement. These results confirm that the relationship between the speed and geometry of surgical movements is indicative of surgical skill, open a new avenue for understanding the effect of teleoperation on the movements of surgeons, and lay the foundation for the development of new algorithms for automatic segmentation of surgical tasks.},
  issn = {2424-905X},
}

@inproceedings{Seita2018a,
  author = {Seita, Daniel and Krishnan, Sanjay and Fox, Roy and McKinley, Stephen and Canny, John and Goldberg, Ken},
  title = {Fast and Reliable Autonomous Surgical Debridement with Cable-Driven Robots Using a Two-Phase Calibration Procedure},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2018.8460583},
  semanticscholar = {https://www.semanticscholar.org/paper/ce97028f90ad3379c49a6a6b0f7beccca18caef7},
  arxiv = {https://arxiv.org/abs/1709.06668},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {Automating precision subtasks such as debridement (removing dead or diseased tissue fragments) with Robotic Surgical Assistants (RSAs) such as the da Vinci Research Kit (dVRK) is challenging due to inherent nOnlinearities in cable-driven systems. We propose and evaluate a novel two-phase coarse-to-fine calibration method. In Phase I (coarse), we place a red calibration marker on the end effector and let it randomly move through a set of open-loop trajectories to obtain a large sample set of camera pixels and internal robot end-effector configurations. This coarse data is then used to train a Deep Neural Network (DNN) to learn the coarse transformation bias. In Phase II (fine), the bias from Phase I is applied to move the end -effector toward a small set of specific target points on a printed sheet. For each target, a human operator manually adjusts the end -effector position by direct contact (not through teleoperation) and the residual compensation bias is recorded. This fine data is then used to train a Random Forest (RF) to learn the fine transformation bias. Subsequent experiments suggest that without calibration, position errors average 4.55mm. Phase I can reduce average error to 2.14mm and the combination of Phase I and Phase II can reduces average error to 1.08mm. We apply these results to debridement of raisins and pumpkin seeds as fragment phantoms. Using an endoscopic stereo camera with standard edge detection, experiments with 120 trials achieved average success rates of 94.5 %, exceeding prior results with much larger fragments (89.4%) and achieving a speedup of 2.1x, decreasing time per fragment from 15.8 seconds to 7.3 seconds. Source code, data, and videos are available at https://sites.google.com/view/calib-icra/.},
  isbn = {978-1-5386-3081-5},
}

@inproceedings{Wu2018a,
  author = {Wu, Jie Ying and Chen, Zihan and Deguet, Anton and Kazanzides, Peter},
  title = {FPGA-Based Velocity Estimation for Control of Robots with Low-Resolution Encoders},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/IROS.2018.8594139},
  semanticscholar = {https://www.semanticscholar.org/paper/7c661912adc0c750f30e5281e16bb5b5f0fee1ea},
  research_field = {},
  data_type = {},
  abstract = {Robot control algorithms often rely on measurements of robot joint velocities, which can be estimated by measuring the time between encoder edges. When encoder edges occur infrequently, such as at low velocities and/or with low resolution encoders, this measurement delay may affect the stability of closed-loop control. This is evident in both the joint position control and Cartesian impedance control of the da Vinci Research Kit (dVRK), which contains several low-resolution encoders. We present a hardware-based method that gives more frequent velocity updates and is not affected by common encoder imperfections such as non-uniform duty cycles and quadrature phase error. The proposed method measures the time between consecutive edges of the same type but, unlike prior methods, is implemented for the rising and falling edges of both channels. Additionally, it estimates acceleration to enable software compensation of the measurement delay. The method is shown to improve Cartesian impedance control of the dVRK.},
  isbn = {978-1-5386-8094-0},
}

@article{Li2018a,
  author = {Li, Zhaoshuo and Tong, Irene and Metcalf, Leo and Hennessey, Craig and Salcudean, Septimiu E.},
  title = {Free Head Movement Eye Gaze Contingent Ultrasound Interfaces for the da Vinci Surgical System},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {3},
  doi = {10.1109/LRA.2018.2809512},
  semanticscholar = {https://www.semanticscholar.org/paper/20bfdebb50a806e0e7cdc44f710b996fd0420b85},
  research_field = {HW},
  data_type = {RI and KD and ED},
  dvrk_site = {UBC},
  abstract = {The current practice of intraoperative ultrasound requires an assistant because the surgeon's hands are occupied with surgical tools or console instruments. This process can be tedious and prone to error. Eye gaze is a promising control modality that can help address this issue. In previous work, a novel feature-based retro-fit eye gaze tracker has been designed for the <italic>da Vinci</italic> surgical system. In this letter, leveraging the <italic>da Vinci</italic> research kit, three interfaces incorporate eye gaze, and voice recognition into the <italic> da Vinci</italic> surgical system for ultrasound control in one common framework. This letter aims to improve autonomous use of ultrasound for surgeons. Since eye gaze tracking is sensitive to head movement, a novel calibration procedure is also proposed to accommodate head motion by decomposing pupil movement into eye rotation and head motion. This ensures that the eye gaze tracking can be reliably used as a control modality. A user study (<italic>N</italic> = 20) has shown that the designed eye gaze tracker has a mean binocular accuracy of 1.98<inline-formula> <tex-math notation="LaTeX">$^\{\circ \}$</tex-math></inline-formula> with mean <inline-formula><tex-math notation="LaTeX"> $-$</tex-math></inline-formula>0.92 mm horizontal and 16.83-mm vertical head movement. A preliminary user study ( <italic>N</italic> = 9) has shown that eye gaze tracking for ultrasound control has the potential to improve the way surgeons interact with their instrumentation and increase surgical autonomy.},
  issn = {2377-3766},
}

@inproceedings{Mazomenos2018,
  author = {Mazomenos, E and Watson, Dave and Kotorov, Rado and Stoyanov, D},
  title = {Gesture Classification in Robotic Surgery using Recurrent Neural Networks with Kinematic Information},
  booktitle = {8th Joint Workshop on New Technology for Computer/Robot Assisted Surgery (CRAS 2018)},
  year = {2018},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2},
  research_field = {},
  data_type = {},
}

@inproceedings{Ji2018b,
  author = {Ji, Jessica J. and Krishnan, Sanjay and Patel, Vatsal and Fer, Danyal and Goldberg, Ken},
  title = {Learning 2D Surgical Camera Motion From Demonstrations},
  booktitle = {2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/COASE.2018.8560468},
  semanticscholar = {https://www.semanticscholar.org/paper/6838cac0edc58fceb6b708767f49dce52f14d598},
  research_field = {},
  data_type = {},
  abstract = {Automating camera movement during robot-assisted surgery has the potential to reduce burden on surgeons and remove the need to manually move the camera. An important sub-problem is automatic viewpoint selection, proposing camera poses that focus on important anatomical features. We use the 6 DoF Stewart Platform Research Kit (SPRK) to move the environment with a fixed endoscope, as a dual to moving the endoscope itself, to study camera motion in surgical robotics. To provide demonstrations, we link the platform's control directly to the da Vinci Research Kit (dVRK) master control system and allow control of the platform using the same pedals and tools as a clinical movable endoscope. We propose a probabilistic model that identifies image features that “dwell” close to the camera's focal point in expert demonstrations. Our experiments consider a surgical debridement scenario on silicone phantoms with inclusions of varying color and shape. We evaluate the extent to which the system correctly segments candidate debridement targets (box accuracy) and correctly ranks those targets (rank accuracy). For debridement of a single uniquely colored inclusion, the box accuracy is 80% and the rank accuracy is 100% after 100 training data points. For debridement of multiple inclusions of the same color, the box accuracy is 70.8% and the rank accuracy is 100% after 100 training data points. For debridement of inclusions of a particular shape, the box accuracy is 70.5% and the rank accuracy is 90% after 100 training data points. A demonstration video is available at: https://vimeo.com/260362958},
  isbn = {978-1-5386-3593-3},
}

@article{Penza2018,
  author = {Penza, Veronica and Du, Xiaofei and Stoyanov, Danail and Forgione, Antonello and Mattos, Leonardo S and {De Momi}, Elena},
  title = {Long term safety area tracking (LT-SAT) with online failure detection and recovery for robotic minimally invasive surgery},
  journal = {Medical image analysis},
  year = {2018},
  volume = {45},
  publisher = {Elsevier},
  doi = {10.1016/j.media.2017.12.010},
  semanticscholar = {https://www.semanticscholar.org/paper/f52a2f32eb3eb2bbbc0c27779fa56613591c460e},
  research_field = {},
  data_type = {},
  dvrk_site = {UCL},
  issn = {1361-8415},
}

@inproceedings{Kamikawa2018,
  author = {Kamikawa, Yasuhisa and Enayati, Nima and Okamura, Allison M},
  title = {Magnified Force Sensory Substitution for Telemanipulation via Force-Controlled Skin Deformation},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/icra36916.2018},
  semanticscholar = {https://www.semanticscholar.org/paper/11c1cf0e6c0481bc99443c8777643905e2740855},
  research_field = {},
  data_type = {},
  isbn = {1538630818},
}

@article{Shahbazi2018a,
  author = {Shahbazi, Mahya and Atashzar, S. Farokh and Ward, Christopher and Talebi, Heidar Ali and Patel, Rajni V.},
  title = {Multimodal Sensorimotor Integration for Expert-in-the-Loop Telerobotic Surgical Training},
  journal = {IEEE Transactions on Robotics},
  year = {2018},
  volume = {34},
  number = {6},
  doi = {10.1109/TRO.2018.2861916},
  semanticscholar = {https://www.semanticscholar.org/paper/785a4cb6047de170668317df035219edb245803c},
  research_field = {},
  data_type = {},
  dvrk_site = {UWO},
  abstract = {This paper presents a novel multimodal training platform integrated with hand-over-hand (HOH) haptic guidance for dual-console surgical robotic systems such as the da Vinci Si system. The expert-in-the-loop (EIL) framework incorporates a fuzzy interface system in order to provide a trainee with adaptive authority over the procedure as well as hand-over-hand haptic guidance adjusted in real time based on the proficiency level of the trainee. The EIL expertise-oriented framework enables performance of a surgical procedure by an expert surgeon on a patient, while simultaneously providing a trainee at any stage of the motor-skills development with multimodal training without jeopardizing patient safety. Closed-loop stability of the system is investigated using the circle criterion and it is shown that the proposed architecture is unconditionally stable. Experimental evaluations are presented in support of the proposed platform through the implementation of a dual-console surgical setup consisting of the classic da Vinci surgical system (Intuitive Surgical, Inc., Sunnyvale, CA, USA) and the dV-Trainer master console (Mimic Technology, Inc., Seattle, WA, USA). To the best of our knowledge, the implemented setup is the first research platform for dual-console studies involving the classic da Vinci surgical system.},
  issn = {1552-3098},
}

@article{Nagy2018d,
  author = {Nagy, D{\'{e}}nes {\'{A}}kos and Nagy, Tam{\'{a}}s D{\'{a}}niel and Elek, Ren{\'{a}}ta and Rudas, Imre J. and Haidegger, Tam{\'{a}}s},
  title = {Ontology-Based Surgical Subtask Automation, Automating Blunt Dissection},
  journal = {Journal of Medical Robotics Research},
  year = {2018},
  volume = {03},
  doi = {10.1142/S2424905X18410052},
  semanticscholar = {https://www.semanticscholar.org/paper/5ff23d8a9dfb88085b678a33a83f1c586f39672b},
  research_field = {},
  data_type = {},
  dvrk_site = {OU},
  abstract = {Automation of surgical processes (SPs) is an utterly complex, yet highly demanded feature by medical experts. Currently, surgical tools with advanced sensory and diagnostic capabilities are only available. A major criticism towards the newly developed instruments that they are not fitting into the existing medical workflow often creating more annoyance than benefit for the surgeon. The first step in achieving streamlined integration of computer technologies is gaining a better understanding of the SP. Surgical ontologies provide a generic platform for describing elements of the surgical procedures. Surgical Process Models (SPMs) built on top of these ontologies have the potential to accurately represent the surgical workflow. SPMs provide the opportunity to use ontological terms as the basis of automation, allowing the developed algorithm to easily integrate into the surgical workflow, and to apply the automated SPMs wherever the linked ontological term appears in the workflow. In this work, as an example to this concept, the subtask level ontological term “blunt dissection” was targeted for automation. We implemented a computer vision-driven approach to demonstrate that automation on this task level is feasible. The algorithm was tested on an experimental silicone phantom as well as in several ex vivo environments. The implementation used the da Vinci surgical robot, controlled via the Da Vinci Research Kit (DVRK), relying on a shared code-base among the DVRK institutions. It is believed that developing and linking further building blocks of lower level surgical subtasks could lead to the introduction of automated soft tissue surgery. In the future, the building blocks could be individually unit tested, leading to incremental automation of the domain. This framework could potentially standardize surgical performance, eventually improving patient outcomes.},
  issn = {2424-905X},
}

@article{Selvaggio2018,
  author = {Selvaggio, Mario and Fontanelli, Giuseppe Andrea and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},
  title = {Passive virtual fixtures adaptation in minimally invasive robotic surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {4},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2849876},
  semanticscholar = {https://www.semanticscholar.org/paper/0f13e38c8367ba8a94ee4e7caba648b6adeee08c},
  research_field = {HW},
  data_type = {RI and KD and DD},
  dvrk_site = {UNFII},
  abstract = {During robot-aided surgical interventions, the surgeon can be benefitted from the application of virtual fixtures (VFs). Though very effective, this technique is very often not practicable in unstructured surgical environments. In order to comply with the environmental deformation, both the VF geometry and the constraint enforcement parameters need to be online defined/adapted. This letter proposes a strategy for an effective use of VF assistance in minimally invasive robotic surgical tasks. An online VF generation technique based on the interaction force measurements is presented. Pose and geometry adaptations of the VF are considered. Passivity of the overall system is guaranteed by using energy tanks passivity-based control. The proposed method is validated through experiments on the da Vinci Research Kit.},
  issn = {2377-3766},
}

@article{Abdelaal2018,
  author = {Abdelaal, Alaa Eldin and Sakr, Maram and Avinash, Apeksha and Mohammed, Shahed K and Bajwa, Armaan Kaur and Sahni, Mohakta and Hor, Soheil and Fels, Sidney and Salcudean, Septimiu E},
  title = {Play me back: a unified training platform for robotic and laparoscopic surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {4},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2890209},
  semanticscholar = {https://www.semanticscholar.org/paper/153e1ec0ff1907700f5a612271ab39b40b0227e6},
  research_field = {TR},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {UBC},
  abstract = {In this letter, we propose a training approach combining hand-over-hand and trial and error training approaches and we evaluate its effectiveness for both robotic and standard laparoscopic surgical training. The proposed approach makes use of the data of an expert collected while using the da Vinci Surgical System. We present our data collection system and how we use it in the proposed training approach. We conduct two user studies (N = 21 for each) to evaluate the effectiveness of this approach. Our results show that subjects trained using this combined approach can better balance the speed and accuracy of their task execution compared with others trained using only one of either hand-over-hand or trial and error training approaches. Moreover, this combined approach leads to the best performance when it comes to the transferability of the acquired skills when testing on another task. We show that the results of the two studies are consistent with an established model in the literature for motor skill learning. Moreover, our results show for the first time the feasibility of using a surgical robot and data collected from it as a training platform for conventional laparoscopic surgery without robotic assistance.},
  issn = {2377-3766},
}

@article{Chalasani2018a,
  author = {Chalasani, Preetham and Wang, Long and Yasin, Rashid and Simaan, Nabil and Taylor, Russell H.},
  title = {Preliminary Evaluation of an Online Estimation Method for Organ Geometry and Tissue Stiffness},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {3},
  doi = {10.1109/LRA.2018.2801481},
  semanticscholar = {https://www.semanticscholar.org/paper/1e14db8cbe1bce66ea2104e0c387f599db47a2de},
  research_field = {AU},
  data_type = {RI and KD and DD and SD and ED},
  dvrk_site = {JHU},
  abstract = {During open surgeries, surgeons use tactile palpation to form an understanding of the anatomy, including any underlying anatomical structures such as arteries or tumors. This letter explores methods for restoring this capability in the context of minimally invasive robot-assisted surgery. Previous works have demonstrated the ability of robots to use discrete palpation to characterize organ shape and, when followed with offline data processing, to produce a stiffness map of the organ. In our earlier work, we presented an offline estimation technique, independent of palpation strategy, to estimate organ shape and stiffness using Gaussian processes. This study extends our prior work by demonstrating a fast online technique for estimation of organ shape and stiffness. Our goal is to provide near video-frame-rate updates of the organ geometry and tissue stiffness during force controlled exploration. Two different palpation modes are experimentally explored: autonomous palpation and constrained semiautonomous teleoperation. We report the experimental evaluation of our approach for stiffness estimation using autonomous palpation. And we demonstrate the feasibility of using our method during interactive teleoperation in a simulated surgical scenario. We believe that future use of the online stiffness and geometry information based on our proposed method can help surgeons in improving their understanding of the surgical scene and its correlation to preoperative imaging, thereby increasing safety and improving surgical outcomes.},
  issn = {2377-3766},
}

@inproceedings{Enayati2018b,
  author = {Enayati, Nima and Okamura, Allison M. and Mariani, Andrea and Pellegrini, Edoardo and Coad, Margaret M. and Ferrigno, Giancarlo and {De Momi}, Elena},
  title = {Robotic Assistance-as-Needed for Enhanced Visuomotor Learning in Surgical Robotics Training: An Experimental Study},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2018.8463168},
  semanticscholar = {https://www.semanticscholar.org/paper/8e328be0b5675a27f3ab9ff084c82fcbf49eb00d},
  research_field = {},
  data_type = {},
  isbn = {978-1-5386-3081-5},
}

@inproceedings{Wang2018d,
  author = {Wang, Ziheng and Fey, Ann Majewicz},
  title = {SATR-DL: Improving surgical skill assessment and task recognition in robot-assisted surgery with deep neural networks},
  booktitle = {2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  year = {2018},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/3b53eec8706908a0f8486773f39f7ab3c7735546},
  research_field = {TR},
  data_type = {RI and KD},
  isbn = {1538636468},
}

@article{Vagvolgyi2018,
  author = {Vagvolgyi, Balazs P and Pryor, Will and Reedy, Ryan and Niu, Wenlong and Deguet, Anton and Whitcomb, Louis L and Leonard, Simon and Kazanzides, Peter},
  title = {Scene modeling and augmented virtuality interface for telerobotic satellite servicing},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {4},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2864358},
  semanticscholar = {https://www.semanticscholar.org/paper/c0c56d4e79ab16119bf096d98e959b7b394693e3},
  research_field = {},
  data_type = {},
  dvrk_site = {JHU},
  abstract = {Teleoperation in extreme environments can be hindered by limitations in telemetry and in operator perception of the remote environment. Often, the primary mode of perception is visual feedback from remote cameras, which do not always provide suitable views and are subject to telemetry delays. To address these challenges, we propose to build a model of the remote environment and provide an augmented virtuality visualization system that augments the model with projections of real camera images. The approach is demonstrated in a satellite servicing scenario, with a multisecond round-trip telemetry delay between the operator on Earth and the satellite on orbit. The scene modeling enables both virtual fixtures to assist the human operator and augmented virtuality visualization that allows the operator to teleoperate a virtual robot from a convenient virtual viewpoint, with the delayed camera images projected onto the three-dimensional model. Experiments on a ground-based telerobotic platform, with software-created telemetry delays, indicate that the proposed method leads to better teleoperation performance with 30% better blade alignment and 50% reduction in task execution time compared to the baseline case where visualization is restricted to the available camera views.},
  issn = {2377-3766},
}

@article{Ma2018,
  author = {Ma, Xin and Chiu, Philip Wai-Yan and Li, Zheng},
  title = {Shape sensing of flexible manipulators with visual occlusion based on Bezier curve},
  journal = {IEEE Sensors Journal},
  year = {2018},
  volume = {18},
  number = {19},
  publisher = {IEEE},
  doi = {10.1109/JSEN.2018.2862925},
  semanticscholar = {https://www.semanticscholar.org/paper/f899e67f5284c634a5c11a69fdaf48fd3a44743e},
  research_field = {HW},
  data_type = {RI and KD and ED},
  dvrk_site = {CUHK},
  abstract = {Flexible manipulators are promising in minimally invasive surgery, because they can work well in complex and confined environment. The size of the manipulator is usually small, which makes it very difficult to install sensors to measure the distal tip position and manipulator shape. However, the shape information is crucial in controlling the instruments. In this paper, the shape of the manipulator is estimated by endoscopic view based on Bezier curve and the Levenberg–Marquardt algorithm. This method works even the view is partially occluded. Compared with existing none-vision based shape sensing method, no additional sensor is needed in the proposed method. The method is evaluated by both simulations and experiments. Meanwhile, the influence of the occlusion on the sensing accuracy is analyzed. The experimental results show that the shape information of the flexible manipulator both with and without payload can be well estimated by the proposed method.},
  issn = {1530-437X},
}

@article{Enayati2018a,
  author = {Enayati, Nima and Ferrigno, Giancarlo and {De Momi}, Elena},
  title = {Skill-based human–robot cooperation in tele-operated path tracking},
  journal = {Autonomous Robots},
  year = {2018},
  volume = {42},
  number = {5},
  publisher = {Springer},
  doi = {10.1007/s10514-017-9675-4},
  semanticscholar = {https://www.semanticscholar.org/paper/e903ae33aae5a166b6d6da5daacc7e2c812596b8},
  research_field = {TR},
  data_type = {RI and KD and DD and SD},
  dvrk_site = {POLIMI},
  issn = {0929-5593},
}

@article{Diodato2018a,
  author = {Diodato, Alessandro and Brancadoro, Margherita and {De Rossi}, Giacomo and Abidi, Haider and Dall'Alba, Diego and Muradore, Riccardo and Ciuti, Gastone and Fiorini, Paolo and Menciassi, Arianna and Cianchetti, Matteo},
  title = {Soft Robotic Manipulator for Improving Dexterity in Minimally Invasive Surgery},
  journal = {Surgical Innovation},
  year = {2018},
  volume = {25},
  number = {1},
  doi = {10.1177/1553350617745953},
  semanticscholar = {https://www.semanticscholar.org/paper/1d5f9d05cff4c1f823ab62746c16e76c0ea22171},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {SSSA},
  issn = {1553-3506},
}

@inproceedings{Patel2018b,
  author = {Patel, Vatsal and Krishnan, Sanjay and Goncalves, Aimee and Goldberg, Ken},
  title = {SPRK: A low-cost stewart platform for motion study in surgical robotics},
  booktitle = {2018 International Symposium on Medical Robotics (ISMR)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ISMR.2018.8333300},
  semanticscholar = {https://www.semanticscholar.org/paper/92e4de2584eba6bd785ecd1963e391240c446b14},
  arxiv = {https://arxiv.org/abs/1712.02923},
  research_field = {HW},
  data_type = {KD and DD and ED},
  abstract = {To simulate body organ motion due to breathing, heart beats, or peristaltic movements, we designed a low-cost, miniaturized SPRK (Stewart Platform Research Kit) to translate and rotate phantom tissue. This platform is 20cm χ 20cm χ 10cm to fit in the workspace of a da Vinci Research Kit (DVRK) surgical robot and costs $250, two orders of magnitude less than a commercial Stewart platform. The platform has a range of motion of ± 1.27 cm in translation along x, y, and z, and of ± 15o in roll, pitch, and yaw directions. The platform also has motion modes for sinusoidal motion, breathing-inspired motion, and multi-axis motion. Modular mounts facilitate pattern cutting and debridement experiments. The platform's positional controller has a time-constant of 0.2 seconds and the root-mean-square error is 1.22 mm, 1.07 mm, and 0.20 mm in x, y, and z directions respectively. Construction directions, CAD models, and control software for the platform are available at github.com/BerkeleyAutomation/sprk.},
  isbn = {978-1-5386-2512-5},
}

@inproceedings{Nagy2018e,
  author = {Nagy, Tamas D. and Takacs, Marta and Rudas, Imre J. and Haidegger, Tamas},
  title = {Surgical subtask automation — Soft tissue retraction},
  booktitle = {2018 IEEE 16th World Symposium on Applied Machine Intelligence and Informatics (SAMI)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/SAMI.2018.8323986},
  semanticscholar = {https://www.semanticscholar.org/paper/01d22cf342c050549d6b76100b05186a3915fc57},
  research_field = {},
  data_type = {},
  isbn = {978-1-5386-4772-1},
}

@inproceedings{Ozguner2018a,
  author = {{\"{O}}zg{\"{u}}ner, Orhan and Hao, Ran and Jackson, Russell C and Shkurti, Tom and Newman, Wyatt and Cavusoglu, M Cenk},
  title = {Three-dimensional surgical needle localization and tracking using stereo endoscopic image streams},
  booktitle = {2018 IEEE international conference on robotics and automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/icra36916.2018},
  semanticscholar = {https://www.semanticscholar.org/paper/11c1cf0e6c0481bc99443c8777643905e2740855},
  research_field = {},
  data_type = {},
  isbn = {1538630818},
}

@inproceedings{Ferguson2018c,
  author = {Ferguson, James M. and Cai, Leon Y. and Reed, Alexander and Siebold, Michael and De, Smita and Herrell, Stanley D. and Webster, Robert J.},
  title = {Toward image-guided partial nephrectomy with the da Vinci robot: exploring surface acquisition methods for intraoperative re-registration},
  booktitle = {Medical Imaging 2018: Image-Guided Procedures, Robotic Interventions, and Modeling},
  year = {2018},
  publisher = {SPIE},
  doi = {10.1117/12.2296464},
  semanticscholar = {https://www.semanticscholar.org/paper/f87ff13b29f907eb56148eb8ae0856a919ff32f2},
  research_field = {},
  data_type = {},
  editor = {Webster, Robert J. and Fei, Baowei},
  isbn = {9781510616417},
}

@article{Moradi2018,
  author = {Moradi, Hamid and Tang, Shuo and Salcudean, Septimiu E},
  title = {Toward intra-operative prostate photoacoustic imaging: configuration evaluation and implementation using the da Vinci research kit},
  journal = {IEEE transactions on medical imaging},
  year = {2018},
  volume = {38},
  number = {1},
  publisher = {IEEE},
  doi = {10.1109/TMI.2018.2855166},
  semanticscholar = {https://www.semanticscholar.org/paper/c8a3bed5fffbc89a47ce7b0a11790fb971ce6805},
  research_field = {HW},
  data_type = {RI and KD},
  dvrk_site = {UBC},
  abstract = {We compare different possible scanning geometries for prostate photoacoustic tomography (PAT) while considering a realistic reconstruction scenario in which the limited view of the prostate and the directivity effect of the transducer are considered. Simulations and experiments confirm that an intra-operative configuration in which the photoacoustic signal is received by a pickup transducer from the anterior surface of the prostate provides the best approach. We propose a PAT acquisition system that includes a da Vinci system controlled by the da Vinci Research Kit, an illumination laser, and an ultrasound machine with parallel data acquisition. The robot maneuvers the pickup transducer to form a cylindrical detection surface around the prostate. The robot is programmed to acquire trajectories in which the transducer face is parallel to and oriented toward a rotational tomography axis, while the laser is fired and PAT data are collected at regular intervals. We present our initial images acquired with this novel system.},
  issn = {0278-0062},
}

@article{Alambeigi2018f,
  author = {Alambeigi, Farshid and Wang, Zerui and Liu, Yun-hui and Taylor, Russell H. and Armand, Mehran},
  title = {Toward Semi-autonomous Cryoablation of Kidney Tumors via Model-Independent Deformable Tissue Manipulation Technique},
  journal = {Annals of Biomedical Engineering},
  year = {2018},
  volume = {46},
  number = {10},
  doi = {10.1007/s10439-018-2074-y},
  semanticscholar = {https://www.semanticscholar.org/paper/6fa03b55fb2ac5520e29de81dbf2fbd3d4f0f879},
  research_field = {AU},
  data_type = {RI and KD and DD and SD and ED},
  dvrk_site = {JHU},
  issn = {0090-6964},
}

@inproceedings{Stilli2018,
  author = {Stilli, Agostino and Dimitrakakis, E and Tran, Maxine and Stoyanov, Danail},
  title = {Track-Guided Ultrasound Scanning for Tumour Margins Outlining in Robot-Assisted Partial Nephrectomy},
  booktitle = {Joint Workshop on New Technologies for Computer/Robot Assisted Surgery (CRAS 2018)},
  year = {2018},
  volume = {8},
  publisher = {CRAS},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2},
  research_field = {},
  data_type = {},
}

@inproceedings{Salman2018b,
  author = {Salman, Hadi and Ayvali, Elif and Srivatsan, Rangaprasad Arun and Ma, Yifei and Zevallos, Nicolas and Yasin, Rashid and Wang, Long and Simaan, Nabil and Choset, Howie},
  title = {Trajectory-Optimized Sensing for Active Search of Tissue Abnormalities in Robotic Surgery},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2018.8460936},
  semanticscholar = {https://www.semanticscholar.org/paper/eb302b3cf98ff8597eaa49163e5a03d5be86eeb5},
  arxiv = {https://arxiv.org/abs/1711.07063},
  research_field = {},
  data_type = {},
  abstract = {In this work, we develop an approach for guiding robots to automatically localize and find the shapes of tumors and other stiff inclusions present in the anatomy. Our approach uses Gaussian processes to model the stiffness distribution and active learning to direct the palpation path of the robot. The palpation paths are chosen such that they maximize an acquisition function provided by an active learning algorithm. Our approach provides the flexibility to avoid obstacles in the robot's path, incorporate uncertainties in robot position and sensor measurements, include prior information about location of stiff inclusions while respecting the robot-kinematics. To the best of our knowledge this is the first work in literature that considers all the above conditions while localizing tumors. The proposed framework is evaluated via simulation and experimentation on three different robot platforms: 6-DoF industrial arm, da Vinci Research Kit (dVRK), and the Insertable Robotic Effector Platform (IREP). Results show that our approach can accurately estimate the locations and boundaries of the stiff inclusions while reducing exploration time.},
  isbn = {978-1-5386-3081-5},
}

@inproceedings{Patel2018c,
  author = {Patel, Vatsal and Krishnan, Sanjay and Goncalves, Aimee and Chen, Carolyn and Boyd, Walter Doug and Goldberg, Ken},
  title = {Using intermittent synchronization to compensate for rhythmic body motion during autonomous surgical cutting and debridement},
  booktitle = {2018 International Symposium on Medical Robotics (ISMR)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/ISMR.2018.8333301},
  semanticscholar = {https://www.semanticscholar.org/paper/608298f059b806a3c2e75b09ef619bd029321d5e},
  arxiv = {https://arxiv.org/abs/1712.02917},
  research_field = {AU},
  data_type = {RI and KD},
  abstract = {Anatomical structures are rarely static during a surgical procedure due to breathing, heartbeats, and peristaltic movements. Inspired by observing an expert surgeon, we propose an intermittent synchronization with the extrema of the rhythmic motion (i.e., the lowest velocity windows). We performed 2 experiments: (1) pattern cutting and (2) debridement. In (1), we found that the intermittent synchronization approach, while 1.8× slower than tracking motion, is significantly more robust to noise and control latency, and it reduces the max cutting error by 2.6× except when motion is along 3 or more orthogonal axes. In (2), a baseline approach with no synchronization succeeds in 62% of debridement attempts while intermittent synchronization achieves an 80% success rate.},
  isbn = {978-1-5386-2512-5},
}

@article{Haouchine2018,
  author = {Haouchine, Nazim and Kuang, Winnie and Cotin, Stephane and Yip, Michael},
  title = {Vision-based force feedback estimation for robot-assisted surgery using instrument-constrained biomechanical three-dimensional maps},
  journal = {IEEE Robotics and Automation Letters},
  year = {2018},
  volume = {3},
  number = {3},
  publisher = {IEEE},
  doi = {10.1109/LRA.2018.2810948},
  semanticscholar = {https://www.semanticscholar.org/paper/e6508575b1d540e07aba7bb17c31f93c69034728},
  research_field = {HW},
  data_type = {RI and KD},
  dvrk_site = {UCSD},
  abstract = {We present a method for estimating visual and haptic force feedback on robotic surgical systems that currently do not include significant force feedback for the operator. Our approach permits to compute contact forces between instruments and tissues without additional sensors, relying only on endoscopic images acquired by a stereoscopic camera. Using an underlying biomechanical model built on-the-fly from the organ shape and by considering the surgical tool as boundary conditions acting on the surface of the model, contact force can be estimated at the tip of the tool. At the same time, these constraints generate stresses that permit to compose a new endoscopic image as visual feedback for the surgeon. The results are demonstrated on in vivo sequences of a human liver during robotic surgery, whereas quantitative validation is performed on a DejaVu and ex vivo experimentation with ground truth to show the advantage of our approach.},
  issn = {2377-3766},
}

@inproceedings{Hao2018a,
  author = {Hao, Ran and Ozguner, Orhan and Cavusoglu, M. Cenk},
  title = {Vision-Based Surgical Tool Pose Estimation for the da Vinci{\textregistered} Robotic Surgical System},
  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2018},
  publisher = {IEEE},
  doi = {10.1109/IROS.2018.8594471},
  semanticscholar = {https://www.semanticscholar.org/paper/9ae709a669b30167c7f999b1c90a51d40f2cca70},
  research_field = {},
  data_type = {},
  abstract = {This paper presents an approach to surgical tool tracking using stereo vision for the da Vinci® Surgical Robotic System. The proposed method is based on robot kinematics, computer vision techniques and Bayesian state estimation. The proposed method employs a silhouette rendering algorithm to create virtual images of the surgical tool by generating the silhouette of the defined tool geometry under the da Vinci® robot endoscopes. The virtual rendering method provides the tool representation in image form, which makes it possible to measure the distance between the rendered tool and real tool from endoscopic stereo image streams. Particle Filter algorithm employing the virtual rendering method is then used for surgical tool tracking. The tracking performance is evaluated on an actual da Vinci® surgical robotic system and a ROS/Gazebo-based simulation of the da Vinci® system.},
  isbn = {978-1-5386-8094-0},
}

@article{Naidu2017,
  author = {Naidu, Anish S and Naish, Michael D and Patel, Rajni V},
  title = {A breakthrough in tumor localization: Combining tactile sensing and ultrasound to improve tumor localization in robotics-assisted minimally invasive surgery},
  journal = {IEEE Robotics & Automation Magazine},
  year = {2017},
  volume = {24},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/MRA.2017.2680544},
  semanticscholar = {https://www.semanticscholar.org/paper/a92888855c3c0d7693026f4a73c81f54b4cbe6f0},
  research_field = {},
  data_type = {},
  dvrk_site = {UWO},
  issn = {1070-9932},
}

@article{Zhang2017e,
  author = {Zhang, Zhiqiang and Zhang, Lin and Yang, Guang-Zhong},
  title = {A computationally efficient method for hand–eye calibration},
  journal = {International journal of computer assisted radiology and surgery},
  year = {2017},
  volume = {12},
  number = {10},
  publisher = {Springer},
  doi = {10.1007/s11548-017-1646-x},
  semanticscholar = {https://www.semanticscholar.org/paper/50fc9397d7b33fae7d0815674a78c913364b6166},
  research_field = {IM},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {ICL},
  abstract = {PurposeSurgical robots with cooperative control and semiautonomous features have shown increasing clinical potential, particularly for repetitive tasks under imaging and vision guidance. Effective performance of an autonomous task requires accurate hand–eye calibration so that the transformation between the robot coordinate frame and the camera coordinates is well defined. In practice, due to changes in surgical instruments, online hand–eye calibration must be performed regularly. In order to ensure seamless execution of the surgical procedure without affecting the normal surgical workflow, it is important to derive fast and efficient hand–eye calibration methods.MethodsWe present a computationally efficient iterative method for hand–eye calibration. In this method, dual quaternion is introduced to represent the rigid transformation, and a two-step iterative method is proposed to recover the real and dual parts of the dual quaternion simultaneously, and thus the estimation of rotation and translation of the transformation.ResultsThe proposed method was applied to determine the rigid transformation between the stereo laparoscope and the robot manipulator. Promising experimental and simulation results have shown significant convergence speed improvement to 3 iterations from larger than 30 with regard to standard optimization method, which illustrates the effectiveness and efficiency of the proposed method.},
  issn = {1861-6410},
}

@inproceedings{Enayati2017,
  author = {Enayati, N and Mariani, A and Pellegrini, E and Chupin, T and Ferrigno, G and {De Momi}, E},
  title = {A Framework for Assisted Tele-operation with Augmented Reality},
  booktitle = {CRAS: Joint Workshop on New Technologies for Computer/Robot Assisted Surgery},
  year = {2017},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2},
  research_field = {TR},
  data_type = {RI and KD and DD and SD},
}

@inproceedings{Anooshahpour2017,
  author = {Anooshahpour, Farshad and Yadmellat, Peyman and Polushin, Ilia G and Patel, Rajni V},
  title = {A motion transmission model for multi-DOF tendon-driven mechanisms with hysteresis and coupling: Application to a da Vinci{\textregistered} instrument},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/iros37595.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  research_field = {HW},
  data_type = {RI and KD},
  isbn = {1538626829},
}

@article{Sang2017c,
  author = {Sang, Hongqiang and Monfaredi, Reza and Wilson, Emmanuel and Fooladi, Hadi and Preciado, Diego and Cleary, Kevin},
  title = {A New Surgical Drill Instrument With Force Sensing and Force Feedback for Robotically Assisted Otologic Surgery},
  journal = {Journal of Medical Devices},
  year = {2017},
  volume = {11},
  number = {3},
  doi = {10.1115/1.4036490},
  semanticscholar = {https://www.semanticscholar.org/paper/c4ac18cff9dc8669d0ba2760a0a5bd32c0c3b54f},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {SZIPS},
  issn = {1932-6181},
}

@inproceedings{Fontanelli2017a,
  author = {Fontanelli, Giuseppe Andrea and Buonocore, Luca Rosario and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},
  title = {A novel force sensing integrated into the trocar for minimally invasive robotic surgery},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/iros37595.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  research_field = {},
  data_type = {},
  isbn = {1538626829},
}

@inproceedings{Eslamian2017,
  author = {Eslamian, Shahab and Reisner, Luke A. and King, Brady W. and Pandya, Abhilash K.},
  title = {An Autonomous Camera System using the da Vinci Research Kit},
  booktitle = {arXiv preprint},
  year = {2017},
  doi = {10.1145/3613904.3642858},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  arxiv = {https://arxiv.org/abs/2310.09985},
  research_field = {AU},
  data_type = {RI and KD and SD},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@inproceedings{Vagvolgyi2017,
  author = {Vagvolgyi, Balazs and Niu, Wenlong and Chen, Zihan and Wilkening, Paul and Kazanzides, Peter},
  title = {Augmented virtuality for model-based teleoperation},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/iros37595.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  research_field = {},
  data_type = {},
  isbn = {1538626829},
}

@inproceedings{Zhang2017i,
  author = {Zhang, Lin and Ye, Menglong and Giataganas, Petros and Hughes, Michael and Yang, Guang-Zhong},
  title = {Autonomous scanning for endomicroscopic mosaicing and 3D fusion},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2017.7989412},
  semanticscholar = {https://www.semanticscholar.org/paper/c563baa3bb8452e487229b1b72591ec4dbd1416a},
  arxiv = {https://arxiv.org/abs/1604.04137},
  research_field = {},
  data_type = {},
  abstract = {Robot-assisted minimally invasive surgery can benefit from the automation of common, repetitive or well-defined but ergonomically difficult tasks. One such task is the scanning of a pick-up endomicroscopy probe over a complex, undulating tissue surface to enhance the effective field-of-view through video mosaicing. In this paper, the da Vinci® surgical robot, through the dVRK framework, is used for autonomous scanning and 2D mosaicing over a user-defined region of interest. To achieve the level of precision required for high quality mosaic generation, which relies on sufficient overlap between consecutive image frames, visual servoing is performed using a combination of a tracking marker attached to the probe and the endomicroscopy images themselves. The resulting sub-millimetre accuracy of the probe motion allows for the generation of large mosaics with minimal intervention from the surgeon. Images are streamed from the endomicroscope and overlaid live onto the surgeons view, while 2D mosaics are generated in real-time, and fused into a 3D stereo reconstruction of the surgical scene, thus providing intuitive visualisation and fusion of the multi-scale images. The system therefore offers significant potential to enhance surgical procedures, by providing the operator with cellular-scale information over a larger area than could typically be achieved by manual scanning.},
  isbn = {978-1-5090-4633-1},
}

@inproceedings{Li2017,
  author = {Li, Lu and Yu, Bocheng and Yang, Chen and Vagdargi, Prasad and Srivatsan, Rangaprasad Arun and Choset, Howie},
  title = {Development of an inexpensive tri-axial force sensor for minimally invasive surgery},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/iros37595.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  research_field = {HW},
  data_type = {ED},
  isbn = {1538626829},
}

@article{Kim2017,
  author = {Kim, Myungjoon and Lee, Chiwon and Hong, Nhayoung and Kim, Yoon Jae and Kim, Sungwan},
  title = {Development of stereo endoscope system with its innovative master interface for continuous surgical operation},
  journal = {Biomedical engineering online},
  year = {2017},
  volume = {16},
  number = {1},
  publisher = {Springer},
  doi = {10.1186/s12938-017-0376-1},
  semanticscholar = {https://www.semanticscholar.org/paper/4081966e4540c1078b72f60ce6433af579a9ad3a},
  research_field = {},
  data_type = {},
  dvrk_site = {SNU},
  abstract = {BackgroundAlthough robotic laparoscopic surgery has various benefits when compared with conventional open surgery and minimally invasive surgery, it also has issues to overcome and one of the issues is the discontinuous surgical flow that occurs whenever control is swapped between the endoscope system and the operating robot arm system. This can lead to problems such as collision between surgical instruments, injury to patients, and increased operation time. To achieve continuous surgical operation, a wireless controllable stereo endoscope system is proposed which enables the simultaneous control of the operating robot arm system and the endoscope system.MethodsThe proposed system consists of two improved novel master interfaces (iNMIs), a four-degrees of freedom (4-DOFs) endoscope control system (ECS), and a simple three-dimensional (3D) endoscope. In order to simultaneously control the proposed system and patient side manipulators of da Vinci research kit (dVRK), the iNMIs are installed to the master tool manipulators of dVRK system. The 4-DOFs ECS consists of four servo motors and employs a two-parallel link structure to provide translational and fulcrum point motion to the simple 3D endoscope. The images acquired by the endoscope undergo stereo calibration and rectification to provide a clear 3D vision to the surgeon as available in clinically used da Vinci surgical robot systems. Tests designed to verify the accuracy, data transfer time, and power consumption of the iNMIs were performed. The workspace was calculated to estimate clinical applicability and a modified peg transfer task was conducted with three novice volunteers.ResultsThe iNMIs operated for 317 min and moved in accordance with the surgeon’s desire with a mean latency of 5 ms. The workspace was calculated to be 20378.3 cm3, which exceeds the reference workspace of 549.5 cm3. The novice volunteers were able to successfully execute the modified peg transfer task designed to evaluate the proposed system’s overall performance.ConclusionsThe experimental results verify that the proposed 3D endoscope system enables continuous surgical flow. The workspace is suitable for the performance of numerous types of surgeries. Therefore, the proposed system is expected to provide much higher safety and efficacy for current surgical robot systems.},
  issn = {1475-925X},
}

@article{Fard2017,
  author = {Fard, Mahtab J and Pandya, Abhilash K and Chinnam, Ratna B and Klein, Michael D and Ellis, R Darin},
  title = {Distance‐based time series classification approach for task recognition with application in surgical robot autonomy},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2017},
  volume = {13},
  number = {3},
  publisher = {Wiley Online Library},
  doi = {10.1002/rcs.1766},
  semanticscholar = {https://www.semanticscholar.org/paper/ac37b0c8349589ae2a7df78878717f4c685bca14},
  research_field = {TR},
  data_type = {RI and KD and SD},
  dvrk_site = {WSU},
  issn = {1478-5951},
}

@article{Penza2017,
  author = {Penza, Veronica and {De Momi}, Elena and Enayati, Nima and Chupin, Thibaud and Ortiz, Jes{\'{u}}s and Mattos, Leonardo S},
  title = {enVisors: enhanced Vision system for robotic surgery. a User-Defined safety Volume Tracking to Minimize the risk of intraoperative Bleeding},
  journal = {Frontiers in Robotics and AI},
  year = {2017},
  volume = {4},
  publisher = {Frontiers},
  doi = {10.3389/frobt.2017.00015},
  semanticscholar = {https://www.semanticscholar.org/paper/d3cfba60a260dc5894166d438151e7816f638e34},
  research_field = {},
  data_type = {},
  dvrk_site = {POLIMI},
  abstract = {In abdominal surgery, intra-operative bleeding is one of the major complications that affect the outcome of minimally invasive surgical procedures. One of the causes is attributed to accidental damages to arteries or veins, and one of the possible risk factors falls on the surgeon's skills. This paper presents the development and application of an Enhanced Vision System for Robotic Surgery (EnViSoRS), based on a user-defined Safety Volume (SV) tracking to minimise the risk of intra-operative bleeding. It aims at enhancing the surgeon's capabilities by providing Augmented Reality (AR) assistance towards the protection of vessels from injury during the execution of surgical procedures with a robot. The core of the framework consists in: (i) a hybrid tracking algorithm (LT-SAT tracker) that robustly follows a user-defined Safety Area (SA) in long term; (ii) a dense soft tissue 3D reconstruction algorithm, necessary for the computation of the SV; (iii) AR features for visualisation of the SV to be protected and of a graphical gauge indicating the current distance between the instruments and the reconstructed surface. EnViSoRS was integrated with a commercial robotic surgery system (the dVRK system) for testing and validation. The experiments aimed at demonstrating the accuracy, robustness, performance and usability of EnViSoRS during the execution of a simulated surgical task on a liver phantom. Results show an overall accuracy in accordance with surgical requirements (< 5mm), and high robustness in the computation of the SV in terms of precision and recall of its identification. The optimisation strategy implemented to speed up the computational time is also described and evaluated, providing AR features update rate up to 4 fps without impacting the real-time visualisation of the stereo endoscopic video. Finally, qualitative results regarding the system usability indicate that the proposed system integrates well with the commercial surgical robot and has indeed potential to offer useful assistance during real surgeries.},
  issn = {2296-9144},
}

@article{Sang2017b,
  author = {Sang, Hongqiang and Yun, Jintian and Monfaredi, Reza and Wilson, Emmanuel and Fooladi, Hadi and Cleary, Kevin},
  title = {External force estimation and implementation in robotically assisted minimally invasive surgery},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  year = {2017},
  volume = {13},
  number = {2},
  doi = {10.1002/rcs.1824},
  semanticscholar = {https://www.semanticscholar.org/paper/db28445114e3c6c74df9f6fcc1e31cb29da519d1},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {SZIPS},
  issn = {14785951},
}

@article{Wang2017,
  author = {Wang, Long and Chen, Zihan and Chalasani, Preetham and Yasin, Rashid M and Kazanzides, Peter and Taylor, Russell H and Simaan, Nabil},
  title = {Force-controlled exploration for updating virtual fixture geometry in model-mediated telemanipulation},
  journal = {Journal of Mechanisms and Robotics},
  year = {2017},
  volume = {9},
  number = {2},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  doi = {10.1115/1.4035684},
  semanticscholar = {https://www.semanticscholar.org/paper/6fc55e847886f0581497be762c1e128df1f8e8c2},
  research_field = {HW},
  data_type = {RI and KD and DD and ED},
  dvrk_site = {VU},
  issn = {1942-4302},
}

@article{Zhang2017h,
  author = {Zhang, Lin and Ye, Menglong and Giataganas, Petros and Hughes, Michael and Bradu, Adrian and Podoleanu, Adrian and Yang, Guang-Zhong},
  title = {From Macro to Micro: Autonomous Multiscale Image Fusion for Robotic Surgery},
  journal = {IEEE Robotics & Automation Magazine},
  year = {2017},
  volume = {24},
  number = {2},
  doi = {10.1109/MRA.2017.2680543},
  semanticscholar = {https://www.semanticscholar.org/paper/db7c7614427f78953b1a19da0d68c298ad1de48a},
  research_field = {IM},
  data_type = {KD and ED},
  dvrk_site = {ICL},
  abstract = {Minimally invasive surgery (MIS), performed through a small number of keyhole incisions, has become the standard of care for many general surgical procedures, reducing trauma, blood loss, and other complications and offering patients the prospect of a faster recovery with less postoperative pain. These improvements for the patient, however, require higher dexterity and complex instrument control by the surgeons. Keyhole incisions constrain the motion of surgical instruments, while the loss of stereovision when using a laparoscope or endoscope means that depth perception is much poorer than in traditional open surgery. The desire to tackle these issues has been the main driver behind the development of robotic MIS systems with stereovision. In particular, the da Vinci robot (Intuitive Surgical, Inc., Sunnyvale, California) is a successful surgical platform, used widely in the treatment of gynecological and urological cancers. While human guidance is essential for MIS, recent studies [1] have suggested that automation of some surgical subtasks, particularly those that are tedious and repetitive or require high precision, can be beneficial in improving accuracy and reducing the cognitive load of the surgeon. For example, several studies have investigated automation of surgical suturing subtasks, including using a suturing tool under fluorescence guidance [2], and other studies have explored areas such as autonomous tissue dissection [3].},
  issn = {1070-9932},
}

@article{Wang2017a,
  author = {Wang, Zerui and Lee, Sing Chun and Zhong, Fangxun and Navarro-Alarcon, David and Liu, Yun-hui and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H},
  title = {Image-based trajectory tracking control of 4-DOF laparoscopic instruments using a rotation distinguishing marker},
  journal = {IEEE Robotics and Automation Letters},
  year = {2017},
  volume = {2},
  number = {3},
  publisher = {IEEE},
  doi = {10.1109/LRA.2017.2676350},
  semanticscholar = {https://www.semanticscholar.org/paper/423aeea78a401ba384351a083b703d4955c7a6b9},
  research_field = {IM},
  data_type = {RI and KD and DD and SD},
  dvrk_site = {JHU},
  issn = {2377-3766},
}

@inproceedings{Kim2017a,
  author = {Kim, Sungmin and Gandhi, Neeraj and Bell, Muyinatu A Lediju and Kazanzides, Peter},
  title = {Improving the safety of telerobotic drilling of the skull base via photoacoustic sensing of the carotid arteries},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/icra33291.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/e0aba478ab135e3b72b38dc9dd3e4608b213ba53},
  research_field = {IM},
  data_type = {RI and KD and DD and ED},
  isbn = {150904633X},
}

@article{Sharon2017,
  author = {Sharon, Yarden and Lendvay, Thomas Sean and Nisky, Ilana},
  title = {Instrument orientation-based metrics for surgical skill evaluation in robot-assisted and open needle driving},
  journal = {arXiv preprint},
  year = {2017},
  semanticscholar = {https://www.semanticscholar.org/paper/4a670217817c7c38cbce724d5b5d381e8391d123},
  research_field = {TR},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {BGUN},
}

@article{Francis2017,
  author = {Francis, Peter and Eastwood, Kyle W and Bodani, Vivek and Price, Karl and Upadhyaya, Kunj and Podolsky, Dale and Azimian, Hamidreza and Looi, Thomas and Drake, James},
  title = {Miniaturized instruments for the da Vinci research kit: Design and implementation of custom continuum tools},
  journal = {IEEE Robotics & Automation Magazine},
  year = {2017},
  volume = {24},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/MRA.2017.2680547},
  semanticscholar = {https://www.semanticscholar.org/paper/10560a043afe159d2f45dc4fec29347640130c46},
  research_field = {HW},
  data_type = {KD and DD},
  dvrk_site = {SKCH},
  issn = {1070-9932},
}

@inproceedings{Fiorini2017,
  author = {Fiorini, P and Dall'Alba, D and {De Rossi}, G and Naftalovich, D and Burdick, J W},
  title = {Mining Robotic Surgery Data: Training and Modeling using the DVRK},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2017},
  pages = {45},
  doi = {10.31256/hsmr2018},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  research_field = {TR},
  data_type = {RI and KD and ED},
}

@inproceedings{Fontanelli2017b,
  author = {Fontanelli, G. A. and Ficuciello, F. and Villani, L. and Siciliano, B.},
  title = {Modelling and identification of the da Vinci Research Kit robotic arms},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/IROS.2017.8205948},
  semanticscholar = {https://www.semanticscholar.org/paper/481add0c55ad9dcf348398f7ead578775b1ef61d},
  research_field = {},
  data_type = {},
  isbn = {978-1-5386-2682-5},
}

@inproceedings{Zhang2017d,
  author = {Zhang, Lin and Ye, Menglong and Giannarou, Stamatia and Pratt, Philip and {Yang Guang-Zhong"}, editor="Descoteaux Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},
  title = {Motion-Compensated Autonomous Scanning for Tumour Localisation Using Intraoperative Ultrasound},
  booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2017},
  year = {2017},
  doi = {10.1007/978-3-319-66185-8},
  semanticscholar = {https://www.semanticscholar.org/paper/86a7eb7b016971465ba705cec826e9c254e3ca8c},
  research_field = {HW},
  data_type = {RI and KD and ED},
}

@inproceedings{Thananjeyan2017b,
  author = {Thananjeyan, Brijen and Garg, Animesh and Krishnan, Sanjay and Chen, Carolyn and Miller, Lauren and Goldberg, Ken},
  title = {Multilateral surgical pattern cutting in 2D orthotropic gauze with deep reinforcement learning policies for tensioning},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2017.7989275},
  semanticscholar = {https://www.semanticscholar.org/paper/07be721c0268da8baf08db09da0dea8213b7bdcd},
  research_field = {AU},
  data_type = {RI and KD and ED},
  isbn = {978-1-5090-4633-1},
}

@article{Gandhi2017,
  author = {Gandhi, Neeraj and Allard, Margaret and Kim, Sungmin and Kazanzides, Peter and Bell, Muyinatu A Lediju},
  title = {Photoacoustic-based approach to surgical guidance performed with and without a da Vinci robot},
  journal = {Journal of Biomedical Optics},
  year = {2017},
  volume = {22},
  number = {12},
  publisher = {International Society for Optics and Photonics},
  doi = {10.1117/1.JBO.22.12.121606},
  semanticscholar = {https://www.semanticscholar.org/paper/8c5036b90119ca9a5bc4d7a55865a3b4845f85cd},
  research_field = {IM},
  data_type = {RI and KD and DD and SD and ED},
  dvrk_site = {JHU},
  abstract = {Abstract. Death and paralysis are significant risks of modern surgeries, caused by injury to blood vessels and nerves hidden by bone and other tissue. We propose an approach to surgical guidance that relies on photoacoustic (PA) imaging to determine the separation between these critical anatomical features and to assess the extent of safety zones during surgical procedures. Images were acquired as an optical fiber was swept across vessel-mimicking targets, in the absence and presence of teleoperation with a research da Vinci Surgical System. Vessel separation distances were measured directly from PA images. Vessel positions were additionally recorded based on the fiber position (calculated from the da Vinci robot kinematics) that corresponded to an observed PA signal, and these recordings were used to indirectly measure vessel separation distances. Amplitude- and coherence-based beamforming were used to estimate vessel separations, resulting in 0.52- to 0.56-mm mean absolute errors, 0.66- to 0.71-mm root-mean-square errors, and 65% to 68% more accuracy compared to fiber position measurements obtained through the da Vinci robot kinematics. Similar accuracy was achieved in the presence of up to 4.5-mm-thick ex vivo tissue. Results indicate that PA image-based measurements of the separation among anatomical landmarks could be a viable method for real-time path planning in multiple interventional PA applications.},
  issn = {1083-3668},
}

@inproceedings{Kim2017b,
  author = {Kim, Sungmin and Tan, Youri and Deguet, Anton and Kazanzides, Peter},
  title = {Real-time image-guided telerobotic system integrating 3D Slicer and the da Vinci Research Kit},
  booktitle = {2017 First IEEE International Conference on Robotic Computing (IRC)},
  year = {2017},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/6dd6a6310171b45598885b4408c49b3facb2ea7c},
  research_field = {IM},
  data_type = {RI and KD and DD and SD and ED},
  isbn = {1509067248},
}

@article{Zhang2017g,
  author = {Zhang, Lin and Ye, Menglong and Chan, Po-Ling and Yang, Guang-Zhong},
  title = {Real-time surgical tool tracking and pose estimation using a hybrid cylindrical marker},
  journal = {International journal of computer assisted radiology and surgery},
  year = {2017},
  volume = {12},
  number = {6},
  publisher = {Springer},
  doi = {10.1007/s11548-017-1558-9},
  semanticscholar = {https://www.semanticscholar.org/paper/d4d3fae3fa810d13fab7db28fc45bdc6b84ca15c},
  research_field = {IM},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {ICL},
  abstract = {PurposeTo provide an integrated visualisation of intraoperative ultrasound and endoscopic images to facilitate intraoperative guidance, real-time tracking of the ultrasound probe is required. State-of-the-art methods are suitable for planar targets while most of the laparoscopic ultrasound probes are cylindrical objects. A tracking framework for cylindrical objects with a large work space will improve the usability of the intraoperative ultrasound guidance.MethodsA hybrid marker design that combines circular dots and chessboard vertices is proposed for facilitating tracking cylindrical tools. The circular dots placed over the curved surface are used for pose estimation. The chessboard vertices are employed to provide additional information for resolving the ambiguous pose problem due to the use of planar model points under a monocular camera. Furthermore, temporal information between consecutive images is considered to minimise tracking failures with real-time computational performance.ResultsDetailed validation confirms that our hybrid marker provides a large working space for different tool sizes (6–14 mm in diameter). The tracking framework allows translational movements between 40 and 185 mm along the depth direction and rotational motion around three local orthogonal axes up to $$ \pm 80^\circ $$±80∘. Comparative studies with the current state of the art confirm that our approach outperforms existing methods by providing nearly 100% detection rates and accurate pose estimation with mean errors of 2.8 mm and 0.72$$^\circ $$∘. The tracking algorithm runs at 20 frames per second for $$960\times 540$$960×540 image resolution videos.ConclusionExperiments show that the proposed hybrid marker can be applied to a wide range of surgical tools with superior detection rates and pose estimation accuracies. Both the qualitative and quantitative results demonstrate that our framework can be used not only for assisting intraoperative ultrasound guidance but also for tracking general surgical tools in MIS.},
  issn = {1861-6410},
}

@article{Jackson2017,
  author = {Jackson, Russell C and Yuan, Rick and Chow, Der-Lin and Newman, Wyatt S and {\c{C}}avuşoğlu, M Cenk},
  title = {Real-time visual tracking of dynamic surgical suture threads},
  journal = {IEEE Transactions on Automation science and Engineering},
  year = {2017},
  volume = {15},
  number = {3},
  publisher = {IEEE},
  doi = {10.1109/TASE.2017.2726689},
  semanticscholar = {https://www.semanticscholar.org/paper/e19ae705aec24e9f23086c81ceff44e88352050b},
  research_field = {},
  data_type = {},
  dvrk_site = {CWRU},
  abstract = {In order to realize many of the potential benefits associated with robotically assisted minimally invasive surgery, the robot must be more than a remote controlled device. Currently, using a surgical robot can be challenging, fatiguing, and time-consuming. Teaching the robot to actively assist surgical tasks, such as suturing, has the potential to vastly improve both the patient’s outlook and the surgeon’s efficiency. One obstacle to completing surgical sutures autonomously is the difficulty in tracking surgical suture threads. This paper presents novel stereo image processing algorithms for the detection, initialization, and tracking of a surgical suture thread. A nonuniform rational B-spline (NURBS) curve is used to model a thin, deformable, and dynamic length thread. The NURBS model is initialized and grown from a single selected point located on the thread. The NURBS curve is optimized by minimizing the image matching energy between the projected stereo NURBS image and the segmented thread image. The algorithms are evaluated using suture threads, a calibrated test pattern, and a simulated thread image. In addition, the accuracies of the algorithms presented are validated as they track a suture thread undergoing translation, deformation, and apparent length changes. All of the tracking is in real time. Note to Practitioners—The problem of tracking a surgical suture thread was addressed in this paper. Since the suture thread is highly deformable, any tracking algorithm must be robust to intersections, occlusions, knot tying, and length changes. The detection algorithm introduced in this paper is capable of distinguishing different threads when they intersect. The tracking algorithm presented here demonstrates that it is possible, using polynomial curves, to track a suture thread as it deforms, becomes occluded, changes length, and even ties a knot in real time. The detection algorithm can enhance directional thin features, while the polynomial curve modeling can track any string-like structure. Further integration of the polynomial curve with a feed-forward thread model could improve the stability and robustness of the thread tracking.},
  issn = {1545-5955},
}

@inproceedings{Penza2017a,
  author = {Penza, V and {De Momi}, E and Enayati, N and Chupin, T and Ortiz, J and Mattos, L. S},
  title = {Safety Enhancement Framework for Robotic Minimally Invasive Surgery},
  booktitle = {10th Hamlyn Symposium on Medical Robotics 2017},
  year = {2017},
  publisher = {The Hamlyn Centre, Faculty of Engineering, Imperial College London},
  doi = {10.31256/HSMR2017.26},
  semanticscholar = {https://www.semanticscholar.org/paper/136af21d8e212cda33f66488939aed946eb644ec},
  research_field = {HW},
  data_type = {KD},
  isbn = {9780956377685},
}

@article{Ye2017a,
  author = {Ye, Menglong and Johns, Edward and Handa, Ankur and Zhang, Lin and Pratt, Philip and Yang, Guang-Zhong},
  title = {Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery},
  journal = {arXiv preprint arXiv:1705.08260},
  year = {2017},
  doi = {10.31256/HSMR2017.14},
  url = {https://arxiv.org/abs/1705.08260},
  semanticscholar = {https://www.semanticscholar.org/paper/16832680c0b9ec642cfdcd94b644eedcfb4cf266},
  arxiv = {https://arxiv.org/abs/1705.08260},
  research_field = {IM},
  data_type = {RI},
  dvrk_site = {ICL},
  abstract = {Robotic surgery has become a powerful tool for performing minimally invasive procedures, providing advantages in dexterity, precision, and 3D vision, over traditional surgery. One popular robotic system is the da Vinci surgical platform, which allows preoperative information to be incorporated into live procedures using Augmented Reality (AR). Scene depth estimation is a prerequisite for AR, as accurate registration requires 3D correspondences between preoperative and intraoperative organ models. In the past decade, there has been much progress on depth estimation for surgical scenes, such as using monocular or binocular laparoscopes [1,2]. More recently, advances in deep learning have enabled depth estimation via Convolutional Neural Networks (CNNs) [3], but training requires a large image dataset with ground truth depths. Inspired by [4], we propose a deep learning framework for surgical scene depth estimation using self-supervision for scalable data acquisition. Our framework consists of an autoencoder for depth prediction, and a differentiable spatial transformer for training the autoencoder on stereo image pairs without ground truth depths. Validation was conducted on stereo videos collected in robotic partial nephrectomy.},
}

@inproceedings{Chen2017,
  author = {Chen, Zihan and Deguet, Anton and Taylor, Russell H and Kazanzides, Peter},
  title = {Software architecture of the da Vinci Research Kit},
  booktitle = {2017 First IEEE International Conference on Robotic Computing (IRC)},
  year = {2017},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/6dd6a6310171b45598885b4408c49b3facb2ea7c},
  research_field = {},
  data_type = {},
  isbn = {1509067248},
}

@inproceedings{Chow2017b,
  author = {Chow, Der-Lin and Xu, Peng and Tuna, Eser and Huang, Siqi and Cavusoglu, M. Cenk and Newman, Wyatt},
  title = {Supervisory control of a DaVinci surgical robot},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/IROS.2017.8206389},
  semanticscholar = {https://www.semanticscholar.org/paper/ee5927a650f5ecb9b86994c3efca599be267bc24},
  research_field = {AU},
  data_type = {RI and KD and ED},
  isbn = {978-1-5386-2682-5},
}

@inproceedings{Haidegger2017,
  author = {Haidegger, Tam{\'{a}}s},
  title = {Surgical robots of the next decade: New trends and paradigms in the 21th century},
  booktitle = {2017 IEEE 30th Neumann Colloquium (NC)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.7275/R5WM1BKZ},
  semanticscholar = {https://www.semanticscholar.org/paper/36b7dbfb2abe2fb5e9f0d0f35fdf3d0f38b1cede},
  research_field = {},
  data_type = {},
  isbn = {1538646366},
}

@inproceedings{Garcia-Peraza-Herrera2017a,
  author = {Garc{\'{i}}a-Peraza-Herrera, Luis C and Li, Wenqi and Fidon, Lucas and Gruijthuijsen, Caspar and Devreker, Alain and Attilakos, George and Deprest, Jan and {Vander Poorten}, Emmanuel and Stoyanov, Danail and Vercauteren, Tom},
  title = {Toolnet: holistically-nested real-time segmentation of robotic surgical tools},
  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/iros37595.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  research_field = {},
  data_type = {},
  isbn = {1538626829},
}

@inproceedings{Elek2017,
  author = {Elek, Ren{\'{a}}ta and Nagy, Tam{\'{a}}s D and Nagy, D{\'{e}}nes {\'{A}} and Garamv{\"{o}}lgyi, Tivadar and Tak{\'{a}}cs, Bence and Galambos, P{\'{e}}ter and Tar, J{\'{o}}zsef K and Rudas, Imre J and Haidegger, Tam{\'{a}}s},
  title = {Towards surgical subtask automation—Blunt dissection},
  booktitle = {2017 IEEE 21st International Conference on Intelligent Engineering Systems (INES)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/ines41804.2017},
  semanticscholar = {https://www.semanticscholar.org/paper/836040bb4cd9c1d07d10444a32fd3d4292ea3201},
  research_field = {},
  data_type = {},
  isbn = {1479976784},
}

@inproceedings{Coad2017a,
  author = {Coad, Margaret M. and Okamura, Allison M. and Wren, Sherry and Mintz, Yoav and Lendvay, Thomas S. and Jarc, Anthony M. and Nisky, Ilana},
  title = {Training in divergent and convergent force fields during 6-DOF teleoperation with a robot-assisted surgical system},
  booktitle = {2017 IEEE World Haptics Conference (WHC)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/WHC.2017.7989900},
  semanticscholar = {https://www.semanticscholar.org/paper/a0f8ab29fc09e480d6b187e93239347de172d0f6},
  research_field = {TR},
  data_type = {KD},
  isbn = {978-1-5090-1425-5},
}

@article{Krishnan2017,
  author = {Krishnan, Sanjay and Garg, Animesh and Patil, Sachin and Lea, Colin and Hager, Gregory and Abbeel, Pieter and Goldberg, Ken},
  title = {Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning},
  journal = {The International Journal of Robotics Research},
  year = {2017},
  volume = {36},
  number = {13-14},
  publisher = {SAGE Publications Sage UK: London, England},
  doi = {10.1177/0278364917743319},
  semanticscholar = {https://www.semanticscholar.org/paper/694d284d7c63c7f29c48b540bc4ece9f63b79160},
  research_field = {},
  data_type = {},
  dvrk_site = {UCB},
  issn = {0278-3649},
}

@inproceedings{Liang2017b,
  author = {Liang, Jacky and Mahler, Jeffrey and Laskey, Michael and Li, Pusong and Goldberg, Ken},
  title = {Using dVRK teleoperation to facilitate deep learning of automation tasks for an industrial robot},
  booktitle = {2017 13th IEEE Conference on Automation Science and Engineering (CASE)},
  year = {2017},
  publisher = {IEEE},
  doi = {10.1109/COASE.2017.8256067},
  semanticscholar = {https://www.semanticscholar.org/paper/a7ab653e7bc6af1fa5442c5f56bcd2a4562e8e96},
  research_field = {AU},
  data_type = {RI and KD and ED},
  isbn = {978-1-5090-6781-7},
}

@article{Bouget2017,
  author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},
  title = {Vision-based and marker-less surgical tool detection and tracking: a review of the literature},
  journal = {Medical image analysis},
  year = {2017},
  volume = {35},
  publisher = {Elsevier},
  doi = {10.1016/j.media.2016.09.003},
  semanticscholar = {https://www.semanticscholar.org/paper/703f8fb5aefdc684c2ef7794a47bbd3bfabca628},
  research_field = {IM},
  data_type = {RI},
  dvrk_site = {UCL},
  issn = {1361-8415},
}

@article{Wang2017b,
  author = {Wang, Zerui and Liu, Ziwei and Ma, Qianli and Cheng, Alexis and Liu, Yun-hui and Kim, Sungmin and Deguet, Anton and Reiter, Austin and Kazanzides, Peter and Taylor, Russell H},
  title = {Vision-based calibration of dual RCM-based robot arms in human-robot collaborative minimally invasive surgery},
  journal = {IEEE Robotics and Automation Letters},
  year = {2017},
  volume = {3},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/LRA.2017.2737485},
  semanticscholar = {https://www.semanticscholar.org/paper/1ac0078cdfdb0080ddcdb46c42fe2f22405d1c5d},
  research_field = {IM},
  data_type = {RI and KD and ED},
  dvrk_site = {JHU},
  issn = {2377-3766},
}

@article{Kim2016,
  author = {Kim, Myungjoon and Lee, Chiwon and Park, Woo Jung and Suh, Yun Suhk and Yang, Han Kwang and Kim, H Jin and Kim, Sungwan},
  title = {A development of assistant surgical robot system based on surgical-operation-by-wire and hands-on-throttle-and-stick},
  journal = {Biomedical engineering online},
  year = {2016},
  volume = {15},
  number = {1},
  publisher = {Springer},
  doi = {10.1186/s12938-016-0189-7},
  semanticscholar = {https://www.semanticscholar.org/paper/9d56fc4aecd26875376a74abc822ef645a62ace6},
  research_field = {HW},
  data_type = {RI and KD and DD and SD and ED},
  dvrk_site = {SNU},
  abstract = {Robot-assisted laparoscopic surgery offers several advantages compared with open surgery and conventional minimally invasive surgery. However, one issue that needs to be resolved is a collision between the robot arm and the assistant instrument. This is mostly caused by miscommunication between the surgeon and the assistant. To resolve this limitation, an assistant surgical robot system that can be simultaneously manipulated via a wireless controller is proposed to allow the surgeon to control the assistant instrument. The system comprises two novel master interfaces (NMIs), a surgical instrument with a gripper actuated by a micromotor, and 6-axis robot arm. Two NMIs are attached to master tool manipulators of da Vinci research kit (dVRK) to control the proposed system simultaneously with patient side manipulators of dVRK. The developments of the surgical instrument and NMI are based on surgical-operation-by-wire concept and hands-on-throttle-and-stick concept from the earlier research, respectively. Tests for checking the accuracy, latency, and power consumption of the NMI are performed. The gripping force, reaction time, and durability are assessed to validate the surgical instrument. The workspace is calculated for estimating the clinical applicability. A simple peg task using the fundamentals of laparoscopic surgery board and an in vitro test are executed with three novice volunteers. The NMI was operated for 185 min and reflected the surgeon’s decision successfully with a mean latency of 132 ms. The gripping force of the surgical instrument was comparable to that of conventional systems and was consistent even after 1000 times of gripping motion. The reaction time was 0.4 s. The workspace was calculated to be 8397.4 cm3. Recruited volunteers were able to execute the simple peg task within the cut-off time and successfully performed the in vitro test without any collision. Various experiments were conducted and it is verified that the proposed assistant surgical robot system enables collision-free and simultaneous operation of the dVRK’s robot arm and the proposed assistant robot arm. The workspace is appropriate for the performance of various kinds of surgeries. Therefore, the proposed system is expected to provide higher safety and effectiveness for the current surgical robot system.},
  issn = {1475-925X},
}

@article{Munawar2016b,
  author = {Munawar, Adnan and Fischer, Gregory},
  title = {A Surgical Robot Teleoperation Framework for Providing Haptic Feedback Incorporating Virtual Environment-Based Guidance},
  journal = {Frontiers in Robotics and AI},
  year = {2016},
  volume = {3},
  doi = {10.3389/frobt.2016.00047},
  semanticscholar = {https://www.semanticscholar.org/paper/af7aacf2f3c60bbf7d69aae4c48ee62caaee385a},
  research_field = {HW},
  data_type = {RI and KD and DD and SD},
  dvrk_site = {WPI},
  abstract = {In robot-assisted tele-operated laparoscopic surgeries, the patient side manipulators are controlled via the master manipulators that are controlled by the surgeon. The current generation of robots approved for laparoscopic surgery lack haptic feedback. In theory, haptic feedback would enhance the surgical procedures by enabling better coordination between the hand movements that are improved by the tactile sense of the operating environment. This research presents an overall control framework for a haptic feedback on existing robot platforms, and demonstrated on the daVinci Research Kit (dVRK) system. The paper discusses the implementation of a flexible framework that incorporates a stiffness control with gravity compensation for the surgeons manipulator and a sensing and collision detection algorithm for calculating the interaction between the patients manipulators and the surgical area.},
  issn = {2296-9144},
}

@inproceedings{McKinley2016b,
  author = {McKinley, Stephen and Garg, Animesh and Sen, Siddarth and Gealy, David V. and McKinley, Jonathan P. and Jen, Yiming and {Menglong Guo} and Boyd, Doug and Goldberg, Ken},
  title = {An interchangeable surgical instrument system with application to supervised automation of multilateral tumor resection},
  booktitle = {2016 IEEE International Conference on Automation Science and Engineering (CASE)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/COASE.2016.7743487},
  semanticscholar = {https://www.semanticscholar.org/paper/fd12947f733e7a8a76c1eaa2e1bd6c1be76a4099},
  research_field = {AU},
  data_type = {KD},
  isbn = {978-1-5090-2409-4},
}

@inproceedings{Sen2016,
  author = {Sen, Siddarth and Garg, Animesh and Gealy, David V and McKinley, Stephen and Jen, Yiming and Goldberg, Ken},
  title = {Automating multi-throw multilateral surgical suturing with a mechanical needle guide and sequential convex optimization},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2016.7487141},
  semanticscholar = {https://www.semanticscholar.org/paper/e44596de68c5289a56998a1c5215434c381fc0c9},
  research_field = {},
  data_type = {},
  isbn = {1467380261},
}

@inproceedings{Ruszkowski2016,
  author = {Ruszkowski, Angelica and Schneider, Caitlin and Mohareri, Omid and Salcudean, Septimiu},
  title = {Bimanual teleoperation with heart motion compensation on the da Vinci{\textregistered} Research Kit: Implementation and preliminary experiments},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2016.7487141},
  semanticscholar = {https://www.semanticscholar.org/paper/e44596de68c5289a56998a1c5215434c381fc0c9},
  research_field = {},
  data_type = {},
  isbn = {1467380261},
}

@article{Du2016a,
  author = {Du, Xiaofei and Allan, Maximilian and Dore, Alessio and Ourselin, Sebastien and Hawkes, David and Kelly, John D. and Stoyanov, Danail},
  title = {Combined 2D and 3D tracking of surgical instruments for minimally invasive and robotic-assisted surgery},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  year = {2016},
  volume = {11},
  number = {6},
  doi = {10.1007/s11548-016-1393-4},
  semanticscholar = {https://www.semanticscholar.org/paper/e6b49eb5a3b4c62902f73563fbed88a8d92d1adc},
  research_field = {},
  data_type = {},
  dvrk_site = {UCL},
  abstract = {PurposeComputer-assisted interventions for enhanced minimally invasive surgery (MIS) require tracking of the surgical instruments. Instrument tracking is a challenging problem in both conventional and robotic-assisted MIS, but vision-based approaches are a promising solution with minimal hardware integration requirements. However, vision-based methods suffer from drift, and in the case of occlusions, shadows and fast motion, they can be subject to complete tracking failure.MethodsIn this paper, we develop a 2D tracker based on a Generalized Hough Transform using SIFT features which can both handle complex environmental changes and recover from tracking failure. We use this to initialize a 3D tracker at each frame which enables us to recover 3D instrument pose over long sequences and even during occlusions.ResultsWe quantitatively validate our method in 2D and 3D with ex vivo data collected from a DVRK controller as well as providing qualitative validation on robotic-assisted in vivo data.ConclusionsWe demonstrate from our extended sequences that our method provides drift-free robust and accurate tracking. Our occlusion-based sequences additionally demonstrate that our method can recover from occlusion-based failure. In both cases, we show an improvement over using 3D tracking alone suggesting that combining 2D and 3D tracking is a promising solution to challenges in surgical instrument tracking.},
  issn = {1861-6410},
}

@inproceedings{Kim2016a,
  author = {Kim, Sungmin and Tan, Youri and Kazanzides, Peter and Bell, Muyinatu A Lediju},
  title = {Feasibility of photoacoustic image guidance for telerobotic endonasal transsphenoidal surgery},
  booktitle = {2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  year = {2016},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/f0321d338a9689fe331612c9b597a2e999d6e7b0},
  research_field = {},
  data_type = {},
  isbn = {1509032878},
}

@inproceedings{Pachtrachai2016a,
  author = {Pachtrachai, Krittin and Allan, Max and Pawar, Vijay and Hailes, Stephen and Stoyanov, Danail},
  title = {Hand-eye calibration for robotic assisted minimally invasive surgery without a calibration object},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/IROS.2016.7759387},
  semanticscholar = {https://www.semanticscholar.org/paper/2031c29890fecbe746039194b6d422f39850b0d8},
  research_field = {},
  data_type = {},
  isbn = {978-1-5090-3762-9},
}

@inproceedings{Grammatikopoulou2016a,
  author = {Grammatikopoulou, Maria and Leibrandt, Konrad and Yang, Guang-Zhong},
  title = {Motor channelling for safe and effective dynamic constraints in Minimally Invasive Surgery},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/IROS.2016.7759635},
  semanticscholar = {https://www.semanticscholar.org/paper/370c41b720a7bfd13cf607778c87407ff88a235f},
  research_field = {HW},
  data_type = {RI and KD and SD and ED},
  abstract = {Motor channelling is a concept to provide navigation and sensory feedback to operators in master-slave surgical setups. It is beneficial since the introduction of robotic surgery creates a physical separation between the surgeon and patient anatomy. Active Constraints/Virtual Fixtures are proposed which integrate Guidance and Forbidden Region Constraints into a unified control framework. The developed approach provides guidance and safe manipulation to improve precision and reduce the risk of inadvertent tissue damage. Online three-degree-of-freedom motion prediction and compensation of the target anatomy is performed to complement the master constraints. The presented Active Constraints concept is applied to two clinical scenarios; surface scanning for in situ medical imaging and vessel manipulation in cardiac surgery. The proposed motor channelling control strategy is implemented on the da Vinci Surgical System using the da Vinci Research Kit (dVRK) and its effectiveness is demonstrated through a detailed user study.},
  isbn = {978-1-5090-3762-9},
}

@article{Liu2016,
  author = {Liu, Taoming and Cavusoglu, Murat Cenk},
  title = {Needle grasp and entry port selection for automatic execution of suturing tasks in robotic minimally invasive surgery},
  journal = {IEEE Transactions on Automation Science and Engineering},
  year = {2016},
  volume = {13},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/TASE.2016.2515161},
  semanticscholar = {https://www.semanticscholar.org/paper/d26610516a40292b0061ee60923bd9cc20fdb3da},
  research_field = {AU},
  data_type = {RI and KD and ED},
  dvrk_site = {CWRU},
  issn = {1545-5955},
}

@inproceedings{Ye2016,
  author = {Ye, Menglong and Zhang, Lin and Giannarou, Stamatia and Yang, Guang-Zhong},
  title = {Real-time 3d tracking of articulated tools for robotic surgery},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention -- MICCAI},
  year = {2016},
  publisher = {Springer},
  doi = {10.1007/978-3-032-05162-2},
  semanticscholar = {https://www.semanticscholar.org/paper/f90ea6007f7206eef60ff17a82f951f602f9511d},
  research_field = {},
  data_type = {},
}

@inproceedings{Elek2016,
  author = {Elek, Ren{\'{a}}ta and Nagy, Tam{\'{a}}s D{\'{a}}niel and Nagy, D{\'{e}}nes {\'{A}}kos and Kronreif, Gernot and Rudas, Imre J and Haidegger, Tam{\'{a}}s},
  title = {Recent trends in automating robotic surgery},
  booktitle = {2016 IEEE 20th Jubilee International Conference on Intelligent Engineering Systems (INES)},
  year = {2016},
  publisher = {IEEE},
  research_field = {},
  data_type = {},
  isbn = {1509012168},
}

@inproceedings{Shahzada2016a,
  author = {Shahzada, Kaspar S. and Yurkewich, Aaron and Xu, Ran and Patel, Rajni V.},
  title = {Sensorization of a surgical robotic instrument for force sensing},
  booktitle = {Optical Fibers and Sensors for Medical Diagnostics and Treatment Applications XVI},
  year = {2016},
  doi = {10.1117/12.2213385},
  semanticscholar = {https://www.semanticscholar.org/paper/4e8f6e73bfbfbdda0a4388108ba355143ef4d22b},
  research_field = {HW},
  data_type = {KD and DD},
  editor = {Gannot, Israel},
}

@inproceedings{Kaplan2016a,
  author = {Kaplan, Kirsten E. and Nichols, Kirk A. and Okamura, Allison M.},
  title = {Toward human-robot collaboration in surgery: Performance assessment of human and robotic agents in an inclusion segmentation task},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2016.7487199},
  semanticscholar = {https://www.semanticscholar.org/paper/25d2eb190571d46d18c046fc6758d83db80f8bbf},
  research_field = {AU},
  data_type = {RI and DD},
  isbn = {978-1-4673-8026-3},
}

@inproceedings{Munawar2016a,
  author = {Munawar, Adnan and Fischer, Gregory},
  title = {Towards a haptic feedback framework for multi-DOF robotic laparoscopic surgery platforms},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1007/978-3-319-94568-2_13},
  semanticscholar = {https://www.semanticscholar.org/paper/de5d5043c29e95891f2e52d30806646dba159d09},
  research_field = {HW},
  data_type = {KD and DD and SD},
  isbn = {1509037624},
}

@inproceedings{Eslamian2016,
  author = {Eslamian, Shahab and Reisner, Luke A and King, Brady W and Pandya, Abhilash K},
  title = {Towards the Implementation of an Autonomous Camera Algorithm on the da Vinci Platform.},
  booktitle = {Stud Health Technol Inform},
  year = {2016},
  doi = {10.1002/aorn.12159},
  semanticscholar = {https://www.semanticscholar.org/paper/0d15aff91a3c60089003db2c958d38e80925ea5f},
  research_field = {},
  data_type = {},
}

@inproceedings{Murali2016,
  author = {Murali, Adithyavairavan and Garg, Animesh and Krishnan, Sanjay and Pokorny, Florian T and Abbeel, Pieter and Darrell, Trevor and Goldberg, Ken},
  title = {Tsc-dl: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with deep learning},
  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2016.7487141},
  semanticscholar = {https://www.semanticscholar.org/paper/e44596de68c5289a56998a1c5215434c381fc0c9},
  research_field = {TR},
  data_type = {RI and KD},
  isbn = {1467380261},
}

@inproceedings{Garg2016b,
  author = {Garg, Animesh and Sen, Siddarth and Kapadia, Rishi and Jen, Yiming and McKinley, Stephen and Miller, Lauren and Goldberg, Ken},
  title = {Tumor localization using automated palpation with Gaussian Process Adaptive Sampling},
  booktitle = {2016 IEEE International Conference on Automation Science and Engineering (CASE)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/COASE.2016.7743380},
  semanticscholar = {https://www.semanticscholar.org/paper/d319a1ac5e30272aeaafca8ca61eec7466af82c9},
  research_field = {},
  data_type = {},
  abstract = {In surgical tumor removal, inaccurate localization can lead to removal of excessive healthy tissue and failure to completely remove cancerous tissue. Automated palpation with a tactile sensor has the potential to precisely estimate the geometry of embedded tumors during robot-assisted minimally invasive surgery (RMIS). We formulate tumor boundary localization as a Bayesian optimization model along implicit curves over estimated tissue stiffness. We propose a Gaussian Process Adaptive Sampling algorithm called Implicit Level Set Upper Confidence Bound (ILS-UCB), that prioritizes sampling near a level set of the estimate. We compare the ILS-UCB algorithm to two alternative palpation algorithms: (1) Expected Variance Reduction (EVR), which emphasizes exploration by minimizing variance, and (2) Upper Confidence Bound (UCB), which balances exploration with exploitation using only the estimated mean. We compare these algorithms in simulated experiments varying the levels of measurement noise and bias. We find that ILS-UCB significantly outperforms the other two algorithms as measured by the symmetric difference between tumor boundary estimate and ground truth, reducing error by up to 10×. Physical experiments on a dVRK show that our approach can localize the tumor boundary with approximately the same accuracy as a dense raster scan while requiring at least 10× fewer measurements.},
  isbn = {978-1-5090-2409-4},
}

@inproceedings{Che2016,
  author = {Che, Yuhang and Haro, Gabriel M and Okamura, Allison M},
  title = {Two is not always better than one: Effects of teleoperation and haptic coupling},
  booktitle = {2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  year = {2016},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/f0321d338a9689fe331612c9b597a2e999d6e7b0},
  research_field = {HW},
  data_type = {KD and DD and ED},
  isbn = {1509032878},
}

@inproceedings{Wang2016,
  author = {Wang, Long and Chen, Zihan and Chalasani, Preetham and Pile, Jason and Kazanzides, Peter and Taylor, Russell H and Simaan, Nabil},
  title = {Updating virtual fixtures from exploration data in force-controlled model-based telemanipulation},
  booktitle = {ASME 2016 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference},
  year = {2016},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  doi = {10.1115/DETC2016-60250},
  semanticscholar = {https://www.semanticscholar.org/paper/920270441e2e4089da4ae5807cf8870b28abaab6},
  research_field = {HW},
  data_type = {RI and KD and DD and SD and ED},
}

@inproceedings{Chen2016a,
  author = {Chen, Zihan and Malpani, Anand and Chalasani, Preetham and Deguet, Anton and Vedula, S. Swaroop and Kazanzides, Peter and Taylor, Russell H.},
  title = {Virtual fixture assistance for needle passing and knot tying},
  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2016},
  publisher = {IEEE},
  doi = {10.1109/IROS.2016.7759365},
  semanticscholar = {https://www.semanticscholar.org/paper/9ef4b2ab8ecfba1451a21b5820d1aeacd00fa904},
  research_field = {AU},
  data_type = {RI and KD and DD and SD and ED},
  isbn = {978-1-5090-3762-9},
}

@inproceedings{Shamaei2015b,
  author = {Shamaei, Kamran and Che, Yuhang and Murali, Adithyavairavan and Sen, Siddarth and Patil, Sachin and Goldberg, Ken and Okamura, Allison M.},
  title = {A paced shared-control teleoperated architecture for supervised automation of multilateral surgical tasks},
  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/IROS.2015.7353556},
  semanticscholar = {https://www.semanticscholar.org/paper/930f4e1c4a3b0e9b28ac4a782e40cbcd22ce35ee},
  research_field = {},
  data_type = {},
  isbn = {978-1-4799-9994-1},
}

@inproceedings{Tong2015a,
  author = {Tong, Irene and Mohareri, Omid and Tatasurya, Samuel and Hennessey, Craig and Salcudean, Septimiu},
  title = {A retrofit eye gaze tracker for the da Vinci and its integration in task execution using the da Vinci Research Kit},
  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/IROS.2015.7353648},
  semanticscholar = {https://www.semanticscholar.org/paper/9fa9e607fca7601c94abac984d307cf1ecc6b50a},
  research_field = {},
  data_type = {},
  isbn = {978-1-4799-9994-1},
}

@inproceedings{McKinley2015b,
  author = {McKinley, Stephen and Garg, Animesh and Sen, Siddarth and Kapadia, Rishi and Murali, Adithyavairavan and Nichols, Kirk and Lim, Susan and Patil, Sachin and Abbeel, Pieter and Okamura, Allison M. and Goldberg, Ken},
  title = {A single-use haptic palpation probe for locating subcutaneous blood vessels in robot-assisted minimally invasive surgery},
  booktitle = {2015 IEEE International Conference on Automation Science and Engineering (CASE)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/CoASE.2015.7294253},
  semanticscholar = {https://www.semanticscholar.org/paper/2f63e718e5593ef5151c7024a94ea05eb4730f6d},
  research_field = {},
  data_type = {},
  isbn = {978-1-4673-8183-3},
}

@inproceedings{Ferraguti2015,
  author = {Ferraguti, Federica and Preda, Nicola and {De Rossi}, Giacomo and Bonfe, Marcello and Muradore, Riccardo and Fiorini, Paolo and Secchi, Cristian},
  title = {A two-layer approach for shared control in semi-autonomous robotic surgery},
  booktitle = {2015 European Control Conference (ECC)},
  year = {2015},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/24396f11bdc9ca698083e969c159bb8c34401924},
  research_field = {},
  data_type = {},
  isbn = {3952426938},
}

@inproceedings{Qian2015,
  author = {Qian, Long and Chen, Zihan and Kazanzides, Peter},
  title = {An Ethernet to FireWire bridge for real-time control of the da Vinci Research Kit (dVRK)},
  booktitle = {2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)},
  year = {2015},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/ea13cc8fc5198fec59645f2d729036f9e8707f1e},
  research_field = {},
  data_type = {},
  isbn = {1467379298},
}

@inproceedings{Khalaji2015a,
  author = {Khalaji, Iman and Naish, Michael D. and Patel, Rajni V.},
  title = {Articulating minimally invasive ultrasonic tool for robotics-assisted surgery},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2015.7139236},
  semanticscholar = {https://www.semanticscholar.org/paper/a80f824e53015d36c548403a92cf80105edc0c60},
  research_field = {HW},
  data_type = {KD and DD and ED},
  isbn = {978-1-4799-6923-4},
}

@inproceedings{Pratt2015c,
  author = {Pratt, Philip and Hughes-Hallett, Archie and Zhang, Lin and Patel, Nisha and Mayer, Erik and Darzi, Ara and Yang, Guang-Zhong},
  title = {Autonomous Ultrasound-Guided Tissue Dissection},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
  year = {2015},
  doi = {10.1007/978-3-319-24574-4},
  semanticscholar = {https://www.semanticscholar.org/paper/ee146a6677c610c3d094c58a38091e9c3cfff57e},
  research_field = {},
  data_type = {},
}

@article{Kumar2015,
  author = {Kumar, Suren and Singhal, Pankaj and Krovi, Venkat N},
  title = {Computer-vision-based decision support in surgical robotics},
  journal = {IEEE Design & Test},
  year = {2015},
  volume = {32},
  number = {5},
  publisher = {IEEE},
  doi = {10.1109/MDAT.2015.2465135},
  semanticscholar = {https://www.semanticscholar.org/paper/62b95792400fb513f02eb21640fda332ed55d888},
  research_field = {},
  data_type = {},
  dvrk_site = {CU},
  issn = {2168-2356},
}

@inproceedings{RuszkowskiHamlyn2015,
  author = {Ruszkowski, Angelica and Quek, Zhan Fan and Okamura, Allison M. and Salcudean, Septimiu},
  title = {Dynamic Non-Continuous Virtual Fixtures for Operations on a Beating Heart using the da Vinci{\textregistered} Surgical Robots},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2015},
  pages = {45--46},
  month = {June},
  address = {London, UK},
}

@inproceedings{Kim2015a,
  author = {Kim, Lawrence H. and Bargar, Clifford and Che, Yuhang and Okamura, Allison M.},
  title = {Effects of master-slave tool misalignment in a teleoperated surgical robot},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2015.7139948},
  semanticscholar = {https://www.semanticscholar.org/paper/2788f85b0e28f9a170737d763c33f99bdc26e6ca},
  research_field = {HW},
  data_type = {KD and SD and ED},
  isbn = {978-1-4799-6923-4},
}

@inproceedings{Vozar2015a,
  author = {Vozar, Steve and L{\'{e}}onard, Simon and Kazanzides, Peter and Whitcomb, Louis L},
  title = {Experimental evaluation of force control for virtual-fixture-assisted teleoperation for on-orbit manipulation of satellite thermal blanket insulation},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d},
  research_field = {},
  data_type = {},
  isbn = {1479969230},
}

@inproceedings{Allan2015,
  author = {Allan, Max and Chang, Ping-Lin and Ourselin, S{\'{e}}bastien and Hawkes, David J and Sridhar, Ashwin and Kelly, John and Stoyanov, Danail},
  title = {Image based surgical instrument pose estimation with multi-class labelling and optical flow},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year = {2015},
  publisher = {Springer},
  doi = {10.1007/978-3-032-05162-2},
  semanticscholar = {https://www.semanticscholar.org/paper/f90ea6007f7206eef60ff17a82f951f602f9511d},
  research_field = {IM},
  data_type = {RI},
}

@article{Chen2015,
  author = {Chen, Zihan and Deguet, Anton and Vozar, Steve and Munawar, Adnan and Fischer, Gregory and Kazanzides, Peter},
  title = {Interfacing the da Vinci Research Kit (dVRK) with the Robot Operating System (ROS)},
  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  semanticscholar = {https://www.semanticscholar.org/paper/896c8df3bee4dd806bea30e29be62ab5e71ea3c8},
  research_field = {HW},
  data_type = {RI and KD and DD and SD},
  dvrk_site = {JHU},
}

@article{Takacs2015,
  author = {Tak{\'{a}}cs, {\'{A}}rp{\'{a}}d and Jord{\'{a}}n, S{\'{a}}ndor and Nagy, D{\'{e}}nes and Pausits, P{\'{e}}ter and Haidegger, Tam{\'{a}}s and Tar, J{\'{o}}zsef K and Rudas, Imre J},
  title = {Joint platforms and community efforts in surgical robotics research},
  journal = {MACRo 2015},
  year = {2015},
  volume = {1},
  number = {1},
  publisher = {Sciendo},
  doi = {10.1515/macro-2015-0009},
  semanticscholar = {https://www.semanticscholar.org/paper/1a2fdc5568ce11bd668e7a5c1bd39a207f221f96},
  research_field = {},
  data_type = {},
  dvrk_site = {OU},
  abstract = {Abstract In modern medical research and development, the variety of research tools has extended in the previous years. Exploiting the benefits of shared hardware platforms and software frameworks is crucial to keep up with the technological development rate. Sharing knowledge in terms of algorithms, applications and instruments allows researchers to help each other’s work effectively. Community workshops and publications provide a throughout overview of system design, capabilities, know-how sharing and limitations. This paper provides sneak peek into the emerging collaborative platforms, focusing on available open-source research kits, software frameworks, cloud applications, teleoperation training environments and shared domain ontologies.},
  issn = {2247-0948},
}

@inproceedings{Murali2015c,
  author = {Murali, Adithyavairavan and Sen, Siddarth and Kehoe, Ben and Garg, Animesh and McFarland, Seth and Patil, Sachin and Boyd, W. Douglas and Lim, Susan and Abbeel, Pieter and Goldberg, Ken},
  title = {Learning by observation for surgical subtasks: Multilateral cutting of 3D viscoelastic and 2D Orthotropic Tissue Phantoms},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2015.7139344},
  semanticscholar = {https://www.semanticscholar.org/paper/7ff7a763c7b09459524329d689d953d43b740d04},
  research_field = {AU},
  data_type = {RI and KD and DD},
  isbn = {978-1-4799-6923-4},
}

@article{Kazanzides2015,
  author = {Kazanzides, Peter and Deguet, Anton and Vagvolgyi, Balazs and Chen, Zihan and Taylor, Russell H},
  title = {Modular interoperability in surgical robotics software},
  journal = {Mechanical Engineering},
  year = {2015},
  volume = {137},
  number = {09},
  publisher = {American Society of Mechanical Engineers},
  doi = {10.1115/1.2015-SEP-10},
  semanticscholar = {https://www.semanticscholar.org/paper/0d210fdfe1fc2413594944d725dcd33ff241b468},
  research_field = {},
  data_type = {},
  dvrk_site = {JHU},
  issn = {0025-6501},
}

@inproceedings{Ruszkowski2015a,
  author = {Ruszkowski, Angelica and Mohareri, Omid and Lichtenstein, Sam and Cook, Richard and Salcudean, Septimiu},
  title = {On the feasibility of heart motion compensation on the daVinci surgical robot for coronary artery bypass surgery: Implementation and user studies},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2015.7139812},
  semanticscholar = {https://www.semanticscholar.org/paper/183a31c43ccdaf52c88e772ee251717ca827a497},
  research_field = {HW},
  data_type = {RI and KD},
  isbn = {978-1-4799-6923-4},
}

@article{Takacs2015a,
  author = {Tak{\'{a}}cs, {\'{A}}rp{\'{a}}d and Rudas, Imre and Haidegger, Tam{\'{a}}s},
  title = {Open-source research platforms and system integration in modern surgical robotics},
  journal = {Acta Universitatis Sapientiae; Electrical and Mechanical Engineering},
  year = {2015},
  volume = {14},
  number = {6},
  publisher = {Sapientia Hungarian University of Transylvania, Scientia Publishing House},
  semanticscholar = {https://www.semanticscholar.org/paper/d27bbc03cd3c28a08991ed44c08b5d7a2a8085a1},
  research_field = {AU},
  data_type = {RI and KD and SD and ED},
  dvrk_site = {OU},
  issn = {2069-7449},
}

@inproceedings{Liu2015,
  author = {Liu, Taoming and {\c{C}}avuşoğlu, M Cenk},
  title = {Optimal needle grasp selection for automatic execution of suturing tasks in robotic minimally invasive surgery},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/TASE.2016.2515161},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d},
  research_field = {AU},
  data_type = {RI and KD and ED},
  isbn = {1479969230},
}

@inproceedings{Vozar2015,
  author = {Vozar, Steve and Chen, Zihan and Kazanzides, Peter and Whitcomb, Louis L},
  title = {Preliminary study of virtual nonholonomic constraints for time-delayed teleoperation},
  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/iros33365.2015},
  semanticscholar = {https://www.semanticscholar.org/paper/358a2f17483ed79929cd90dc3863451a9f974347},
  research_field = {},
  data_type = {},
  isbn = {1479999946},
}

@inproceedings{Leonard2015,
  author = {Leonard, Simon},
  title = {Registration of planar virtual fixtures by using augmented reality with dynamic textures},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d},
  research_field = {IM},
  data_type = {RI and KD and ED},
  isbn = {1479969230},
}

@article{Jarc2015,
  author = {Jarc, Anthony M and Nisky, Ilana},
  title = {Robot-assisted surgery: an emerging platform for human neuroscience research},
  journal = {Frontiers in human neuroscience},
  year = {2015},
  volume = {9},
  publisher = {Frontiers},
  doi = {10.3389/fnhum.2015.00315},
  semanticscholar = {https://www.semanticscholar.org/paper/0bd0a15287d42b363d1d7096831df45863dffb2e},
  research_field = {TR},
  data_type = {RI and KD},
  dvrk_site = {BGUN},
  abstract = {Classic studies in human sensorimotor control use simplified tasks to uncover fundamental control strategies employed by the nervous system. Such simple tasks are critical for isolating specific features of motor, sensory, or cognitive processes, and for inferring causality between these features and observed behavioral changes. However, it remains unclear how these theories translate to complex sensorimotor tasks or to natural behaviors. Part of the difficulty in performing such experiments has been the lack of appropriate tools for measuring complex motor skills in real-world contexts. Robot-assisted surgery (RAS) provides an opportunity to overcome these challenges by enabling unobtrusive measurements of user behavior. In addition, a continuum of tasks with varying complexity—from simple tasks such as those in classic studies to highly complex tasks such as a surgical procedure—can be studied using RAS platforms. Finally, RAS includes a diverse participant population of inexperienced users all the way to expert surgeons. In this perspective, we illustrate how the characteristics of RAS systems make them compelling platforms to extend many theories in human neuroscience, as well as, to develop new theories altogether.},
  issn = {1662-5161},
}

@inproceedings{Quek2015,
  author = {Quek, Zhan Fan and Schorr, Samuel B and Nisky, Ilana and Provancher, William R and Okamura, Allison M},
  title = {Sensory substitution of force and torque using 6-DoF tangential and normal skin deformation feedback},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d},
  research_field = {HW},
  data_type = {KD and DD and ED},
  isbn = {1479969230},
}

@inproceedings{Ruszkowski2015,
  author = {Ruszkowski, Angelica and Quek, Zhan Fan and Okamura, Allison and Salcudean, Septimiu},
  title = {Simulink{\textregistered} to C++ interface for controller development on the da Vinci{\textregistered} Research Kit (dVRK)},
  booktitle = {ICRA Workshop on Shared Frameworks for Medical Robotics Research},
  year = {2015},
  month = {May},
}

@inproceedings{Nisky2015,
  author = {Nisky, Ilana and Che, Yuhang and Quek, Zhan Fan and Weber, Matthew and Hsieh, Michael H and Okamura, Allison M},
  title = {Teleoperated versus open needle driving: Kinematic analysis of experienced surgeons and novice users},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d},
  research_field = {TR},
  data_type = {KD and DD and SD and ED},
  isbn = {1479969230},
}

@inproceedings{Anooshahpour2015a,
  author = {Anooshahpour, Farshad and Polushin, Ilia G. and Patel, Rajni V.},
  title = {Tissue compliance determination using a da Vinci instrument},
  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2015},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2015.7139945},
  semanticscholar = {https://www.semanticscholar.org/paper/8634b048d81cee79e92c911c19225d13fb0c6506},
  research_field = {},
  data_type = {},
  isbn = {978-1-4799-6923-4},
}

@article{Schneider2015,
  author = {Schneider, Caitlin and Nguan, Christopher and Rohling, Robert and Salcudean, Septimiu},
  title = {Tracked “pick-up” ultrasound for robot-assisted minimally invasive surgery},
  journal = {IEEE Transactions on Biomedical Engineering},
  year = {2015},
  volume = {63},
  number = {2},
  publisher = {IEEE},
  doi = {10.1109/TBME.2015.2453173},
  semanticscholar = {https://www.semanticscholar.org/paper/119345923301d08bfccdbcb1d7ebf8396bf8a4a5},
  research_field = {},
  data_type = {},
  dvrk_site = {UBC},
  issn = {0018-9294},
}

@inproceedings{Kazanzides2014,
  author = {Kazanzides, Peter and Chen, Zihan and Deguet, Anton and Fischer, Gregory S. and Taylor, Russell H. and DiMaio, Simon P.},
  title = {An Open-Source Research Kit for the da Vinci (R) Surgical System},
  booktitle = {IEEE Intl. Conf. on Robotics and Auto. (ICRA)},
  year = {2014},
  pages = {6434-6439},
  doi = {10.1109/ICRA.2014.6907809},
  semanticscholar = {https://www.semanticscholar.org/paper/014da00e522ee5a5391a615d855287fb2d4a8f54},
  keywords = {dvrk},
  address = {Hong Kong, China},
  date = {2014-06-01},
  pubstate = {published},
  tppubtype = {inproceedings},
}

@inproceedings{Kazanzides2014b,
  author = {Kazanzides, Peter and Chen, Zihan and Deguet, Anton and Fischer, Gregory S and Taylor, Russell H and DiMaio, Simon P},
  title = {An open-source research kit for the da Vinci{\textregistered} Surgical System},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2014},
  publisher = {IEEE},
  doi = {10.1109/icra18334.2014},
  semanticscholar = {https://www.semanticscholar.org/paper/5a6b873ae7d4b9b3dcc95841a58761b076082a3b},
  research_field = {HW},
  data_type = {RI and KD and DD and SD},
  isbn = {1479936855},
}

@inproceedings{Kehoe2014b,
  author = {Kehoe, Ben and Kahn, Gregory and Mahler, Jeffrey and Kim, Jonathan and Lee, Alex and Lee, Anna and Nakagawa, Keisuke and Patil, Sachin and Boyd, W. Douglas and Abbeel, Pieter and Goldberg, Ken},
  title = {Autonomous multilateral debridement with the Raven surgical robot},
  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  year = {2014},
  publisher = {IEEE},
  doi = {10.1109/ICRA.2014.6907040},
  semanticscholar = {https://www.semanticscholar.org/paper/f3f45d9d37d0e2efbc97ce862a70bba1cbc6c4e6},
  research_field = {AU},
  data_type = {RI and KD},
  isbn = {978-1-4799-3685-4},
}

@inproceedings{Mohareri2014,
  author = {Mohareri, Omid and Schneider, Caitlin and Salcudean, Septimiu},
  title = {Bimanual telerobotic surgery with asymmetric force feedback: a daVinci{\textregistered} surgical system implementation},
  booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year = {2014},
  publisher = {IEEE},
  doi = {10.1109/iros20755.2014},
  semanticscholar = {https://www.semanticscholar.org/paper/267cb7ecfc9fb0f5380aa74cadac4ed608461810},
  research_field = {},
  data_type = {},
  isbn = {1479969346},
}

@inproceedings{Mahler2014b,
  author = {Mahler, Jeffrey and Krishnan, Sanjay and Laskey, Michael and Sen, Siddarth and Murali, Adithyavairavan and Kehoe, Ben and Patil, Sachin and Wang, Jiannan and Franklin, Mike and Abbeel, Pieter and Goldberg, Ken},
  title = {Learning accurate kinematic control of cable-driven surgical robots using data cleaning and Gaussian Process Regression},
  booktitle = {2014 IEEE International Conference on Automation Science and Engineering (CASE)},
  year = {2014},
  publisher = {IEEE},
  doi = {10.1109/CoASE.2014.6899377},
  semanticscholar = {https://www.semanticscholar.org/paper/3fe1910395e97a5ea36b64f3372155c9a6c8a119},
  research_field = {AU},
  data_type = {RI and KD},
  isbn = {978-1-4799-5283-0},
}

@inproceedings{Chen2014a,
  author = {Chen, Zihan and Kazanzides, Peter},
  title = {Multi-kilohertz control of multiple robots via IEEE-1394 (firewire)},
  booktitle = {2014 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)},
  year = {2014},
  publisher = {IEEE},
  doi = {10.1109/TePRA.2014.6869144},
  semanticscholar = {https://www.semanticscholar.org/paper/cb7c11568f1337b6cbb35a5963df5e76d1050ace},
  research_field = {HW},
  data_type = {KD and DD and SD},
  isbn = {978-1-4799-4605-1},
}

@article{Lee2014,
  author = {Lee, Chiwon and Park, Woo Jung and Kim, Myungjoon and Noh, Seungwoo and Yoon, Chiyul and Lee, Choonghee and Kim, Youdan and Kim, Hyeon Hoe and Kim, Hee Chan and Kim, Sungwan},
  title = {Pneumatic-type surgical robot end-effector for laparoscopic surgical-operation-by-wire},
  journal = {Biomedical engineering online},
  year = {2014},
  volume = {13},
  number = {1},
  publisher = {Springer},
  doi = {10.1186/1475-925X-13-130},
  semanticscholar = {https://www.semanticscholar.org/paper/25426d85be595c91ae1e2fa5d737eb2f024001a6},
  research_field = {},
  data_type = {},
  dvrk_site = {SNU},
  abstract = {BackgroundAlthough minimally invasive surgery (MIS) affords several advantages compared to conventional open surgery, robotic MIS systems still have many limitations. One of the limitations is the non-uniform gripping force due to mechanical strings of the existing systems. To overcome this limitation, a surgical instrument with a pneumatic gripping system consisting of a compressor, catheter balloon, micro motor, and other parts is developed.MethodThis study aims to implement a surgical instrument with a pneumatic gripping system and pitching/yawing joints using micro motors and without mechanical strings based on the surgical-operation-by-wire (SOBW) concept. A 6-axis external arm for increasing degrees of freedom (DOFs) is integrated with the surgical instrument using LabVIEW® for laparoscopic procedures. The gripping force is measured over a wide range of pressures and compared with the simulated ideal step function. Furthermore, a kinematic analysis is conducted. To validate and evaluate the system’s clinical applicability, a simple peg task experiment and workspace identification experiment are performed with five novice volunteers using the fundamentals of laparoscopic surgery (FLS) board kit. The master interface of the proposed system employs the hands-on-throttle-and-stick (HOTAS) controller used in aerospace engineering. To develop an improved HOTAS (iHOTAS) controller, 6-axis force/torque sensor was integrated in the special housing.ResultsThe mean gripping force (after 1,000 repetitions) at a pressure of 0.3 MPa was measured to be 5.8 N. The reaction time was found to be 0.4 s, which is almost real-time. All novice volunteers could complete the simple peg task within a mean time of 176 s, and none of them exceeded the 300 s cut-off time. The system’s workspace was calculated to be 11,157.0 cm3.ConclusionsThe proposed pneumatic gripping system provides a force consistent with that of other robotic MIS systems. It provides near real-time control. It is more durable than the existing other surgical robot systems. Its workspace is sufficient for clinical surgery. Therefore, the proposed system is expected to be widely used for laparoscopic robotic surgery. This research using iHOTAS will be applied to the tactile force feedback system for surgeon’s safe operation.},
  issn = {1475-925X},
}

@inproceedings{Chen2013,
  author = {Chen, Zihan and Deguet, Anton and Taylor, Russell and DiMaio, Simon and Fischer, Gregory and Kazanzides, Peter},
  title = {An open-source hardware and software platform for telesurgical robotics research},
  booktitle = {Proceedings of the MICCAI Workshop on Systems and Architecture for Computer Assisted Interventions, Nagoya, Japan},
  year = {2013},
  doi = {10.54294/2dcog6},
  semanticscholar = {https://www.semanticscholar.org/paper/267a6f98764949a2aa0feebd4fb00bd98441851d},
  research_field = {HW},
  data_type = {RI and KD and DD and SD},
  abstract = {We present our work to develop a telerobotics research platform that provides complete access to all levels of control via open-source custom electronics and software. The electronics employs an FPGA to enable a centralized computation and distributed I/O architecture in which all control computations are implemented in a familiar development environment (Linux PC) and low-latency I/O is performed over an IEEE-1394a (Firewire) bus at speeds up to 400 Mbits/sec. The mechanical components of the system are provided by the Research Kit for the da Vinci System, which consists of the Master Tool Manipulators (MTMs), Patient Side Manipulators (PSMs), and stereo console of the first-generation da Vinci surgical robot. This system is currently installed, or planned for installation, at 11 research institutions, with additional installations likely in the future, thereby creating a research community around a common open-source hardware and software platform.},
}

@inproceedings{Xia2013,
  author = {Xia, T. and Leonard, S. and Kandaswamy, I. and Blank, A. and Whitcomb, L.L. and Kazanzides, P.},
  title = {Model-Based Telerobotic Control with Virtual Fixtures For Satellite Servicing Tasks},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  year = {2013},
  pages = {1479-1484},
  doi = {10.1109/ICRA.2013.6630766},
  semanticscholar = {https://www.semanticscholar.org/paper/5bcf1f1d5f29517017d40a241eb6684275cc3229},
  keywords = {dvrk, space},
  address = {Karlsruhe, Germany},
  date = {2013-05-01},
  pubstate = {published},
  tppubtype = {inproceedings},
}

@inproceedings{Xia2012,
  author = {Xia, Tian and Leonard, Simon and Deguet, Anton and Whitcomb, Louis L. and Kazanzides, Peter},
  title = {Augmented Reality Environment with Virtual Fixtures for Robotic Telemanipulation in Space},
  booktitle = {IEEE/RSJ Intl. Conf. on Intell. Robots and Systems (IROS)},
  year = {2012},
  pages = {5059-5064},
  doi = {10.1109/IROS.2012.6386169},
  semanticscholar = {https://www.semanticscholar.org/paper/dd21a9a3166557a4bf5aae14d636d02d996c427d},
  keywords = {dvrk, space},
  address = {Vilamoura, Portugal},
  date = {2012-10-01},
  pubstate = {published},
  tppubtype = {inproceedings},
}

@inproceedings{Xia2011,
  author = {Xia, Tian and Kapoor, Ankur and Kazanzides, Peter and H., Taylor Russell},
  title = {A Constrained Optimization Approach to Virtual Fixtures for Multi-Robot Collaborative Teleoperation},
  booktitle = {IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)},
  year = {2011},
  pages = {639-644},
  doi = {10.1109/IROS.2011.6095056},
  semanticscholar = {https://www.semanticscholar.org/paper/12654cc0342fa3c70df33417d93dc7a6a87f31a2},
  keywords = {dvrk},
  address = {San Francisco, CA},
  date = {2011-09-01},
  pubstate = {published},
  tppubtype = {inproceedings},
}
