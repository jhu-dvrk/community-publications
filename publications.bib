@INPROCEEDINGS{iovene2025situation,
  author={Iovene, Elisa and Lev, Hanna Kossowsky and Sharon, Yarden and Netz, Uri and Geftler, Alex and Ferrigno, Giancarlo and De Momi, Elena and Nisky, Ilana},
  booktitle={2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  title={A Situation-Aware Autonomous Camera Alignment for Enhanced Suturing in Robot-Assisted Minimally Invasive Surgery},
  year={2025},
  volume={},
  number={},
  pages={2126-2133},
research_field={AU and TR and SS},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@INPROCEEDINGS{Xu2025,
  author={Xu, Keshuai and Wu, Jie Ying and Deguet, Anton and Kazanzides, Peter},

  booktitle={2025 International Symposium on Medical Robotics (ISMR)},
  title={dVRK-Si: The Next Generation da Vinci Research Kit},
  year={2025},
  volume={},
  number={},
  pages={185-191},
  keywords={Medical robotics;Systems architecture;Arms;Pulse width modulation;Robot sensing systems;Motors;Manipulators;Software;Interoperability;Microprogramming},
  doi={10.1109/ISMR67322.2025.11025986},
  ieeexplore = {https://ieeexplore.ieee.org/document/11025986},
  research_field={HW},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/7db7f0ed9019634e439e2727debffb70105c3b28},
  abstract = {The da Vinci Research Kit (dVRK) is an opensource and open-hardware surgical robotics research platform that provides complete access to all levels of control on decommissioned da Vinci Surgical Systems. The original dVRK, released to the research community in 2012 and now installed at more than 40 institutions worldwide, is based on the first-generation da Vinci system released in 2000. In this work, we present dVRK-Si, an update that extends support to decommissioned second/third generation da Vinci S/Si patientside robots. We describe extensions to the system architecture that maintain compatibility and interoperability with the original dVRK system, thereby enabling a heterogeneous mix of dVRK and dVRK-Si arms in the same system. Additionally, we describe the following components specific to the dVRK-Si: (1) a new controller that features 10 channels of PWM motor drivers with digital current loops; (2) alternative (closed-source) firmwares for embedded devices in the robot arm that stream serialized sensor data using an open protocol; and (3) electronics to support the passive Setup Joints included in full systems. All electronics designs, software, and firmware (except as noted above) are provided open source to the community.}
}

@inproceedings{WangJ2025ISMR,
  title = {A Digital Twin for Telesurgery under Intermittent Communication},
  author = {Wang, Junxiang and Barragan, Juan Antonio and Ishida, Hisashi and Guo, Jingkai and Ku, Yu-Chun and Kazanzides, Peter},

  year = {2025},
  date = {2025-05-01},
  urldate = {2025-05-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/52ac37919a99926e944a05214a0674330298199c},
  doi = {10.1109/ISMR67322.2025.11025988},
  abstract = {Telesurgery is an effective way to deliver service from expert surgeons to areas without immediate access to specialized resources. However, many of these areas, such as rural districts or battlefields, might be subject to different problems in communication, especially latency and intermittent periods of communication outage. This challenge motivates the use of a digital twin for the surgical system, where a simulation would mirror the robot hardware and surgical environment in the real world. The surgeon would then be able to interact with the digital twin during communication outage, followed by a recovery strategy on the real robot upon reestablishing communication. This paper builds the digital twin for the da Vinci surgical robot, with a buffering and replay strategy that reduces the mean task completion time by 23 % when compared to the baseline, for a peg transfer task subject to intermittent communication outage. The relevant code can be found here: https://github.com/LCSR-CIIS/dvrk_digital_twin_teleoperation.}
}

@inproceedings{WangS2025ISMR,
  title = {An Augmented Reality Measurement Tool for the da Vinci Research Kit},
  author = {Wang, Shiyue and Wang, Jing-Fen and Yubin, Koh and Zhou, Haoying and Deguet, Anton and Kazanzides, Peter},

  year = {2025},
  date = {2025-05-01},
  urldate = {2025-05-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/6fde89f2ef30ccbc62568d0088018096d9b18b3e},
  doi = {10.1109/ISMR67322.2025.11025981},
  abstract = {We present an augmented reality surgical robotics console for the da Vinci Research Kit (dVRK) that integrates a real-time distance measurement tool to enhance spatial awareness during robotic-assisted surgical procedures through an interactive system. The system integrates an RViz-based interface with ROS topics, enabling virtual cursor visualization and interactive controls for precise measurements on tissue or phantom models. It supports two distinct measurement approaches: (1) a virtual measurement capability, where the operator uses the two Master Tool Manipulators (MTMs) to drive virtual 3D measurement cursors in the stereo display (a “masters as mice” mode), and (2) a physical measurement tool, where the user teleoperates one or more Patient Side Manipulators (PSMs) to touch two features and report the intervening distance. We conduct a user study on virtual (MTM) and teleoperation (PSM) modes to assess performance and accuracy and gather user feedback. The results show a Root Mean Square Error (RMSE) of 2.19 mm to 7.25 mm across different modes and tasks. This work underscores the importance of such systems for enhancing accuracy and usability in endoscopic surgery, where spatial awareness and intraoperative assessment are critical. Code is available at https://github.com/capricieuxV/RVinci_ISMR2025.git.}
}

@article{Yang2025TMRB,
  title = {An Effectiveness Study Across Baseline and Learning-Based Force Estimation Methods on the da Vinci Research Kit Si System},
  author = {Yang, Hao and Acar, Ayberk and Xu, Keshuai and Deguet, Anton and Kazanzides, Peter and Wu, Jie Ying},

  doi = {10.1109/TMRB.2025.3589744},
  year = {2025},
  date = {2025-11-01},
  urldate = {2025-01-01},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  volume = {7},
  number = {4},
  pages = {1393-1397},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/0b3a9084602035b6f30d0e4e47bc446c79908476},
  abstract = {Robot-assisted minimally invasive surgery, such as through the da Vinci systems, improves precision and patient outcomes. However, da Vinci systems prior to da Vinci 5 lacked direct force-sensing capabilities, forcing surgeons to operate without the haptic feedback they get through laparoscopy. Our prior work restored force sensing through machine learning-based force estimation for an open-source surgical robotics research platform, the da Vinci Research Kit (dVRK) Classic. This study extends our previous method to the newer dVRK system, the dVRK-Si. Additionally, we benchmark the performance of the learning-based algorithm against baseline methods (which make simplifying assumptions on the torque) to study how the two systems differ. In both systems, the learning-based method outperforms baselines, but the difference is much larger in the dVRK-Si. Nonetheless, dVRK-Si force estimation accuracy lags behind the dVRK Classic, with Root Mean Square Error (RMSE) 2 to 3 times higher. Further analysis reveals suboptimal PID control in the dVRK-Si. We hypothesize that this is because, unlike the dVRK Classic, the dVRK-Si is not mechanically balanced and exhibits more complex internal dynamics. This study advances the understanding of learning-based force estimation and is the first work to implement learning-based dynamics estimation of the new dVRK-Si system.}
}

@Article{Haworth2025SutureBotAP,
 author = {Jesse Haworth and Juo-Tung Chen and Nigel Nelson and Ji Woong Kim and Masoud Moghani and Chelsea Finn and Axel Krieger},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing},
 volume = {abs/2510.20965},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/c08b0a3ce17b970bd6ace500c8455277ac9997dc},
  doi = {10.48550/arXiv.2212.07286},
  abstract = {The research content hosted by arXiv is not fully accessible to everyone due to disabilities and other barriers. This matters because a significant proportion of people have reading and visual disabilities, it is important to our community that arXiv is as open as possible, and if science is to advance, we need wide and diverse participation. In addition, we have mandates to become accessible, and accessible content benefits everyone. In this paper, we will describe the accessibility problems with research, review current mitigations (and explain why they aren't sufficient), and share the results of our user research with scientists and accessibility experts. Finally, we will present arXiv's proposed next step towards more open science: offering HTML alongside existing PDF and TeX formats. An accessible HTML version of this paper is also available at https://info.arxiv.org/about/accessibility_research_report.html},
}

@Article{Thompson2025EarlyFD,
 author = {Jordan Thompson and Ronald Koe and Anthony Le and Gabriella Goodman and Daniel S. Brown and Alan Kuntz},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Early Failure Detection in Autonomous Surgical Soft-Tissue Manipulation via Uncertainty Quantification},
 volume = {abs/2501.10561},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/c08b0a3ce17b970bd6ace500c8455277ac9997dc},
  doi = {10.48550/arXiv.2212.07286},
  abstract = {The research content hosted by arXiv is not fully accessible to everyone due to disabilities and other barriers. This matters because a significant proportion of people have reading and visual disabilities, it is important to our community that arXiv is as open as possible, and if science is to advance, we need wide and diverse participation. In addition, we have mandates to become accessible, and accessible content benefits everyone. In this paper, we will describe the accessibility problems with research, review current mitigations (and explain why they aren't sufficient), and share the results of our user research with scientists and accessibility experts. Finally, we will present arXiv's proposed next step towards more open science: offering HTML alongside existing PDF and TeX formats. An accessible HTML version of this paper is also available at https://info.arxiv.org/about/accessibility_research_report.html},
}

@Article{Abdelaal2025ForceAwareAR,
 author = {A. Abdelaal and Jiaying Fang and Tim N. Reinhart and Jacob A. Mejia and Tony Z. Zhao and Jeannette Bohg and Allison M. Okamura},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Force-Aware Autonomous Robotic Surgery},
 volume = {abs/2501.11742},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/c08b0a3ce17b970bd6ace500c8455277ac9997dc},
  doi = {10.48550/arXiv.2212.07286},
  abstract = {The research content hosted by arXiv is not fully accessible to everyone due to disabilities and other barriers. This matters because a significant proportion of people have reading and visual disabilities, it is important to our community that arXiv is as open as possible, and if science is to advance, we need wide and diverse participation. In addition, we have mandates to become accessible, and accessible content benefits everyone. In this paper, we will describe the accessibility problems with research, review current mitigations (and explain why they aren't sufficient), and share the results of our user research with scientists and accessibility experts. Finally, we will present arXiv's proposed next step towards more open science: offering HTML alongside existing PDF and TeX formats. An accessible HTML version of this paper is also available at https://info.arxiv.org/about/accessibility_research_report.html},
}

@article{Wu2025HTL,
  title = {Augmenting efficient real-time surgical instrument segmentation in video with point tracking and Segment Anything},
  author = {Wu, Zijian and Adam, Birdi and Kazanzides, Peter and Salcudean, Septimiu E.},

  doi = {10.1049/htl2.12111},
  year = {2025},
  date = {2025-01-01},
  urldate = {2025-01-01},
  journal = {Healthcare Technology Letters},
  volume = {12},
  number = {1},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/1cff596218109a2ece4a1c335e5118262dc3bfab},
  abstract = {Abstract The Segment Anything model (SAM) is a powerful vision foundation model that is revolutionizing the traditional paradigm of segmentation. Despite this, a reliance on prompting each frame and large computational cost limit its usage in robotically assisted surgery. Applications, such as augmented reality guidance, require little user intervention along with efficient inference to be usable clinically. This study addresses these limitations by adopting lightweight SAM variants to meet the efficiency requirement and employing fine‐tuning techniques to enhance their generalization in surgical scenes. Recent advancements in tracking any point have shown promising results in both accuracy and efficiency, particularly when points are occluded or leave the field of view. Inspired by this progress, a novel framework is presented that combines an online point tracker with a lightweight SAM model that is fine‐tuned for surgical instrument segmentation. Sparse points within the region of interest are tracked and used to prompt SAM throughout the video sequence, providing temporal consistency. The quantitative results surpass the state‐of‐the‐art semi‐supervised video object segmentation method XMem on the EndoVis 2015 dataset with 84.8 IoU and 91.0 Dice. The method achieves promising performance that is comparable to XMem and transformer‐based fully supervised segmentation methods on ex vivo UCL dVRK and in vivo CholecSeg8k datasets. In addition, the proposed method shows promising zero‐shot generalization ability on the label‐free STIR dataset. In terms of efficiency, the method was tested on a single GeForce RTX 4060/4090 GPU respectively, achieving an over 25/90 FPS inference speed. Code is available at: https://github.com/zijianwu1231/SIS‐PT‐SAM.},
}

@article{sharon2025augmenting,
  title={Augmenting Robot-Assisted Pattern Cutting with Periodic Perturbations-Can We Make Dry Lab Training More Realistic?},
  author={Sharon, Yarden and Nevo, Tifferet and Naftalovich, Daniel and Bahar, Lidor and Refaely, Yael and Nisky, Ilana},
  journal={IEEE Transactions on Biomedical Engineering},
  year={2025},
  volume={72},
  number={1},
  pages={264-275},
  publisher={IEEE},
  research_field={TR},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/aeffcb6913486171744cd0bf91635a5032a15476},
  doi = {10.1109/TBME.2024.3450702},
  abstract = {Objective: Teleoperated robot-assisted minimally-invasive surgery (RAMIS) offers many advan tages over open surgery, but RAMIS training still requires optimization. Existing motor learning theories could improve RAMIS training. However, there is a gap between current knowledge based on simple movements and training approaches required for the more complicated work of RAMIS surgeons. Here, we studied how surgeons cope with time-dependent perturbations. Methods: We used the da Vinci Research Kit and investigated the effect of time-dependent force and motion perturbations on learning a circular pattern-cutting surgical task. Fifty-four participants were assigned to two experiments, with two groups for each: a control group trained without perturbations and an experimental group trained with 1 Hz perturbations. In the first experiment, force perturbations alternatingly pushed participants' hands inwards and outwards in the radial direction. In the second experiment, the perturbation constituted a periodic up-and-down motion of the task platform. Results: Participants trained with perturbations learned how to overcome them and improve their performances during training without impairing them after the perturbations were removed. Moreover, training with motion perturbations provided participants with an advantage when encountering the same or other perturbations after training, compared to training without perturbations. Conclusion: Periodic perturbations can enhance RAMIS training without impeding the learning of the perturbed task. Significance: Our results demonstrate that using challenging training tasks that include perturbations can better prepare surgical trainees for the dynamic environment they will face with patients in the operating room.},
}

@misc{zbinden2025cosmossurgdvrk,
  title={Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning},
  author={Lukas Zbinden and Nigel Nelson and Juo-Tung Chen and Xinhao Chen and Ji Woong Kim and Mahdi Azizian and Axel Krieger and Sean Huver},
  year={2025},
  eprint={2510.16240},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2510.16240},
  semanticscholar = {https://www.semanticscholar.org/paper/a181f98f8d6f15acf8edf32d6c105e22b8dd27ce},
  doi = {10.48550/arXiv.2510.16240},
  abstract = {The rise of surgical robots and vision-language-action models has accelerated the development of autonomous surgical policies and efficient assessment strategies. However, evaluating these policies directly on physical robotic platforms such as the da Vinci Research Kit (dVRK) remains hindered by high costs, time demands, reproducibility challenges, and variability in execution. World foundation models (WFM) for physical AI offer a transformative approach to simulate complex real-world surgical tasks, such as soft tissue deformation, with high fidelity. This work introduces Cosmos-Surg-dVRK, a surgical finetune of the Cosmos WFM, which, together with a trained video classifier, enables fully automated online evaluation and benchmarking of surgical policies. We evaluate Cosmos-Surg-dVRK using two distinct surgical datasets. On tabletop suture pad tasks, the automated pipeline achieves strong correlation between online rollouts in Cosmos-Surg-dVRK and policy outcomes on the real dVRK Si platform, as well as good agreement between human labelers and the V-JEPA 2-derived video classifier. Additionally, preliminary experiments with ex-vivo porcine cholecystectomy tasks in Cosmos-Surg-dVRK demonstrate promising alignment with real-world evaluations, highlighting the platform's potential for more complex surgical procedures.}
}

@article{sharon2025dataset,
  title={Dataset and Analysis of Long-Term Skill Acquisition in Robot-Assisted Minimally Invasive Surgery},
  author={Sharon, Yarden and Geftler, Alex, and Kossowsky Lev, Hanna and Nisky, Ilana},
  journal={IEEE Transactions on Medical Robotics and Bionics},
  volume={7},
  number={4},
  pages={1513-1524},
  year={2025},
  publisher={IEEE},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/c7e352245078f1429e419c17d040a346e931cbce},
  doi = {10.1109/TMRB.2025.3617958},
  abstract = {Objective: We aim to investigate long-term robotic surgical skill acquisition among surgical residents and the effects of training intervals and fatigue on performance. Methods: For six months, surgical residents participated in three training sessions once a month, surrounding a single 26-hour hospital shift. In each shift, they participated in training sessions scheduled before, during, and after the shift. In each training session, they performed three dry-lab training tasks: Ring Tower Transfer, Knot-Tying, and Suturing. We collected a comprehensive dataset, including videos synchronized with kinematic data, activity tracking, and scans of the suturing pads. Results: We are releasing the dataset resulting in 972 trials performed by 18 residents of different surgical specializations. Participants demonstrated consistent performance improvement across all tasks. In addition, we found variations in between-shift learning and forgetting across metrics and tasks, and hints for possible effects of fatigue. Conclusion: The findings from our first analysis shed light on the long-term learning processes of robotic surgical skills with extended intervals and varying levels of fatigue. Significance: This study lays the groundwork for future research aimed at optimizing training protocols and enhancing AI applications in surgery, ultimately contributing to improved patient outcomes.}
}

@Article{Zhou2025GravityCO,
 author = {Haoying Zhou and Hao Yang and A. Deguet and L. Fichera and Jie Ying Wu and Peter Kazanzides},
 booktitle = {HSMR 2025},
 journal = {ArXiv},
 title = {Gravity Compensation of the dVRK-Si Patient Side Manipulator based on Dynamic Model Identification},
 volume = {abs/2501.19058},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/1fce8afc1c245f75f17530ef154c7a1d03d6d7ae},
  doi = {10.31256/hsmr25.59},
  abstract = {Rapid and reliable vascular access is critical in trauma and critical care. Central vascular catheterization enables high-volume resuscitation, hemodynamic monitoring, and advanced interventions like ECMO and REBOA. While peripheral access is common, central access is often necessary but requires specialized ultrasound-guided skills, posing challenges in prehospital settings. The complexity arises from deep target vessels and the precision needed for needle placement. Traditional techniques, like the Seldinger method, demand expertise to avoid complications. Despite its importance, ultrasound-guided central access is underutilized due to limited field expertise. While autonomous needle insertion has been explored for peripheral vessels, only semi-autonomous methods exist for femoral access. This work advances toward full automation, integrating robotic ultrasound for minimally invasive emergency procedures. Our key contribution is the successful femoral vein and artery cannulation in a porcine hemorrhagic shock model.}
}

@Article{Zhang2025HybridDR,
 author = {Hanyi Zhang and Kaizhong Deng and Zhaoyang Jacopo Hu and Baoru Huang and Daniel S. Elson},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2025 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {15465-15471},
 title = {Hybrid Deep Reinforcement Learning for Radio Tracer Localisation in Robotic-Assisted Radioguided Surgery},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Vuong2025EffectsOW,
 author = {Brian B. Vuong and Josie Davidson and Sangheui Cheon and Kyu-Jin Cho and Allison M. Okamura},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {12923-12930},
 title = {Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed During a Teleoperated Robotic Surgery Task},
 volume = {10},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Ho2025SurgIRLTL,
 author = {Yun-Jie Ho and Zih-Yun Chiu and Yuheng Zhi and Michael C. Yip},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {13145-13152},
 title = {SurgIRL: Toward Life-Long Learning for Surgical Automation by Incremental Reinforcement Learning},
 volume = {10},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Haiderbhai2025Sim2RealRC,
 author = {Mustafa Haiderbhai and R. Gondokaryono and Andrew Wu and L. Kahrs},
 booktitle = {IEEE Transactions on Automation Science and Engineering},
 journal = {IEEE Transactions on Automation Science and Engineering},
 pages = {4354-4365},
 title = {Sim2Real Rope Cutting With a Surgical Robot Using Vision-Based Reinforcement Learning},
 volume = {22},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/6edf9e85d10f3342aae626dfa68583132834edaf},
  doi = {10.1109/tase.2018.2887129},
  abstract = {IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING is published by the IEEE Robotics and Automation Society. All members of the IEEE are eligible for membership in the Society and will receive this TRANSACTIONS upon payment of the annual Society membership fee of $9.00 plus an annual subscription fee of $50.00. For information on joining, write to the lEEE Service Center at the address below. Member copies of Transactions/Journals are for personal use only.},
}

@Article{Maguire2025RoboticAC,
 author = {G. Maguire and E. Tang and T. Looi and D. Podolsky},
 booktitle = {IEEE Transactions on Biomedical Engineering},
 journal = {IEEE Transactions on Biomedical Engineering},
 pages = {2085-2094},
 title = {Robotic Assisted Cleft Palate Repair Using Novel 3 mm Tools: A Reachability and Collision Analysis},
 volume = {72},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/ba89509cbe938a3d6e2ac3b17a97fdb52bc5e0d6},
  doi = {10.1109/tbme.10},
  abstract = {Guest Editors : Miguel Nicolelis, M.D., Ph.D. Professor of Neurobiology, Biomedical Research and Experimental Psychology. Department of Neurobiology, Box 3209, Room 327E Bryan Research Bldg. Duke University Medical Center 101 Research Drive Durham, NC 27710 e-mail: nicoleli@neuro.duke.edu Neils Birbaumer, Ph.D. Professor of Behavioral Neurobiology Faculty of Medicine Inst. Medical Psychology and Behavioral Neurobiology Gartenstr. 29, D-72074, Tübingen Germany e-mail: niels.birbaumer@unituebingen.de Klaus Müeller, Ph.D. University of Potsdam and Fraunhofer Institut FIRST Intelligent Data Analysis Group (IDA) Kekulestr. 7, 12489 Berlin}
}

@Article{Liang2025DifferentiableRP,
 author = {Zekai Liang and Zih-Yun Chiu and Florian Richter and Michael C. Yip},
 booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
 journal = {2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 pages = {20898-20905},
 title = {Differentiable Rendering-based Pose Estimation for Surgical Robotic Instruments},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/dda867de88aff74f70fb9b2de0387297183afea5},
  doi = {10.1109/iros47612.2022.9981344},
  abstract = {The 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) will be held on October 23–27, 2022 in The Kyoto International Conference Center, Kyoto, Japan. The IROS is one of the largest and most impacting robotics research conferences worldwide. It provides an international forum for the international robotics research community to explore the frontier of science and technology in intelligent robots and smart machines. The theme of IROS 2022 is “Embodied AI for a Symbiotic Society’’. In addition to technical sessions and multi-media presentations, IROS conferences also hold panel discussions, forums, workshops, tutorials, exhibits, and technical tours to enrich the fruitful discussions among conference attendees.},
}

@Article{Chen2025SurgiPoseES,
 author = {Juo-Tung Chen and Xinhao Chen and Ji Woong Kim and P. M. Scheikl and R. Cha and Axel Krieger},
 booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
 journal = {2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 pages = {20912-20919},
 title = {SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning},
 year = {2025}
}

@article{meli2025inductive,
  title={Inductive learning of robot task knowledge from raw data and online expert feedback},
  author={Meli, Daniele and Fiorini, Paolo},
  journal={Machine Learning},
  volume={114},
  number={4},
  pages={91},
  year={2025},
  publisher={Springer},
  research_field={AU and SS and TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/22b923def342bf1cc007428ffb56f3de9b067613},
  doi = {10.1007/s10994-024-06636-6},
  abstract = {The increasing level of autonomy of robots poses challenges of trust and social acceptance, especially in human-robot interaction scenarios. This requires an interpretable implementation of robotic cognitive capabilities, possibly based on formal methods as logics for the definition of task specifications. However, prior knowledge is often unavailable in complex realistic scenarios. In this paper, we propose an offline algorithm based on inductive logic programming from noisy examples to extract task specifications (i.e., action preconditions, constraints and effects) directly from raw data of few heterogeneous (i.e., not repetitive) robotic executions. Our algorithm leverages on the output of any unsupervised action identification algorithm from video-kinematic recordings. Combining it with the definition of very basic, almost task-agnostic, commonsense concepts about the environment, which contribute to the interpretability of our methodology, we are able to learn logical axioms encoding preconditions of actions, as well as their effects in the event calculus paradigm. Since the quality of learned specifications depends mainly on the accuracy of the action identification algorithm, we also propose an online framework for incremental refinement of task knowledge from user’s feedback, guaranteeing safe execution. Results in a standard manipulation task and benchmark for user training in the safety-critical surgical robotic scenario, show the robustness, data- and time-efficiency of our methodology, with promising results towards the scalability in more complex domains.}
}

@Article{Abkhofte2025DeepLS,
 author = {Sara Abkhofte and A. Naidu and Rajni V. Patel and J. Jagadeesan},
 booktitle = {International Symposium on Medical Robotics},
 journal = {2025 International Symposium on Medical Robotics (ISMR)},
 pages = {30-36},
 title = {Deep Learning-Based Segmentation for Autonomous Robot-Assisted Tumor Localization using Ultrasound B-Mode and Strain Elastography Imaging},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
  doi = {10.1109/ISMR67322.2025}
}

@Article{Chen2025AccuracyAA,
 author = {Xinhao Chen and M. Kam and Nural Yilmaz and A. Deguet and Axel Krieger},
 booktitle = {J. Medical Robotics Res.},
 journal = {J. Medical Robotics Res.},
 pages = {2550001:1-2550001:11},
 title = {Accuracy Analysis and Enhancement via Transformer-based Robot Calibration of the da Vinci Research Kit Si (dVRK-Si)},
 volume = {10},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/f468bb674fac8fbbcbece1d58c89ce2fd6f1cedc},
  doi = {10.48550/arXiv.2502.18586},
  abstract = {Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.}
}

@article{caccianiga2025open,
  title={Open-Source Multi-Viewpoint Surgical Telerobotics},
  author={Caccianiga, Guido and Sharon, Yarden and Javot, Bernard and Polikovsky, Senya and Erg{\"u}n, G{\"o}kce and Capobianco, Ivan and Mihaljevic, Andr{\'e} L and Deguet, Anton and Kuchenbecker, Katherine J},

  journal={arXiv preprint arXiv:2505.11142},
  year={2025},
  url={https://arxiv.org/abs/2505.11142}
}

@article{Long2025,
  title = {Surgical embodied intelligence for generalized task autonomy in laparoscopic robot-assisted surgery},
  author = {Long, Yonghao and Lin, Anran and Kwok, Derek Hang Chun and Zhang, Lin and Yang, Zhenya and Shi, Kejian and Song, Lei and Fu, Jiawei and Lin, Hongbin and Wei, Wang and Chen, Kai and Chu, Xiangyu and Hu, Yang and Yip, Hon Chi and Chiu, Philip Wai Yan and Kazanzides, Peter and Taylor, Russell H. and Liu, Yunhui and Chen, Zihan and Wang, Zerui and Au, Samuel Kwok Wai and Dou, Qi},

  year = {2025},
  date = {2025-01-01},
  urldate = {2025-01-01},
  journal = {Science Robotics},
  volume = {10},
  number = {104},
  pages = {eadt3093},
  publisher = {American Association for the Advancement of Science},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/6de1cfdd5d12a13352d5b0926abd57faf62ef573},
  doi = {10.1126/scirobotics.adt3093},
  abstract = {Surgical robots capable of autonomously performing various tasks could enhance efficiency and augment human productivity in addressing clinical needs. Although current solutions have automated specific actions within defined contexts, they are challenging to generalize across diverse environments in general surgery. Embodied intelligence enables general-purpose robot learning with applications for daily tasks, yet its application in the medical domain remains limited. We introduced an open-source surgical embodied intelligence simulator for an interactive environment to develop reinforcement learning methods for minimally invasive surgical robots. Using such embodied artificial intelligence, this study further addresses surgical task automation, enabling zero-shot transfer of simulation-trained policies to real-world scenarios. The proposed method encompasses visual parsing, a perceptual regressor, policy learning, and a visual servoing controller, forming a paradigm that combines the advantages of data-driven policy and classic controller. The visual parsing uses stereo depth estimation and image segmentation with a visual foundation model to handle complex scenes. Experiments demonstrated autonomy in seven game-based skill training tasks on the da Vinci Research Kit, with a proof-of-concept study on haptic-assisted skill training as a practical application. Moreover, we conducted automation of five surgical assistive tasks with the Sentire surgical system on ex vivo animal tissues with various scenes, object sizes, instrument types, and illuminations. The learned policies were also validated in a live-animal trial for three tasks in dynamic in vivo surgical environments. We hope this open-source infrastructure, coupled with a general-purpose learning paradigm, will inspire and facilitate future research on embodied intelligence toward autonomous surgical robots.}
}

@Article{Khan2025DevelopmentOA,
 author = {Aimal Khan and Hao Yang and Daniel Roy Sadek Habib and Danish Ali and Jie Ying Wu},
 booktitle = {Surgical Endoscopy},
 journal = {Surgical Endoscopy},
 pages = {3422 - 3428},
 title = {Development of a machine learning-based tension measurement method in robotic surgery},
 volume = {39},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a9bddcf7ce391e9e75bf119789ba751ce2545f26},
  doi = {10.1038/s41551-023-01018-0},
  abstract = {Laryngeal lesions can be endoscopically detected with high sensitivity and in real time by measuring differences in the polarization signatures between cancerous and healthy tissues. The standard-of-care for the detection of laryngeal pathologies involves distinguishing suspicious lesions from surrounding healthy tissue via contrasts in colour and texture captured by white-light endoscopy. However, the technique is insufficiently sensitive and thus leads to unsatisfactory rates of false negatives. Here we show that laryngeal lesions can be better detected in real time by taking advantage of differences in the light-polarization properties of cancer and healthy tissues. By measuring differences in polarized-light retardance and depolarization, the technique, which we named ‘surgical polarimetric endoscopy’ (SPE), generates about one-order-of-magnitude greater contrast than white-light endoscopy, and hence allows for the better discrimination of cancerous lesions, as we show with patients diagnosed with squamous cell carcinoma. Polarimetric imaging of excised and stained slices of laryngeal tissue indicated that changes in the retardance of polarized light can be largely attributed to architectural features of the tissue. We also assessed SPE to aid routine transoral laser surgery for the removal of a cancerous lesion, indicating that SPE can complement white-light endoscopy for the detection of laryngeal cancer.},
}

@Article{Spagnulo2025FromPT,
 author = {Roberto Spagnulo and Francesco Marzola and Federica Corso and Giovanni Distefano and Matteo Pescio and Kengo Hayashi and Federica Barontini and Giulio Dagnino and K. Althoefer and Bruno Siciliano and S. Ourselin and Alberto Arezzo},
 booktitle = {Surgical Endoscopy},
 journal = {Surgical endoscopy},
 title = {From precision to strength: computer vision for suture quality assessment-an ex vivo pilot study.},
 year = {2025},
  semanticscholar = {https://www.semanticscholar.org/paper/a9bddcf7ce391e9e75bf119789ba751ce2545f26},
  doi = {10.1038/s41551-023-01018-0},
  abstract = {Laryngeal lesions can be endoscopically detected with high sensitivity and in real time by measuring differences in the polarization signatures between cancerous and healthy tissues. The standard-of-care for the detection of laryngeal pathologies involves distinguishing suspicious lesions from surrounding healthy tissue via contrasts in colour and texture captured by white-light endoscopy. However, the technique is insufficiently sensitive and thus leads to unsatisfactory rates of false negatives. Here we show that laryngeal lesions can be better detected in real time by taking advantage of differences in the light-polarization properties of cancer and healthy tissues. By measuring differences in polarized-light retardance and depolarization, the technique, which we named ‘surgical polarimetric endoscopy’ (SPE), generates about one-order-of-magnitude greater contrast than white-light endoscopy, and hence allows for the better discrimination of cancerous lesions, as we show with patients diagnosed with squamous cell carcinoma. Polarimetric imaging of excised and stained slices of laryngeal tissue indicated that changes in the retardance of polarized light can be largely attributed to architectural features of the tissue. We also assessed SPE to aid routine transoral laser surgery for the removal of a cancerous lesion, indicating that SPE can complement white-light endoscopy for the detection of laryngeal cancer.},
}

@inproceedings{Wu2025ICRA,
  title = {SurgPose: a Dataset for Articulated Robotic Surgical Tool Pose Estimation and Tracking},
  author = {Wu, Zijian and Schmidt, Adam and Moore, Randy and Zhou, Haoying and Banks, Alexandre and Kazanzides, Peter and Salcudean, Septimiu E.},

  year = {2025},
  date = {2025-05-01},
  urldate = {2025-05-01},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  address = {Atlanta, US},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings}
}

@INPROCEEDINGS{Rajarajeswari2024,
  author={Rajarajeswari, S and S, Pruthvi Darshan S and L, Jayanth M and Lohar, Hrishikesh Keraba},

  booktitle={2024 2nd International Conference on Recent Advances in Information Technology for Sustainable Development (ICRAIS)},
  title={Enhancing Surgical Training Through Immersive Simulation and Visualization of the da Vinci Research Kit (dVRK) Surgical System Using ROS 2 and Rviz},
  year={2024},
  volume={},
  number={},
  pages={118-123},
  keywords={Training;Visualization;Technological innovation;Medical robotics;Operating systems;Instruments;Surgery;Virtual environments;Maintenance;Sustainable development;dVRK;ROS2;Rviz;Simulation;Visualization},
  doi={10.1109/ICRAIS62903.2024.10811720},
  ieeexplore = {https://ieeexplore.ieee.org/document/10811720},
  semanticscholar = {https://www.semanticscholar.org/paper/60646681890889589c02e631068d6f5e28fe1eda},
  abstract = {In the modern era, robotic engineering has transformed education, training, and skill development, particularly in the field of surgery. Simulation training offers a safe and controlled environment for surgeons to learn, practice and visualize surgical techniques before performing procedures on patients. The simulation and visualization of the da Vinci Research Kit (dVRK), a platform for creating and evaluating surgical methods, algorithms, and technologies, are the main focus of this research, visualizing the interaction between a dVRK surgical system and a virtual human body using the Rviz simulator and Robot Operating System (ROS). These models are imported as meshes of stereo-lithography format (.STL), defined using xacro. The study explores the role of visualization in enhancing surgical education and training, highlighting it potential to improve procedural competency and patient outcomes. This research contributes to the advancement of surgical education through visualization by providing a realistic and immersive environment for surgical simulation training.}
}

@misc{Yang2024,
  title={A Hybrid Model and Learning-Based Force Estimation Framework for Surgical Robots},
  author={Yang, Hao and Zhou, Haoying and Fischer, Gregory S. and Wu, Jie Ying},

  year={2024},
  eprint={2409.19970},
  archivePrefix={arXiv},
  primaryClass={cs.RO},
  url={https://arxiv.org/abs/2409.19970},
  semanticscholar = {https://www.semanticscholar.org/paper/8afee642d1c86d1ec92e0f495210eae6d69085cb},
  doi = {10.1109/IROS58592.2024.10802648},
  abstract = {Haptic feedback to the surgeon during robotic surgery would enable safer and more immersive surgeries but estimating tissue interaction forces at the tips of robotically controlled surgical instruments has proven challenging. Few existing surgical robots can measure interaction forces directly and the additional sensor may limit the life of instruments. We present a hybrid model and learning-based framework for force estimation for the Patient Side Manipulators (PSM) of a da Vinci Research Kit (dVRK). The model-based component identifies the dynamic parameters of the robot and estimates free-space joint torque, while the learning-based component compensates for environmental factors, such as the additional torque caused by trocar interaction between the PSM instrument and the patient’s body wall. We evaluate our method in an abdominal phantom and achieve an error in force estimation of under 10% normalized root-mean-squared error. We show that by using a model-based method to perform dynamics identification, we reduce reliance on the training data covering the entire workspace. Although originally developed for the dVRK, the proposed method is a generalizable framework for other compliant surgical robots. The code is available at https://github.com/vu-maple-lab/dvrk_force_estimation.},
}

@inproceedings{Ou2024,
  title={A Realistic Surgical Simulator for Non-Rigid and Contact-Rich Manipulation in Surgeries with the da Vinci Research Kit},
  doi = {10.1109/UR61395.2024.10597513},
  booktitle={2024 21st International Conference on Ubiquitous Robots (UR)},
  publisher={IEEE},
  author={Ou, Yafei and Zargarzadeh, Sadra and Sedighi, Paniz and Tavakoli, Mahdi},

  year={2024},
  month=jun,
  pages={64–70},
  research_field={SS},
  semanticscholar = {https://www.semanticscholar.org/paper/2610b7ad97c66a380a4bb4934bfd46f1aa4812f6},
  abstract = {Realistic real-time surgical simulators play an increasingly important role in surgical robotics research, such as surgical robot learning and automation, and surgical skills assessment. Although there are a number of existing surgical simulators for research, they generally lack the ability to simulate the diverse types of objects and contact-rich manipulation tasks typically present in surgeries, such as tissue cutting and blood suction. In this work, we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the da Vinci Research Kit (dVRK) that enables simulating various contact-rich surgical tasks involving different surgical instruments, soft tissue, and body fluids. The real-world dVRK console and the master tool manipulator (MTM) robots are incorporated into the system to allow for teleoperation through virtual reality (VR). To showcase the advantages and potentials of the simulator, we present three examples of surgical tasks, including tissue grasping and deformation, blood suction, and tissue cutting. These tasks are performed using the simulated surgical instruments, including the large needle driver, suction irrigator, and curved scissor, through VR-based teleoperation.}
}

@Article{Rivas-Blanco2024InstrumentDA,
 author = {I. Rivas-Blanco and C. López-Casado and Juan M. Herrera-López and José Cabrera-Villa and C. Pérez-del-Pulgar},
 booktitle = {Applied Sciences},
 journal = {Applied Sciences},
 title = {Instrument Detection and Descriptive Gesture Segmentation on a Robotic Surgical Maneuvers Dataset},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/be31b693f95dbd3a12b4cb3c706c5f40eb4cb9c9},
  doi = {10.1109/ICRA40945.2020.9197436},
  abstract = {Here we present HAMR-Jr, a 22.5mm, 320mg quadrupedal microrobot. With eight independently actuated degrees of freedom, HAMR-Jr is, to our knowledge, the most mechanically dexterous legged robot at its scale and is capable of high-speed locomotion (13.91bodylengthss−1) at a variety of stride frequencies (1-200Hz) using multiple gaits. We achieved this using a design and fabrication process that is flexible, allowing scaling with minimum changes to our workflow. We further characterized HAMR-Jr’s open-loop locomotion and compared it with the larger scale HAMR-VI microrobot to demonstrate the effectiveness of scaling laws in predicting running performance.},
}

@Article{Yang2024EfficientPS,
 author = {Zhenya Yang and Yonghao Long and Kai Chen and Wang Wei and Qi Dou},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Efficient Physically-based Simulation of Soft Bodies in Embodied Environment for Surgical Robot},
 volume = {abs/2402.01181},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/c08b0a3ce17b970bd6ace500c8455277ac9997dc},
  doi = {10.48550/arXiv.2212.07286},
  abstract = {The research content hosted by arXiv is not fully accessible to everyone due to disabilities and other barriers. This matters because a significant proportion of people have reading and visual disabilities, it is important to our community that arXiv is as open as possible, and if science is to advance, we need wide and diverse participation. In addition, we have mandates to become accessible, and accessible content benefits everyone. In this paper, we will describe the accessibility problems with research, review current mitigations (and explain why they aren't sufficient), and share the results of our user research with scientists and accessibility experts. Finally, we will present arXiv's proposed next step towards more open science: offering HTML alongside existing PDF and TeX formats. An accessible HTML version of this paper is also available at https://info.arxiv.org/about/accessibility_research_report.html},
}

@Article{Oh2024ExpandedCR,
 author = {Ki Hwan Oh and Leonardo Borgioli and Alberto Mangano and Valentina Valle and Marco Di Pangrazio and F. Toti and Gioia Pozza and Luciano Ambrosini and A. Ducas and Milos Zefran and Liaohai Chen and P. Giulianotti},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {Expanded Comprehensive Robotic Cholecystectomy Dataset (CRCD)},
 volume = {abs/2412.12238},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/c08b0a3ce17b970bd6ace500c8455277ac9997dc},
  doi = {10.48550/arXiv.2212.07286},
  abstract = {The research content hosted by arXiv is not fully accessible to everyone due to disabilities and other barriers. This matters because a significant proportion of people have reading and visual disabilities, it is important to our community that arXiv is as open as possible, and if science is to advance, we need wide and diverse participation. In addition, we have mandates to become accessible, and accessible content benefits everyone. In this paper, we will describe the accessibility problems with research, review current mitigations (and explain why they aren't sufficient), and share the results of our user research with scientists and accessibility experts. Finally, we will present arXiv's proposed next step towards more open science: offering HTML alongside existing PDF and TeX formats. An accessible HTML version of this paper is also available at https://info.arxiv.org/about/accessibility_research_report.html},
}

@Article{Ho2024SurgIRLTL,
 author = {Yun-Jie Ho and Zih-Yun Chiu and Yuheng Zhi and Michael C. Yip},
 booktitle = {arXiv.org},
 journal = {ArXiv},
 title = {SurgIRL: Towards Life-Long Learning for Surgical Automation by Incremental Reinforcement Learning},
 volume = {abs/2409.15651},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/c08b0a3ce17b970bd6ace500c8455277ac9997dc},
  doi = {10.48550/arXiv.2212.07286},
  abstract = {The research content hosted by arXiv is not fully accessible to everyone due to disabilities and other barriers. This matters because a significant proportion of people have reading and visual disabilities, it is important to our community that arXiv is as open as possible, and if science is to advance, we need wide and diverse participation. In addition, we have mandates to become accessible, and accessible content benefits everyone. In this paper, we will describe the accessibility problems with research, review current mitigations (and explain why they aren't sufficient), and share the results of our user research with scientists and accessibility experts. Finally, we will present arXiv's proposed next step towards more open science: offering HTML alongside existing PDF and TeX formats. An accessible HTML version of this paper is also available at https://info.arxiv.org/about/accessibility_research_report.html},
}

@article{Yilmaz2024IJCARS,
  title = {Enhancing robotic telesurgery with sensorless haptic feedback},
  author = {Yilmaz, Nural and Burkhart, Brendan and Deguet, Anton and Kazanzides, Peter and Tumerdem, Ugur},

  doi = {10.1007/s11548-024-03117-y},
  year = {2024},
  date = {2024-06-01},
  urldate = {2024-06-01},
  journal = {Intl. Journal of Computer Assisted Radiology and Surgery (IJCARS)},
  volume = {19},
  number = {6},
  pages = {1147-1155},
  publisher = {Springer},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/aa27ac609cec644d0761cfcf1e3cfc10c315da89}
}

@ARTICLE{oquendo2024haptic,
  author={Oquendo, Yousi A. and Coad, Margaret M. and Wren, Sherry M. and Lendvay, Thomas S. and Nisky, Ilana and Jarc, Anthony M. and Okamura, Allison M. and Chua, Zonghe},
  journal={IEEE Transactions on Haptics},
  title={Haptic Guidance and Haptic Error Amplification in a Virtual Surgical Robotic Training Environment},
  year={2024},
  volume={17},
  number={3},
  pages={417-428},
  publisher={IEEE},
  research_field={TR and SS},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/4f2b40f626bd30197879bf945f9a33206d54245c},
  doi = {10.1109/TOH.2024.3350128},
  abstract = {Teleoperated robotic systems have introduced more intuitive control for minimally invasive surgery, but the optimal method for training remains unknown. Recent motor learning studies have demonstrated that exaggeration of errors helps trainees learn to perform tasks with greater speed and accuracy. We hypothesized that training in a force field that pushes the user away from a desired path would improve their performance on a virtual reality ring-on-wire task. Thirty-eight surgical novices trained under a no-force, guidance, or error-amplifying force field over five days. Completion time, translational and rotational path error, and combined error-time were evaluated under no force field on the final day. The groups significantly differed in combined error-time, with the guidance group performing the worst. Error-amplifying field participants did not plateau in their performance during training, suggesting that learning was still ongoing. Guidance field participants had the worst performance on the final day, confirming the guidance hypothesis. Observed trends also suggested that participants who had high initial path error benefited more from guidance. Error-amplifying and error-reducing haptic training for robot-assisted telesurgery benefits trainees of different abilities differently, with our results indicating that participants with high initial combined error-time benefited more from guidance and error-amplifying force field training.},
}

@Article{Wu2024AugmentingER,
 author = {Zijian Wu and Adam Schmidt and P. Kazanzides and S. Salcudean},
 booktitle = {Healthcare technology letters},
 journal = {Healthcare Technology Letters},
 title = {Augmenting efficient real‐time surgical instrument segmentation in video with point tracking and Segment Anything},
 volume = {12},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/74d76b6da473e6f140de87e1a56d3065d211491a},
  doi = {10.1049/htl.2019.0091},
  abstract = {In breast reconstruction following a single mastectomy, the surgeon needs to choose between tens of available implants to find the one that can reproduce the symmetry of the patient's breasts. However, due to the lack of measurement tools this decision is made purely visually, which means the surgeon has to order multiple implants to confirm the size for every single patient. In this Letter, the authors present an augmented reality application, which enables surgeons to see the shape of the implants, as 3D holograms on the patient's body. They custom developed a two-chamber implant that can gain different shapes and be used to test the system. Furthermore, the system was tested in a user study with 13 subjects. The study showed that subjects were able to do a comparison between real and holographic implants and come to a decision about which should be used. This method can be quicker than the traditional way and eliminates sizer implants from the process. Further advantages of the method include the use of a more accurate, user-friendly device, which is easily extendable as new implants that are on the market can be easily added to the system dataset.},
}

@Article{Zheng2024AUS,
 author = {Haoyi Zheng and Zhaoyang Jacopo Hu and Yanpei Huang and Xiaoxiao Cheng and Ziwei Wang and Etienne Burdet},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {15195-15201},
 title = {A User-Centered Shared Control Scheme with Learning from Demonstration for Robotic Surgery},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Strohmeyer2024ASD,
 author = {Nicholas A. Strohmeyer and Ji Hwan Park and Braden P. Murphy and Farshid Alambeigi},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {9881-9886},
 title = {A Semi-Autonomous Data-Driven Shared Control Framework for Robotic Manipulation and Cutting of an Unknown Deformable Tissue},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Fu2024MultiobjectiveCL,
 author = {Jiawei Fu and Yonghao Long and Kai Chen and Wang Wei and Qi Dou},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2024 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {13362-13368},
 title = {Multi-objective Cross-task Learning via Goal-conditioned GPT-based Decision Transformers for Surgical Robot Task Automation},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Borgioli2024SensoryGS,
 author = {Leonardo Borgioli and Ki Hwan Oh and Alberto Mangano and A. Ducas and Luciano Ambrosini and Federico Pinto and Paula Lopez and Jessica Cassiani and Milos Zefran and Liaohai Chen and P. Giulianotti},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2025 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {10487-10493},
 title = {Sensory Glove-Based Surgical Robot User Interface},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Li2024RealTimeGJ,
 author = {Bin Li and Hongbin Lin and Fangxun Zhong and Yunhui Liu},
 booktitle = {IEEE International Conference on Robotics and Biomimetics},
 journal = {2024 IEEE International Conference on Robotics and Biomimetics (ROBIO)},
 pages = {2144-2148},
 title = {Real-Time Geometric Joint Uncertainty Tracking for Surgical Automation on the dVRK System},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/7bafb41c9eea715c1aa6b0b0b0fb93a2a1f50b50}
}

@Article{Li2024GMMBasedHD,
 author = {Bin Li and Yiang Lu and Wei Chen and Bo Lu and Fangxun Zhong and Qi Dou and Yunhui Liu},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {1969-1976},
 title = {GMM-Based Heuristic Decision Framework for Safe Automated Laparoscope Control},
 volume = {9},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Marra2024MPCFS,
 author = {Pasquale Marra and Sajjad Hussain and Marco Caianiello and Fanny Ficuciello},
 booktitle = {IEEE Transactions on Medical Robotics and Bionics},
 journal = {IEEE Transactions on Medical Robotics and Bionics},
 pages = {1468-1477},
 title = {MPC for Suturing Stitch Automation},
 volume = {6},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/2529a49e5e437f7168ef3d526e2f60a9690a6cae},
  doi = {10.1109/tmrb.2021.3054138},
}

@Article{Argin2024daVinciRK,
 author = {O. F. Argin and R. Moccia and Cristina Iacono and Fanny Ficuciello},
 booktitle = {IEEE Transactions on Medical Robotics and Bionics},
 journal = {IEEE Transactions on Medical Robotics and Bionics},
 pages = {589-599},
 title = {daVinci Research Kit Patient Side Manipulator Dynamic Model Using Augmented Lagrangian Particle Swarm Optimization},
 volume = {6},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/2529a49e5e437f7168ef3d526e2f60a9690a6cae},
  doi = {10.1109/tmrb.2021.3054138},
}

@Article{Haiderbhai2024SimulatingSR,
 author = {Mustafa Haiderbhai and L. Kahrs},
 booktitle = {IEEE Transactions on Medical Robotics and Bionics},
 journal = {IEEE Transactions on Medical Robotics and Bionics},
 pages = {1401-1404},
 title = {Simulating Surgical Robot Cutting of Thin Deformable Materials Using a Rope Grid Structure},
 volume = {6},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/2529a49e5e437f7168ef3d526e2f60a9690a6cae},
  doi = {10.1109/tmrb.2021.3054138},
}

@Article{Li2024OnTM,
 author = {Bin Li and Bo Lu and Hongbin Lin and Yaxiang Wang and Fangxun Zhong and Qi Dou and Yun-hui Liu},
 booktitle = {IEEE Transactions on Medical Robotics and Bionics},
 journal = {IEEE Transactions on Medical Robotics and Bionics},
 pages = {460-474},
 title = {On the Monocular 3-D Pose Estimation for Arbitrary Shaped Needle in Dynamic Scenes: An Efficient Visual Learning and Geometry Modeling Approach},
 volume = {6},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/2529a49e5e437f7168ef3d526e2f60a9690a6cae},
  doi = {10.1109/tmrb.2021.3054138},
}

@Article{Shinde2024SURESTEPAU,
 author = {N. Shinde and Zih-Yun Chiu and Florian Richter and Jason Lim and Yuheng Zhi and Sylvia L. Herbert and Michael C. Yip},
 booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
 journal = {2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 pages = {6953-6960},
 title = {SURESTEP: An Uncertainty-Aware Trajectory Optimization Framework to Enhance Visual Tool Tracking for Robust Surgical Automation},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/dda867de88aff74f70fb9b2de0387297183afea5},
  doi = {10.1109/iros47612.2022.9981344},
  abstract = {The 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) will be held on October 23–27, 2022 in The Kyoto International Conference Center, Kyoto, Japan. The IROS is one of the largest and most impacting robotics research conferences worldwide. It provides an international forum for the international robotics research community to explore the frontier of science and technology in intelligent robots and smart machines. The theme of IROS 2022 is “Embodied AI for a Symbiotic Society’’. In addition to technical sessions and multi-media presentations, IROS conferences also hold panel discussions, forums, workshops, tutorials, exhibits, and technical tours to enrich the fruitful discussions among conference attendees.},
}

@inproceedings{rota2024implementation,
  title={Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics},
  author={Rota, Alberto and Fan, Ke and De Momi, Elena},

  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9666--9673},
  year={2024},
  organization={IEEE},
  research_field={TR},
  data_type={KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/0f19fb8a556ffcfad4c976b97a502b502a5a4352},
  doi = {10.1109/ICRA57147.2024.10610411},
  abstract = {The integration of high-level assistance algorithms in surgical robotics training curricula may be beneficial in establishing a more comprehensive and robust skillset for aspiring surgeons, improving their clinical performance as a consequence. This work presents the development and validation of a haptic-enhanced Virtual Reality simulator for surgical robotics training, featuring 8 surgical tasks that the trainee can interact with thanks to the embedded physics engine. This virtual simulated environment is augmented by the introduction of high-level haptic interfaces for robotic assistance that aim at re-directing the motion of the trainee’s hands and wrists toward targets or away from obstacles, and providing a quantitative performance score after the execution of each training exercise.An experimental study shows that the introduction of enhanced robotic assistance into a surgical robotics training curriculum improves performance during the training process and, crucially, promotes the transfer of the acquired skills to an unassisted surgical scenario, like the clinical one.}
}

@inproceedings{Barragan2024ISMR,
  title = {Improving the realism of robotic surgery simulation through injection of learning-based estimated errors},
  author = {Barragan, Juan Antonio and Ishida, Hisashi and Munawar, Adnan and Kazanzides, Peter},

  year = {2024},
  date = {2024-06-01},
  urldate = {2024-06-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/96b906922390e3a1c2171bb54082a412be2f5d5e},
  doi = {10.1109/ISMR63436.2024.10585672},
  abstract = {The development of algorithms for automation of subtasks during robotic surgery can be accelerated by the availability of realistic simulation environments. In this work, we focus on one aspect of the realism of a surgical simulator, which is the positional accuracy of the robot. In current simulators, robots have perfect or near-perfect accuracy, which is not representative of their physical counterparts. We therefore propose a pair of neural networks, trained by data collected from a physical robot, to estimate both the controller error and the kinematic and non-kinematic error. These error estimates are then injected within the simulator to produce a simulated robot that has the characteristic performance of the physical robot. In this scenario, we believe it is sufficient for the estimated error used in the simulation to have a statistically similar distribution to the actual error of the physical robot. This is less stringent, and therefore more tenable, than the requirement for error compensation of a physical robot, where the estimated error should equal the actual error. Our results demonstrate that error injection reduces the mean position and orientation differences between the simulated and physical robots from 5.0 mm / 3.6 deg to 1.3 mm / 1.7 deg, respectively, which represents reductions by factors of 3.8 and 2.1.},
}

@Article{Yang2024AutomatedTG,
 author = {Ziqi Yang and Ruiyang Zhang and Junhong Chen and Xuhui Zhou and Yunxiao Ren and Ziyue Tong and Benny Lo},
 booktitle = {International Conference on Advanced Robotics and Mechatronics},
 journal = {2024 International Conference on Advanced Robotics and Mechatronics (ICARM)},
 pages = {39-44},
 title = {Automated Trajectory Generation for Robotic Surgical Tasks},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/c4072a7010f7f96f447658736af6ebb441748c54},
  doi = {10.1109/ICARM62033.2024.10715872},
  abstract = {The study aims to develop an emotional logic engine based on a large language model (LLM), providing emotional connections, personalized interactions, knowledge representation, and logical inference. Using this emotional logic engine, we intend to realize the goals of high-level cognition, autonomous knowledge reasoning, long-horizon planning, and action execution described in embodied artificial intelligence (embodied AI). Ultimately, we will implement an efficient intelligent companion interaction robot (ICIR) based on a novel human-robot interaction (HRI) framework to enhance the interaction between humans and robots. The proposed framework integrates multiple components including a visual language model (VLM), logic reasoning model, pre-trained database integration, and the development of a multi-modal template. Additionally, we introduce a complementary framework termed perception-action loop (PALoop), which is meticulously modeled and constructed to facilitate seamless interactions between human operators and robotic systems. Detailed design aspects of both frameworks are elucidated, providing insights into their architecture and functionality. The research outcomes will be practically applied, offering the robotics industry innovative and practical technology solutions.}
}

@Article{Zargarzadeh2024AugmentedRT,
 author = {Sadra Zargarzadeh and August Sieben and Ericka Wiebe and L. Peiris and Mahdi Tavakoli},
 booktitle = {International Conferences on Human-Machine Systems},
 journal = {2024 IEEE 4th International Conference on Human-Machine Systems (ICHMS)},
 pages = {1-6},
 title = {Augmented Reality-Based Tumor Localization and Visualization for Robot-Assisted Breast Surgeries},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/4a1dd22eb950621d85ec5acebb9aa85cefae2e33},
  doi = {10.1109/ICHMS65439.2025}
}

@Article{Soberanis-Mukul2024CognitiveLI,
 author = {Roger D. Soberanis-Mukul and Paola Ruiz Puentes and Ayberk Acar and Iris Gupta and Joyraj Bhowmick and Yizhou Li and Ahmed Ghazi and Jie Ying Wu and Mathias Unberath},
 booktitle = {International Journal of Computer Assisted Radiology and Surgery},
 journal = {International Journal of Computer Assisted Radiology and Surgery},
 pages = {1281 - 1284},
 title = {Cognitive load in tele-robotic surgery: a comparison of eye tracker designs},
 volume = {19},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/eb831fb9983aa01133923fb47c970036e066ea8f},
  doi = {10.1007/s11548-024-03304-x}
}

@inproceedings{kim2024learning,
  title={Learning a library of surgical manipulation skills for robotic surgery},
  author={Kim, Ji Woong and Schmidgall, Samuel and Krieger, Axel and Kobilarov, Marin},

  booktitle={Bridging the Gap between Cognitive Science and Robot Learning in the Real World: Progresses and New Directions},
  year={2024},
  url={https://openreview.net/forum?id=fYRlaylCI3},
  semanticscholar = {https://www.semanticscholar.org/paper/3f19cceee65ea1cd94cf2e48db7b6dacee4d632d}
}

@article{Ai2024,
  title = {Mixed reality based teleoperation and visualization of surgical robotics},
  author = {Ai, Letian and Kazanzides, Peter and Azimi, Ehsan},

  doi = {10.1049/htl2.12079},
  year = {2024},
  date = {2024-04-01},
  urldate = {2024-04-01},
  journal = {IET Healthcare Technology Letters},
  volume = {11},
  number = {2-3},
  pages = {179-188},
  keywords = {dvrk, hmd},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/2906091e2708fa206d4db64775508f0a7a1d678e},
  abstract = {Abstract Surgical robotics has revolutionized the field of surgery, facilitating complex procedures in operating rooms. However, the current teleoperation systems often rely on bulky consoles, which limit the mobility of surgeons. This restriction reduces surgeons' awareness of the patient during procedures and narrows the range of implementation scenarios. To address these challenges, an alternative solution is proposed: a mixed reality‐based teleoperation system. This system leverages hand gestures, head motion tracking, and speech commands to enable the teleoperation of surgical robots. The implementation focuses on the da Vinci research kit (dVRK) and utilizes the capabilities of Microsoft HoloLens 2. The system's effectiveness is evaluated through camera navigation tasks and peg transfer tasks. The results indicate that, in comparison to manipulator‐based teleoperation, the system demonstrates comparable viability in endoscope teleoperation. However, it falls short in instrument teleoperation, highlighting the need for further improvements in hand gesture recognition and video display quality.},
}

@inproceedings{yu2024orbit,
  title={Orbit-surgical: An open-simulation framework for learning surgical augmented dexterity},
  author={Yu, Qinxi and Moghani, Masoud and Dharmarajan, Karthik and Schorp, Vincent and Panitch, William Chung-Ho and Liu, Jingzhou and Hari, Kush and Huang, Huang and Mittal, Mayank and Goldberg, Ken and others},

  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={15509--15516},
  year={2024},
  organization={IEEE},
  ieeexplore = {https://ieeexplore.ieee.org/abstract/document/10611637}
}

@inproceedings{rota2024performance,
  title={Performance-driven tasks with adaptive difficulty for enhanced surgical robotics training},
  author={Rota, Alberto and Sun, Xianyi Federica and De Momi, Elena},

  booktitle={2024 10th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)},
  pages={465--470},
  year={2024},
  organization={IEEE},
  research_field={TR},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/66e82998beab20dd3a64e5b90e7e155b5bbd1f5d},
  doi = {10.1109/BioRob60516.2024.10719830},
  abstract = {Surgical robotics training most often occurs through standardized curricula and exercises that lack customization and do not adapt to the different levels of proficiency that trainees often present. This work proposes a Virtual Reality (VR) simulator for surgical robotics that autonomously adjusts difficulty levels based on trainee performance, aiming to enhance skill retention and transfer. The study employs a performance-based adaptive difficulty approach, dynamically adjusting parameters of each task's morphology to match individual proficiency levels. The proposed adaptive simulator is evaluated against a non-adaptive counterpart through a week-long training program. The results demonstrate the effectiveness of the adaptive simulator in enhancing performance at higher difficulty levels, supporting its potential to benefit surgical education by providing a tailored and scalable training approach.}
}

@Article{Yang2024AnES,
 author = {Hao Yang and Ayberk Acar and Keshuai Xu and A. Deguet and P. Kazanzides and Jie Ying Wu},
 booktitle = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
 journal = {ArXiv},
 title = {An Effectiveness Study Across Baseline and Neural Network-based Force Estimation Methods on the da Vinci Research Kit Si System},
 volume = {abs/2405.07453},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/e16eceb7d574d504c31e13babe85f516455cd195},
  doi = {10.31256/hsmr2024.50},
  abstract = {Conventional endoscopic instruments restricted by their bulky size, lack the dexterity and intuitiveness required for complex therapeutic interventions. To address this shortfall, our research introduces an innovative endo- scopic robotic system featuring through-the-scope dex- terous instruments coupled with a handwriting-inspired human-robot interface. This novel system is designed to improve surgical precision, intuitiveness, and dexterity, potentially revolutionizing the capabilities of therapeutic endoscopic procedures.}
}

@Article{Walder2024CostEfficientAO,
 author = {Alexander Walder and Simon Winkler and Yeongmi Kim},
 booktitle = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
 journal = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
 title = {Cost-Efficient and Open-Source Desktop Teleoperated Surgical Training System},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/e16eceb7d574d504c31e13babe85f516455cd195},
  doi = {10.31256/hsmr2024.50},
  abstract = {Conventional endoscopic instruments restricted by their bulky size, lack the dexterity and intuitiveness required for complex therapeutic interventions. To address this shortfall, our research introduces an innovative endo- scopic robotic system featuring through-the-scope dex- terous instruments coupled with a handwriting-inspired human-robot interface. This novel system is designed to improve surgical precision, intuitiveness, and dexterity, potentially revolutionizing the capabilities of therapeutic endoscopic procedures.}
}

@Article{Cui2024CalibrationFF,
 author = {Zejian Cui and F. R. Y. Baena},
 booktitle = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
 journal = {Proceedings of the 16th Hamlyn Symposium on Medical Robotics 2024},
 title = {Calibration Framework for Positioning Accuracy Improvement of da Vinci Surgical Instruments},
 year = {2024}
}

@inproceedings{Barragan2024ICRA,
  title = {Realistic Data Generation for 6D Pose Estimation of Surgical Instruments},
  author = {Barragan, Juan Antonio and Zhang, Jintan and Zhou, Haoying and Munawar, Adnan and Kazanzides, Peter},

  year = {2024},
  date = {2024-05-01},
  urldate = {2024-05-01},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  address = {Yokohama, Japan},
  keywords = {ambf, dvrk, simulation},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/72bb0b2bd405f1acf6faab4ac11305ace8a286fc},
  doi = {10.1109/ICRA57147.2024.10611638},
  abstract = {Automation in surgical robotics has the potential to improve patient safety and surgical efficiency, but it is difficult to achieve due to the need for robust perception algorithms. In particular, 6D pose estimation of surgical instruments is critical to enable the automatic execution of surgical maneuvers based on visual feedback. In recent years, supervised deep learning algorithms have shown increasingly better performance at 6D pose estimation tasks; yet, their success depends on the availability of large amounts of annotated data. In household and industrial settings, synthetic data, generated with 3D computer graphics software, has been shown as an alternative to minimize annotation costs of 6D pose datasets. However, this strategy does not translate well to surgical domains as commercial graphics software have limited tools to generate images depicting realistic instrument-tissue interactions. To address these limitations, we propose an improved simulation environment for surgical robotics that enables the automatic generation of large and diverse datasets for 6D pose estimation of surgical instruments. Among the improvements, we developed an automated data generation pipeline and an improved surgical scene. To show the applicability of our system, we generated a dataset of 7.5k images with pose annotations of a surgical needle that was used to evaluate a state-of-the-art pose estimation network. The trained model obtained a mean translational error of 2.59mm on a challenging dataset that presented varying levels of occlusion. These results highlight our pipeline’s success in training and evaluating novel vision algorithms for surgical robotics applications.},
}

@article{YilmazRAL2024,
  title = {Sensorless Transparency Optimized Haptic Teleoperation on the da Vinci Research Kit},
  author = {Yilmaz, Nural and Burkhart, Brendan and Deguet, Anton and Kazanzides, Peter and Tumerdem, Ugur},

  doi = {10.1109/LRA.2023.3335779},
  year = {2024},
  date = {2024-02-01},
  urldate = {2024-02-01},
  journal = {IEEE Robotics and Automation Letters},
  volume = {9},
  number = {2},
  pages = {971-978},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/6bb0bfb973672681a7712f092236e01d9e9f549f},
  abstract = {The da Vinci surgical robot introduced remote control of instruments, providing surgeons with increased dexterity and precision. A major drawback, however, is the loss of sense of touch due to a lack of kinesthetic coupling between the surgical field and the surgeon. This paper presents a framework for sensorless transparency optimized four channel teleoperation. It is sensorless because forces are estimated from existing actuator feedback, with a deep network for dynamics identification. Performance is further optimized by introducing robust acceleration control, with disturbance observers. Experiments performed on the da Vinci Research Kit (dVRK), an open research platform based on the clinically deployed robotic hardware, show improvements in control, force estimation and reflection. The significance is that we demonstrate that high-performance bilateral teleoperation is feasible in clinical systems, without hardware changes, and is available to the dVRK community through a software update.}
}

@article{Cartucho2024SurgT,
  author = {Cartucho, João and Weld, Alistair and Tukra, Samyakh and Xu, Haozheng and Matsuzaki, Hiroki and Ishikawa, Taiyo and Kwon, Minjun and Jang, Yong Eun and Kim, Kwang-Ju and Lee, Gwang and Bai, Bizhe and Kahrs, Lueder A and Boecking, Lars and Allmendinger, Simeon and Müller, Leopold and Zhang, Yitong and Jin, Yueming and Bano, Sophia and Vasconcelos, Francisco and Reiter, Wolfgang and Hajek, Jonas and Silva, Bruno and Lima, Estevão and Vilaça, João L and Queirós, Sandro and Giannarou, Stamatia},

  title = {SurgT challenge: Benchmark of soft-tissue trackers for robotic surgery},
  journal = {Medical Image Analysis},
  volume = {91},
  pages = {102985},
  year = {2024},
  doi = {10.1016/j.media.2023.102985},
  eprint = {2302.03022},
  archivePrefix = {arXiv},
  semanticscholar = {https://www.semanticscholar.org/paper/104bd583ee064c9473b62532e4a8142bbefab650},
  abstract = {This paper introduces the "SurgT: Surgical Tracking" challenge which was organized in conjunction with the 25th International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI 2022). There were two purposes for the creation of this challenge: (1) the establishment of the first standardized benchmark for the research community to assess soft-tissue trackers; and (2) to encourage the development of unsupervised deep learning methods, given the lack of annotated data in surgery. A dataset of 157 stereo endoscopic videos from 20 clinical cases, along with stereo camera calibration parameters, have been provided. Participants were assigned the task of developing algorithms to track the movement of soft tissues, represented by bounding boxes, in stereo endoscopic videos. At the end of the challenge, the developed methods were assessed on a previously hidden test subset. This assessment uses benchmarking metrics that were purposely developed for this challenge, to verify the efficacy of unsupervised deep learning algorithms in tracking soft-tissue. The metric used for ranking the methods was the Expected Average Overlap (EAO) score, which measures the average overlap between a tracker's and the ground truth bounding boxes. Coming first in the challenge was the deep learning submission by ICVS-2Ai with a superior EAO score of 0.617. This method employs ARFlow to estimate unsupervised dense optical flow from cropped images, using photometric and regularization losses. Second, Jmees with an EAO of 0.583, uses deep learning for surgical tool segmentation on top of a non-deep learning baseline method: CSRT. CSRT by itself scores a similar EAO of 0.563. The results from this challenge show that currently, non-deep learning methods are still competitive. The dataset and benchmarking tool created for this challenge have been made publicly available at https://surgt.grand-challenge.org/. This challenge is expected to contribute to the development of autonomous robotic surgery and other digital surgical technologies.}
}

@inproceedings{Zhou2024ISMR,
  title = {Suturing Tasks Automation Based on Skills Learned From Demonstrations: A Simulation Study},
  author = {Zhou, Haoying and Jiang, Yiwei and Gao, Shang and Wang, Shiyue and Kazanzides, Peter and Fischer, Gregory S},

  year = {2024},
  date = {2024-06-01},
  urldate = {2024-06-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {ambf, dvrk, machine-learning, simulation},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/12663f654c2cc1588bc366d8c88d36b173ea3b12},
  doi = {10.1109/ISMR63436.2024.10586017},
  abstract = {In this work, we develop an open-source surgical simulation environment that includes a realistic model obtained by MRI-scanning a physical phantom, for the purpose of training and evaluating a Learning from Demonstration (LfD) algorithm for autonomous suturing. The LfD algorithm utilizes Dynamic Movement Primitives (DMP) and Locally Weighted Regression (LWR), but focuses on the needle trajectory, rather than the instruments, to obtain better generality with respect to needle grasps. We conduct a user study to collect multiple suturing demonstrations and perform a comprehensive analysis of the ability of the LfD algorithm to generalize from a demonstration at one location in one phantom to different locations in the same phantom and to a different phantom. Our results indicate good generalization, on the order of 91.5%, when learning from more experienced subjects, indicating the need to integrate skill assessment in the future.},
}

@article{ding2024towards,
  title={Towards robust automation of surgical systems via digital twin-based scene representations from foundation models},
  author={Ding, Hao and Seenivasan, Lalithkumar and Shu, Hongchao and Byrd, Grayson and Zhang, Han and Xiao, Pu and Barragan, Juan Antonio and Taylor, Russell H and Kazanzides, Peter and Unberath, Mathias},

  journal={arXiv preprint arXiv:2409.13107},
  year={2024},
  url={https://arxiv.org/abs/2409.13107},
  semanticscholar = {https://www.semanticscholar.org/paper/84db0a87c3613b1b418bced000bd06cae1bb8f05},
  doi = {10.48550/arXiv.2409.13107},
  abstract = {Large language model-based (LLM) agents are emerging as a powerful enabler of robust embodied intelligence due to their capability of planning complex action sequences. Sound planning ability is necessary for robust automation in many task domains, but especially in surgical automation. These agents rely on a highly detailed natural language representation of the scene. Thus, to leverage the emergent capabilities of LLM agents for surgical task planning, developing similarly powerful and robust perception algorithms is necessary to derive a detailed scene representation of the environment from visual input. Previous research has focused primarily on enabling LLM-based task planning while adopting simple yet severely limited perception solutions to meet the needs for bench-top experiments but lack the critical flexibility to scale to less constrained settings. In this work, we propose an alternate perception approach -- a digital twin-based machine perception approach that capitalizes on the convincing performance and out-of-the-box generalization of recent vision foundation models. Integrating our digital twin-based scene representation and LLM agent for planning with the dVRK platform, we develop an embodied intelligence system and evaluate its robustness in performing peg transfer and gauze retrieval tasks. Our approach shows strong task performance and generalizability to varied environment settings. Despite convincing performance, this work is merely a first step towards the integration of digital twin-based scene representations. Future studies are necessary for the realization of a comprehensive digital twin framework to improve the interpretability and generalizability of embodied intelligence in surgery.}
}

@Article{Iacono2024DesignAV,
 author = {Cristina Iacono and Marco Caianiello and Serena Bartiromo and Aldo Smaldone and Fanny Ficuciello},
 booktitle = {Workshop on Advanced Robotics and its Social Impacts},
 journal = {2024 IEEE International Conference on Advanced Robotics and Its Social Impacts (ARSO)},
 pages = {98-103},
 title = {Design and Validation of a Multimodal Dataset of Robot-Assisted Suturing Gestures based on Kinematic and Force Information},
 year = {2024},
  semanticscholar = {https://www.semanticscholar.org/paper/b68489d2a260239539fe6e6f16558601def065bb},
  doi = {10.1109/ARSO60199.2024.10557814},
  abstract = {Healthy ageing is a challenge in societies that can be coped with the help of socially assistive robots. This study introduces CelesTE, a theomorphic device designed to support the well-being of older adults. Building upon the foundations laid by SanTO, the Catholic robot, CelesTE takes the form of an angel in prayer and aims to engage users, particularly those of the Christian Catholic faith. The paper delves into CelesTE’s conceptual evolution, addressing challenges related to religious perceptions, fallibility, and user interaction. Quantitative and qualitative feedback was collected from 14 participants across three European countries. Results indicate generally positive acceptance, although limitation were found. Negative responses are considered particularly valuable for CelesTE’s future development.}
}

@article{Rivas-Blanco2021a,
  author = {Rivas-Blanco, Irene and P{\'{e}}rez-del-Pulgar, Carlos J. and Mariani, Andrea and Quaglia, Claudio and Tortora, Giuseppe and Menciassi, Arianna and Mu{\~{n}}oz, V{\'{i}}ctor F.},

  journal = {IEEE Access},
  title = {{A surgical dataset from the da Vinci Research Kit for task automation and recognition}},
  volume = {11},
  year = {2023},
  research_field={TR},
  data_type={RI and KD and SD}
}

@inproceedings{GreeneISMR2023,
  title = {dVPose: Automated Data Collection and Dataset for 6D Pose Estimation of Robotic Surgical Instruments},
  author = {Greene, Nicholas and Luo, Wenkai and Kazanzides, Peter},

  year = {2023},
  date = {2023-04-01},
  urldate = {2023-04-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {arssist, dvrk, hmd},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/4a3fcc3d99ca77809e037ec584dd4041b8b5f460},
  doi = {10.1109/ISMR57123.2023.10130238},
  abstract = {We present dVPose, a realistic multi-modality dataset intended for use in the development and evaluation of real-time single-shot deep-learning based 6D pose estimation algorithms on a head mounted display (HMD). In addition to the dataset, our contribution includes an automated (robotic) data collection platform that integrates an accurate optical tracking system to provide the ground-truth poses. We collected a comprehensive set of data for vision-based 6D pose estimation, including images and poses of the extra-corporeal portions of the instruments and endoscope of a da Vinci surgical robot. The images are collected using the multi-camera rig of the Microsoft HoloLens 2 HMD, mounted on a UR10 robot, and the corresponding poses are collected by optically tracking both the instruments/endoscope and HMD. The intended application is to enable markerless localization of the HMD with respect to the da Vinci robot, considering that the instruments and endoscope are among the few robotic components that are not covered by sterile drapes. Our dataset features synchronized images from the RGB, depth, and grayscale cameras of the HoloLens 2 device. It is unique in that it provides medically focused images, provides images from a HoloLens 2 device where object tracking is a fundamental task, and provides data from multiple visible-light cameras in addition to depth. Furthermore, the automated data collection platform can be easily adapted to collect images and ground-truth poses of other objects.}
}

@inproceedings{Deo2023,
  title = {Feasibility of Mobile Application for Surgical Robot Teleoperation},
  author = {Deo, Akhil and Kazanzides, Peter},

  booktitle = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
  pages={121--122},
  publisher = {The Hamlyn Centre, Imperial College London London, UK},
  year = {2023},
  month = {jun},
  doi = {10.31256/HSMR2023.63},
  research_field={TR and HW},
  data_type={KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/ffc2919d3c966c096e2b89993f5031b8f2a1738a},
  abstract = {In recent years, robotic surgery has gained popularity due to its numerous advantages, including greater control and access for surgeons, shorter recovery times, and lower levels of pain for patients [1]. However, surgical robot systems require extensive training for surgeons to master their use. Simulation-based training has become a component of surgical education [2], providing a safe environment for trainees to acquire and refine their skills. However, most existing training platforms require bulky and expensive control consoles, which limits their availability and convenience. The development of a low- cost and easily deployed control console can address these limitations, thereby potentially enhancing the effectiveness of robotic surgery training. A system that satisfies these criteria can also enable medical robotics research in low-resource environments, where cost and accessibility are the most significant impediments to research. This paper describes the creation and evaluation of an iPhone application for these purposes.},
}

@article{Pasini2023,
  title = {GRACE: Online Gesture Recognition for Autonomous Camera-Motion Enhancement in Robot-Assisted Surgery},
  author = {Pasini, Nicolo and Mariani, Andrea and Deguet, Anton and Kazanzides, Peter and Momi, Elena De},

  doi = {10.1109/LRA.2023.3326690},
  year = {2023},
  date = {2023-12-01},
  urldate = {2023-12-01},
  journal = {IEEE Robotics and Automation Letters},
  volume = {8},
  number = {12},
  pages = {8263-8270},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/e2e33ae352b72bd0a48c4543ad70d0545e377e67},
  abstract = {Camera navigation in minimally invasive surgery changed significantly since the introduction of robotic assistance. Robotic surgeons are subjected to a cognitive workload increase due to the asynchronous control over tools and camera, which also leads to interruptions in the workflow. Camera motion automation has been addressed as a possible solution, but still lacks situation awareness. We propose an online surgical Gesture Recognition for Autonomous Camera-motion Enhancement (GRACE) system to introduce situation awareness in autonomous camera navigation. A recurrent neural network is used in combination with a tool tracking system to offer gesture-specific camera motion during a robotic-assisted suturing task. GRACE was integrated with a research version of the da Vinci surgical system and a user study (involving 10 participants) was performed to evaluate the benefits introduced by situation awareness in camera motion, both with respect to a state of the art autonomous system (S) and current clinical approach (P). Results show GRACE improving completion time by a median reduction of <inline-formula><tex-math notation="LaTeX">$\text\{18.9\} \text\{s\}$</tex-math></inline-formula> (8.1%) with respect to S and <inline-formula><tex-math notation="LaTeX">$\text\{65.1\}\text\{s\}$</tex-math></inline-formula> (21.1%) with respect to P. Also, workload reduction was confirmed by statistical difference in the NASA Task Load Index with respect to S (<inline-formula><tex-math notation="LaTeX">$p < \text\{0.05\}$</tex-math></inline-formula>). Reduction of motion sickness, a common issue related to continuous camera motion of autonomous systems, was assessed by a post-experiment survey (<inline-formula><tex-math notation="LaTeX">$p < \text\{0.01\}$</tex-math></inline-formula>).},
}

@Article{Moccia2023AutonomousEC,
 author = {R. Moccia and F. Ficuciello},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {776-781},
 title = {Autonomous Endoscope Control Algorithm with Visibility and Joint Limits Avoidance Constraints for da Vinci Research Kit Robot},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Huang2023DemonstrationGuidedRL,
 author = {Tao Huang and Kai Chen and Bin Li and Yunhui Liu and Qingxu Dou},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {4640-4647},
 title = {Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Hu2023TowardsHC,
 author = {Zhaoyang Jacopo Hu and Ziwei Wang and Yanpei Huang and Aran Sena and F. Rodriguez y Baena and E. Burdet},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {4553-4560},
 title = {Towards Human-Robot Collaborative Surgery: Trajectory and Strategy Learning in Bimanual Peg Transfer},
 volume = {8},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Ferro2023ACD,
 author = {Marco Ferro and Alessandro Mirante and F. Ficuciello and Marilena Vendittelli},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {129-136},
 title = {A CoppeliaSim Dynamic Simulator for the Da Vinci Research Kit},
 volume = {8},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Liu2023RoboticMO,
 author = {Fei Liu and Entong Su and Jingpei Lu and Ming Li and Michael C. Yip},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {3964-3971},
 title = {Robotic Manipulation of Deformable Rope-Like Objects Using Differentiable Compliant Position-Based Dynamics},
 volume = {8},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Zhang2023AST,
 author = {Rui Zhang and Junhong Chen and Zeyu Wang and Ziqi Yang and Yunxiao Ren and Peilun Shi and James Calo and K. Lam and S. Purkayastha and Benny P. L. Lo},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {2429-2436},
 title = {A Step Towards Conditional Autonomy - Robotic Appendectomy},
 volume = {8},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Minelli2023TwoLayerBasedMB,
 author = {M. Minelli and Nicola Piccinelli and Fabio Falezza and F. Ferraguti and R. Muradore and C. Secchi},
 booktitle = {IEEE Transactions on Control Systems Technology},
 journal = {IEEE Transactions on Control Systems Technology},
 pages = {1266-1279},
 title = {Two-Layer-Based Multiarms Bilateral Teleoperation Architecture},
 volume = {31},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/5144c3b1286fe349ad63ef03e2533d9d5c9f4d8b},
  doi = {10.1109/tcst.87},
  abstract = {M. MAGGIORE University of Toronto C. MANZIE Univ. of Melbourne A. MARINO University of Salerno P. MHASKAR McMaster Univ. G. NOTARSTEFANO University of Salento P. ODGAARD Vestas Wind Systems M. OISHI University of New Mexico N. OLGAC Univ. of Connecticut T. OOMEN Eindhoven University Y. ORLOV CICESE Mexico Y. PAN National University of Singapore C. PANAYIOTOU Univ. of Cyprus G. PAPAFOTIOU ABB A. PAVLOV NTNU, Norway G. PIN Electrolux S. PIROZZI Università di Napoli H. POTA Univ. of New South Wales at ADFA C. PRIEUR CNRS E. PUNTA National Research Council, Italy N. QUIJANO Universidad de los Andes Colombia D. M. RAIMONDO University of Pavia G. A. ROVITHAKIS Univ. of Thessaloniki K. RUDIE Queen’s Univ.}
}

@Article{Sallam2023PrototypeRO,
 author = {M. Sallam and G. A. Fontanelli and A. Gallo and R. Rocca and A. D. S. Sardo and Nicola Longo and F. Ficuciello},
 booktitle = {IEEE Transactions on Medical Robotics and Bionics},
 journal = {IEEE Transactions on Medical Robotics and Bionics},
 pages = {843-856},
 title = {Prototype Realization of a Human Hand-Inspired Needle Driver for Robotic-Assisted Surgery},
 volume = {5},
 year = {2023}
}

@Article{Wang2023UncertaintyAwareSL,
 author = {Ziheng Wang and A. Mariani and A. Menciassi and E. De Momi and A. M. Fey},
 booktitle = {IEEE Transactions on Medical Robotics and Bionics},
 journal = {IEEE Transactions on Medical Robotics and Bionics},
 pages = {301-311},
 title = {Uncertainty-Aware Self-Supervised Learning for Cross-Domain Technical Skill Assessment in Robot-Assisted Surgery},
 volume = {5},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/2529a49e5e437f7168ef3d526e2f60a9690a6cae},
  doi = {10.1109/tmrb.2021.3054138},
}

@Article{Chu2023BootstrappingRS,
 author = {X. Chu and Yunxi Tang and L. Kwok and Yuanpei Cai and K. W. S. Au},
 booktitle = {International Symposium on Experimental Robotics},
 pages = {42-52},
 title = {Bootstrapping Robotic Skill Learning With Intuitive Teleoperation: Initial Feasibility Study},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/64d9374af1ddf5b6f19e65ec5a8af107ea39284c},
  doi = {10.1007/978-3-319-23778-7}
}

@Article{Oh2023ComprehensiveRC,
 author = {Ki Hwan Oh and Leonardo Borgioli and Alberto Mangano and Valentina Valle and Marco Di Pangrazio and F. Toti and Gioia Pozza and Luciano Ambrosini and A. Ducas and Milos Zefran and Liaohai Chen and P. Giulianotti},
 booktitle = {International Symposium on Medical Robotics},
 journal = {2024 International Symposium on Medical Robotics (ISMR)},
 pages = {1-7},
 title = {Comprehensive Robotic Cholecystectomy Dataset (CRCD): Integrating Kinematics, Pedal Signals, and Endoscopic Videos},
 year = {2023}
}

@Article{Dharmarajan2023ATF,
 author = {K. Dharmarajan and Will Panitch and Baiyu Shi and Huang Huang and L. Chen and Thomas Low and Danyal M. Fer and Ken Goldberg},
 booktitle = {International Symposium on Medical Robotics},
 journal = {2023 International Symposium on Medical Robotics (ISMR)},
 pages = {1-8},
 title = {A Trimodal Framework for Robot-Assisted Vascular Shunt Insertion When a Supervising Surgeon is Local, Remote, or Unavailable},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
  doi = {10.1109/ISMR67322.2025}
}

@Article{Baweja2023ExperimentalTW,
 author = {Paramjit Singh Baweja and R. Gondokaryono and L. Kahrs},
 booktitle = {International Symposium on Medical Robotics},
 journal = {2023 International Symposium on Medical Robotics (ISMR)},
 pages = {1-7},
 title = {Experimental Trials with a Shared Autonomy Controller Framework and the da Vinci Research Kit: Pattern Cutting Tasks using Thin Elastic Materials},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
  doi = {10.1109/ISMR67322.2025}
}

@Article{Dharmarajan2023RobotAssistedVS,
 author = {K. Dharmarajan and Will Panitch and Baiyu Shi and Huang Huang and L. Chen and M. Moghani and Qinxi Yu and Kush Hari and Thomas Low and Danyal M. Fer and Animesh Garg and Ken Goldberg},
 booktitle = {J. Medical Robotics Res.},
 journal = {J. Medical Robotics Res.},
 pages = {2340006:1-2340006:15},
 title = {Robot-Assisted Vascular Shunt Insertion with the dVRK Surgical Robot},
 volume = {8},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/f468bb674fac8fbbcbece1d58c89ce2fd6f1cedc},
  doi = {10.48550/arXiv.2502.18586},
  abstract = {Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.}
}

@Article{Ou2023RobotLI,
 author = {Yafei Ou and Sadra Zargarzadeh and Mahdi Tavakoli},
 booktitle = {J. Medical Robotics Res.},
 journal = {J. Medical Robotics Res.},
 pages = {2340004:1-2340004:12},
 title = {Robot Learning Incorporating Human Interventions in the Real World for Autonomous Surgical Endoscopic Camera Control},
 volume = {8},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/f468bb674fac8fbbcbece1d58c89ce2fd6f1cedc},
  doi = {10.48550/arXiv.2502.18586},
  abstract = {Existing tracheal tumor resection methods often lack the precision required for effective airway clearance, and robotic advancements offer new potential for autonomous resection. We present a vision-guided, autonomous approach for palliative resection of tracheal tumors. This system models the tracheal surface with a fifth-degree polynomial to plan tool trajectories, while a custom Faster R-CNN segmentation pipeline identifies the trachea and tumor boundaries. The electrocautery tool angle is optimized using handheld surgical demonstrations, and trajectories are planned to maintain a 1 mm safety clearance from the tracheal surface. We validated the workflow successfully in five consecutive experiments on ex-vivo animal tissue models, successfully clearing the airway obstruction without trachea perforation in all cases (with more than 90% volumetric tumor removal). These results support the feasibility of an autonomous resection platform, paving the way for future developments in minimally-invasive autonomous resection.}
}

@Article{Soudan2023DevelopmentAV,
 author = {Mulham Soudan and Shannon L. King and Scotty A. Chung and Philip Brown},
 booktitle = {Journal of Medical and Biological Engineering},
 journal = {Journal of Medical and Biological Engineering},
 pages = {332-338},
 title = {Development and Validation of a 3DOF Force Sensing Tool for In-Situ Surgical Robotics},
 volume = {43},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/fa63e4936e26f82cfbdc80b707bd0416887a58b7},
  doi = {10.1007/s40846-015-0091-y}
}

@Article{Vries2023MRguidedHP,
 author = {M. de Vries and M. Wijntjes and J. Sikorski and P. Moreira and N. J. van de Berg and J. J. van den Dobbelsteen and S. Misra},
 booktitle = {Journal of Robotic Surgery},
 journal = {Journal of Robotic Surgery},
 pages = {2461 - 2469},
 title = {MR-guided HDR prostate brachytherapy with teleoperated steerable needles},
 volume = {17},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/58253e7b7ceea8a141829529091c75d0bda3190f},
  doi = {10.1007/s11701-025-02399-x}
}

@Article{Büter2023EyeTF,
 author = {Regine Büter and Roger D. Soberanis-Mukul and Paola Ruiz Puentes and Ahmed Ghazi and Jie Ying Wu and Mathias Unberath},
 booktitle = {Medical Imaging},
 pages = {129281Y - 129281Y-6},
 title = {Eye tracking for tele-robotic surgery: a comparative evaluation of head-worn solutions},
 volume = {12928},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/f9b5c08711b30103ffc7ba144d4f590bbd9b065b},
  doi = {10.1038/nrclinonc.2017.141}
}

@inproceedings{ChenISMR2023,
  title = {Mixed Reality Based Teleoperation of Surgical Robotics},
  author = {Chen, An Chi and Hadi, Muhammad and Kazanzides, Peter and Azimi, Ehsan},

  year = {2023},
  date = {2023-04-01},
  urldate = {2023-04-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk, hmd},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/dccfbc6e37fe0a1cef7fc49598d4b70b6234f288},
  doi = {10.1109/ISMR57123.2023.10130178},
  abstract = {Many surgical robotic systems are controlled by mechanical based devices that require the operator to remain at a fixed location away from the robot. This restriction in mobility and physical barrier between the surgeon and the robot may reduce procedural efficiency. Thus, we propose an alternative teleoperation approach and mixed reality based system that uses the surgeon's tracked hand poses to control the robot through the use of an untethered head mounted display. We conducted a controlled user study to assess the efficacy of our system. Our experimental results indicate that, for the ring-wire task we tested, there is not a considerable difference in the performance of users compared to existing mechanical based teleoperation devices.}
}

@Article{Senthilkumar2023SimulatingMC,
 author = {Kanishkan Senthilkumar and R. Gondokaryono and Mustafa Haiderbhai and L. Kahrs},
 booktitle = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
 journal = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
 title = {Simulating Mesh Cutting with the dVRK in Unity},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/1e93093119bb9106c0303e97200ee4ddada832c8},
  doi = {10.31256/hsmr2023.75},
  abstract = {Raman spectroscopy is a photonic modality defined as the inelastic backscattering of excitation coherent laser light. It is particularly beneficial for rapid tissue diagnosis in sensitive intraoperative environments like those involving the brain, due to its nonionizing potential, point-scanning capability, and highly-specific spectral fingerprint signatures that can characterize tissue pathology [1]. While Raman scattering is an inherently weak process, Surface-Enhanced Raman Spectroscopy (SERS), which is based on the use of metal nanostructure surfaces to amplify Raman signals, has become a compelling method for achieving highly specific Raman spectra with detection sensitivity comparable to conventional modalities such as fluorescence [2]. A unique plasmonics-active nanoplatform, SERS gold nanostars (GNS) have previously been designed in our group to accumulate preferentially in brain tumors [2]. Raman detection, when combined with machine learning and robotics, stands to enhance the diagnosis of ambiguous tissue during tumor resection surgery, with the potential to improve extent-of-resection and rapidly reconstruct the dynamic surgical field. Here we demonstrate preliminary results from the use of a SERS-based robotics platform to efficiently recreate a tumor embedded in healthy tissue, which is modeled here as a GNS-infused phantom. Transfer learning, specifically through use of the open-source RRUFF mineral database, is employed here to address the dearth of collected biomedical Raman data [3].},
}

@Article{He2023dVRKbasedTO,
 author = {Changyan He and Robert H. Nguyen and E. Diller and James M. Drake and T. Looi},
 booktitle = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
 journal = {Proceedings of The 15th Hamlyn Symposium on Medical Robotics 2023},
 title = {dVRK-based teleoperation of a CTR robot with stereovision feedback for neurosurgery},
 year = {2023},
  semanticscholar = {https://www.semanticscholar.org/paper/1e93093119bb9106c0303e97200ee4ddada832c8},
  doi = {10.31256/hsmr2023.75},
  abstract = {Raman spectroscopy is a photonic modality defined as the inelastic backscattering of excitation coherent laser light. It is particularly beneficial for rapid tissue diagnosis in sensitive intraoperative environments like those involving the brain, due to its nonionizing potential, point-scanning capability, and highly-specific spectral fingerprint signatures that can characterize tissue pathology [1]. While Raman scattering is an inherently weak process, Surface-Enhanced Raman Spectroscopy (SERS), which is based on the use of metal nanostructure surfaces to amplify Raman signals, has become a compelling method for achieving highly specific Raman spectra with detection sensitivity comparable to conventional modalities such as fluorescence [2]. A unique plasmonics-active nanoplatform, SERS gold nanostars (GNS) have previously been designed in our group to accumulate preferentially in brain tumors [2]. Raman detection, when combined with machine learning and robotics, stands to enhance the diagnosis of ambiguous tissue during tumor resection surgery, with the potential to improve extent-of-resection and rapidly reconstruct the dynamic surgical field. Here we demonstrate preliminary results from the use of a SERS-based robotics platform to efficiently recreate a tumor embedded in healthy tissue, which is modeled here as a GNS-infused phantom. Transfer learning, specifically through use of the open-source RRUFF mineral database, is employed here to address the dearth of collected biomedical Raman data [3].},
}

@article{Puentes2023,
  title = {Pupillometry in telerobotic surgery: A comparative evaluation of algorithms for cognitive effort estimation},
  author = {Puentes, Paola Ruiz and Soberanis-Mukul, Roger D and Acar, Ayberk and Gupta, Iris and Bhowmick, Joyraj and Li, Yizhou and Ghazi, Ahmed and Kazanzides, Peter and Wu, Jie Ying and Unberath, Mathias},

  year = {2023},
  date = {2023-08-01},
  urldate = {2023-08-01},
  journal = {Medical Robotics},
  volume = {1},
  number = {3},
  pages = {1-8},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/520b0ad6d8babb5120f0a90d338a0ce981ddfec9},
  doi = {10.54844/mr.2023.0420},
  abstract = {Background: Eye gaze tracking and pupillometry are emerging topics in telerobotic surgery as it is believed that they will enable novel gaze-based interaction paradigms and provide insights into the user’s cognitive load (CL). Further, the successful integration of CL estimation into telerobotic systems is thought to catalyze the development of new human-computer interfaces for personalized assistance and training processes. However, this field is in its infancy, and identifying reliable gaze and pupil-tracking solutions in robotic surgery is still an area of ongoing research and high uncertainty. Methods: Considering the potential benefits of pupillometry-based CL assessments in telerobotic surgery, we seek to better understand the possibilities and limitations of contemporary pupillometry-based cognitive effort estimation algorithms in telerobotic surgery. To this end, we conducted a user study using the da Vinci Research Kit (dVRK) and performed two experiments where participants were asked to perform a series of N-Back tests, either while (i) idling or (ii) performing a peg transfer task. We then compare four contemporary CL estimation methods based on direct analysis of pupil diameter in the spatial and frequency domains. Results: We find that some methods can detect the presence of cognitive effort in simple scenarios (e.g., when the user is not performing any manual task), they fail to differentiate the different levels of CL. Similarly, when the manual peg transfer task is added, the reliability of all models is compromised, highlighting the necessity of more robust methods that consider different factors that complement the pupil diameter information. Conclusion: Our results offer a quantitative perspective of the limitations of the current solutions and highlight the necessity of developing tailored designs for the telerobotic surgery environment.},
}

@inproceedings{Ishida2023CommLoss,
  title = {Semi-Autonomous Assistance for Telesurgery Under Communication Loss},
  author = {Ishida, Hisashi and Munawar, Adnan and Taylor, Russell H and Kazanzides, Peter},

  year = {2023},
  date = {2023-10-01},
  urldate = {2023-10-01},
  booktitle = {IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)},
  pages = {8467-8473},
  address = {Detroit, MI},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/6807e3e9e112329f4bdd9b2cbe479b73c4e279f1},
  doi = {10.1109/IROS55552.2023.10341450},
  abstract = {Telesurgery has a clear potential for providing high-quality surgery to medically underserved areas like rural areas, battlefields, and spacecraft; nevertheless, effective methods to overcome unreliable communication systems are still lacking. Furthermore, it is not well understood how users react at the moment of communication loss and also during the loss. In this paper, we aim to analyze human response by proposing a telesurgery simulation framework that models an environment incorporating local and remote sites. Furthermore, this framework generates structural data for human behavior analysis and can provide different forms of assistance during the communication failure and at the communication recovery. We investigated three different types of assistance: User-centered, Robot-centered and Hybrid. A 12-person user-study was carried out using the proposed telesurgery simulation where participants completed a peg transfer task with random communication loss. The collected data was used to analyze the human response to a communication failure. The proposed Hybrid method reduced temporal demand with no increase in completion time compared to the baseline control method where users were unable to move the input device during the communication loss. The Hybrid method also significantly reduced both the task completion time and workload compared to the other two proposed methods (User-centered and Robot-centered).}
}

@inproceedings{ZhangISMR2023,
  title = {Velocity Control for the da Vinci Research Kit},
  author = {Zhang, Jintan and Kazanzides, Peter},

  year = {2023},
  date = {2023-04-01},
  urldate = {2023-04-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/7a41616900a1e57b910d37fd0265a330540b7351},
  doi = {10.1109/ISMR57123.2023.10130265},
  abstract = {The da Vinci Research Kit (dVRK) consists of open-source electronics and software that provides access to all levels of control but, until now, has relied primarily on an inner motor current control loop in analog hardware and an outer position control loop on the PC. In this work, we present a low-level velocity control loop, implemented on the FPGA, as an alternative to the PC-based (existing) position controller or (potential) velocity controller. The proposed velocity controller takes advantage of hardware-based measurement of the encoder period, which is inversely proportional to velocity. To avoid division on the FPGA, we implement closed loop control of the encoder period. Our implementation requires the controller gains to be adjusted based on the reference period; thus, the PC supplies both the reference period and the adjusted gains. We evaluate the proposed controller against a conventional implementation of a velocity control loop wrapped around the existing position control loop on the PC. The results demonstrate that the proposed controller yields improvements in tracking performance and disturbance rejection. The proposed velocity controller will be released open source to the dVRK community.}
}

@Conference{Nagy2022AutonomousPT,
 author = {T. D. Nagy and T. Haidegger},
 booktitle = {2022 IEEE 10th Jubilee International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)},
 journal = {2022 IEEE 10th Jubilee International Conference on Computational Cybernetics and Cyber-Medical Systems (ICCC)},
 pages = {000069-000076},
 title = {Autonomous Peg Transfer—a Gateway to Surgery 4.0},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/24e249b18b27e74886dd154706d8c6a0dad8b508},
  doi = {10.1109/iccc202255925.2022.9922860},
}

@INPROCEEDINGS{Long2022,
  author={Long, Yonghao and Cao, Jianfeng and Deguet, Anton and Taylor, Russell H. and Dou, Qi},

  booktitle={2022 International Symposium on Medical Robotics (ISMR)},
  title={Integrating Artificial Intelligence and Augmented Reality in Robotic Surgery: An Initial dVRK Study Using a Surgical Education Scenario},
  year={2022},
  volume={},
  number={},
  pages={1-8},
  keywords={Visualization;Three-dimensional displays;Education;Merging;Surgery;Trajectory;Artificial intelligence},
  doi={10.1109/ISMR48347.2022.9807505},
  ieeexplore = {https://ieeexplore.ieee.org/document/9807505},
  semanticscholar = {https://www.semanticscholar.org/paper/f79c918fdf59716a15feba1537e883d677976721},
  abstract = {Robot-assisted surgery has become progressively more and more popular due to its clinical advantages. In the meanwhile, the artificial intelligence and augmented reality in robotic surgery are developing rapidly and receive lots of attention. However, current methods have not discussed the coherent integration of AI and AR in robotic surgery. In this paper, we develop a novel system by seamlessly merging artificial intelligence module and augmented reality visualization to automatically generate the surgical guidance for robotic surgery education. Specifically, we first leverage reinforcement leaning to learn from expert demonstration and then generate 3D guidance trajectory, providing prior context information of the surgical procedure. Along with other information such as text hint, the 3D trajectory is then overlaid in the stereo view of dVRK, where the user can perceive the 3D guidance and learn the procedure. The proposed system is evaluated through a preliminary experiment on surgical education task peg-transfer, which proves its feasibility and potential as the next generation of robot-assisted surgery education solution.},
}

@ARTICLE{zruya2022new,
  author={Zruya, Or and Sharon, Yarden and Kossowsky, Hanna and Forni, Fulvio and Geftler, Alex and Nisky, Ilana},
  journal={IEEE Robotics and Automation Letters},
  title={A New Power Law Linking the Speed to the Geometry of Tool-Tip Orientation in Teleoperation of a Robot-Assisted Surgical System},
  year={2022},
  volume={7},
  number={4},
  pages={10762-10769},
research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/1bb4488df86b1e5ab4cee3938f3bb112ff7e211d},
  doi = {10.1101/2022.03.02.482648},
  abstract = {Fine manipulation is important in dexterous tasks executed via teleoperation, including in robot-assisted surgery. Discovering fundamental laws of human movement can benefit the design and control of teleoperated systems, and the training of their users. These laws are formulated as motor invariants, such as the well-studied speed-curvature power law. However, while the majority of these laws characterize translational movements, fine manipulation requires controlling the orientation of objects as well. This subject has received little attention in human motor control studies. Here, we report a new power law linking the speed to the geometry in orientation control – humans rotate their hands with an angular speed that is exponentially related to the local change in the direction of rotation. We demonstrate this law in teleoperated tasks performed by surgeons using surgical robotics research platforms. Additionally, we show that the law’s parameters change slowly with the surgeons’ training, and are robust within participants across task segments and repetitions. The fact that this power law is a robust motor invariant suggests that it may be an outcome of sensorimotor control. It also opens questions about the nature of this control and how it can be harnessed for better control of human-teleoperated robotic systems.},
}

@inproceedings{fan2022unity,
  title={A unity-based da vinci robot simulator for surgical training},
  author={Fan, Ke and Marzullo, Aldo and Pasini, Nicolo and Rota, Alberto and Pecorella, Matteo and Ferrigno, Giancarlo and De Momi, Elena},

  booktitle={2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)},
  pages={1--6},
  year={2022},
  organization={IEEE},
  research_field={TR},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/38632f61717336df2693a56e504fb0e7e2416e4b},
  doi = {10.1109/BioRob52689.2022.9925319},
  abstract = {The development of the Robot-Assisted Minimally Invasive Surgery (RAMIS) imposes an increasing demand for surgical training platforms, especially low-cost simulation-based surgical training through the creation of new open-source modules. For this goal, a da Vinci Surgical robot simulator based on Unity Physics Engine is developed. The simulator is integrated with da Vinci Research Kit (dVRK), robot kinematic models and multiple sensors. The Robot Operating System (ROS) interface is embedded for better integration with ROS based software components. The simulator can provide interactive information such as haptic feedback with master input devices. An application of a virtual fixture is implemented to test and verify the performance of the simulator. The results show that the simulator has high expansibility and support interactive training tasks well.},
}

@inproceedings{Pasini2022,
  title = {A virtual suturing task: proof of concept for awareness in autonomous camera motion},
  author = {Pasini, Nicolo and Mariani, Andrea and Munawar, Adnan and De Momi, Elena and Kazanzides, Peter},

  doi = {10.1109/IRC55401.2022.00073},
  year = {2022},
  date = {2022-12-01},
  urldate = {2022-12-01},
  booktitle = {IEEE Intl. Conf. on Robotic Computing (IRC)},
  pages = {376-382},
  address = {Naples, Italy},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/92829ce84e653e7c07452453ac526e8091a641bc},
  abstract = {Robot-assisted Minimally Invasive Surgery (MIS) requires the surgeon to alternatively control both the surgical instruments and the endoscopic camera, or to leave this burden to an assistant. This increases the cognitive load and interrupts the workflow of the operation. Camera motion automation has been examined in the literature to mitigate these aspects, but still lacks situation awareness, a key factor for camera navigation enhancement. This paper presents the development of a phase-specific camera motion automation, implemented in Virtual Reality (VR) during a suturing task. A user study involving 10 users was carried out using the master console of the da Vinci Research Kit. Each subject performed the suturing task undergoing both the proposed autonomous camera motion and the traditional manual camera control. Results show that the proposed system can reduce operational time, decreasing both the user's mental and physical demand. Situational awareness is shown to be fundamental in exploiting the benefits introduced by camera motion automation.},
}

@inproceedings{Ding2022,
  title = {CaRTS: Causality-driven robot tool segmentation from vision and kinematics data},
  author = {Ding, Hao and Zhang, Jintan and Kazanzides, Peter and Wu, Jie Ying and Unberath, Mathias},

  doi = {10.1007/978-3-031-16449-1_37},
  year = {2022},
  date = {2022-09-01},
  urldate = {2022-09-01},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  pages = {387-398},
  address = {Singapore},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/56ad522f775297d8b5fb63c960f0893d9352e224},
  abstract = {Vision-based segmentation of the robotic tool during robot-assisted surgery enables downstream applications, such as augmented reality feedback, while allowing for inaccuracies in robot kinematics. With the introduction of deep learning, many methods were presented to solve instrument segmentation directly and solely from images. While these approaches made remarkable progress on benchmark datasets, fundamental challenges pertaining to their robustness remain. We present CaRTS, a causality-driven robot tool segmentation algorithm, that is designed based on a complementary causal model of the robot tool segmentation task. Rather than directly inferring segmentation masks from observed images, CaRTS iteratively aligns tool models with image observations by updating the initially incorrect robot kinematic parameters through forward kinematics and differentiable rendering to optimize image feature similarity end-to-end. We benchmark CaRTS with competing techniques on both synthetic as well as real data from the dVRK, generated in precisely controlled scenarios to allow for counterfactual synthesis. On training-domain test data, CaRTS achieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when tested on counterfactually altered test data, exhibiting low brightness, smoke, blood, and altered background patterns. This compares favorably to Dice scores of 95.0 and 86.7, respectively, of the SOTA image-based method. Future work will involve accelerating CaRTS to achieve video framerate and estimating the impact occlusion has in practice. Despite these limitations, our results are promising: In addition to achieving high segmentation accuracy, CaRTS provides estimates of the true robot kinematics, which may benefit applications such as force estimation. Code is available at: https://github.com/hding2455/CaRTS},
}

@inproceedings{tagliabue2022deliberation,
  title={Deliberation in autonomous robotic surgery: a framework for handling anatomical uncertainty},
  author={Tagliabue, Eleonora and Meli, Daniele and Dall'Alba, Diego and Fiorini, Paolo},

  booktitle={2022 International Conference on Robotics and Automation (ICRA)},
  pages={11080--11086},
  year={2022},
  organization={IEEE},
  research_field={AU and SS},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/91dc2eaa204146dcaec3114f3670611997cb611e},
  doi = {10.1109/ICRA46639.2022.9811820},
  abstract = {Autonomous robotic surgery requires deliberation, i.e. the ability to plan and execute a task adapting to uncer-tain and dynamic environments. Uncertainty in the surgical domain is mainly related to the partial pre-operative knowledge about patient-specific anatomical properties. In this paper, we introduce a logic-based framework for surgical tasks with deliberative functions of monitoring and learning. The DE-liberative Framework for Robot-Assisted Surgery (DEFRAS) estimates a pre-operative patient-specific plan, and executes it while continuously measuring the applied force obtained from a biomechanical pre-operative model. Monitoring module compares this model with the actual situation reconstructed from sensors. In case of significant mismatch, the learning module is invoked to update the model, thus improving the estimate of the exerted force. DEFRAS is validated both in simulated and real environment with da Vinci Research Kit executing soft tissue retraction. Compared with state-of-the-art related works, the success rate of the task is improved while minimizing the interaction with the tissue to prevent unintentional damage.},
}

@ARTICLE{itzkovich2022generalization,
  author={Itzkovich, Danit and Sharon, Yarden and Jarc, Anthony and Refaely, Yael and Nisky, Ilana},
  journal={IEEE Journal of Biomedical and Health Informatics},
  title={Generalization of Deep Learning Gesture Classification in Robotic-Assisted Surgical Data: From Dry Lab to Clinical-Like Data},
  year={2022},
  volume={26},
  number={3},
  pages={1329-1340},
research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/6d0f631f865f16a6f1855e28716e17772cf75c25},
  doi = {10.1109/JBHI.2021.3117784},
  abstract = {Objective: Robotic-assisted minimally invasive surgery (RAMIS) became a common practice in modern medicine and is widely studied. Surgical procedures require prolonged and complex movements; therefore, classifying surgical gestures could be helpful to characterize surgeon performance. The public release of the JIGSAWS dataset facilitates the development of classification algorithms; however, it is not known how algorithms trained on dry-lab data generalize to real surgical situations. Methods: We trained a Long Short-Term Memory (LSTM) network for the classification of dry lab and clinical-like data into gestures. Results: We show that a network that was trained on the JIGSAWS data does not generalize well to other dry-lab data and to clinical-like data. Using rotation augmentation improves performance on dry-lab tasks, but fails to improve the performance on clinical-like data. However, using the same network architecture, adding the six joint angles of the patient-side manipulators (PSMs) features, and training the network on the clinical-like data together lead to notable improvement in the classification of the clinical-like data. Discussion: Using the JIGSAWS dataset alone is insufficient for training a gesture classification network for clinical data. However, it can be very informative for determining the architecture of the network, and with training on a small sample of clinical data, can lead to acceptable classification performance. Significance: Developing efficient algorithms for gesture classification in clinical surgical data is expected to advance understanding of surgeon sensorimotor control in RAMIS, the automation of surgical skill evaluation, and the automation of surgery.}
}

@Article{Dharmarajan2022AutomatingVS,
 author = {K. Dharmarajan and Will Panitch and Muyan Jiang and Kishore Srinivas and Baiyu Shi and Yahav Avigal and Huang Huang and Thomas Low and Danyal M. Fer and Ken Goldberg},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2023 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {6781-6788},
 title = {Automating Vascular Shunt Insertion with the dVRK Surgical Robot},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Zhang2022HumanRobotSC,
 author = {Dandan Zhang and Zicong Wu and Junhong Chen and Ruiqi Zhu and A. Munawar and Bo Xiao and Yuan Guan and Hang Su and Wuzhou Hong and Yao Guo and G. Fischer and Benny P. L. Lo and Guang Yang},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2022 International Conference on Robotics and Automation (ICRA)},
 pages = {7694-7700},
 title = {Human-Robot Shared Control for Surgical Robot Based on Context-Aware Sim-to-Real Adaptation},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Cui2022CaveatsOT,
 author = {Zejian Cui and João Cartucho and S. Giannarou and Ferdinando Rodriguez y Baena},
 booktitle = {IEEE robotics & automation magazine},
 journal = {IEEE Robotics & Automation Magazine},
 pages = {113-128},
 title = {Caveats on the First-Generation da Vinci Research Kit: Latent Technical Constraints and Essential Calibrations [Survey]},
 volume = {32},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/55c204f73459a88e25cff109d3d05da24d7ae004},
  doi = {10.1109/mra.2005.1577011}
}

@Article{Huang2022ASP,
 author = {Yisen Huang and Jian Li and Xue Zhang and Ke Xie and Jixiu Li and Yue Liu and C. Ng and P. Chiu and Zheng Li},
 booktitle = {IEEE Robotics and Automation Letters},
 journal = {IEEE Robotics and Automation Letters},
 pages = {1-1},
 title = {A Surgeon Preference-Guided Autonomous Instrument Tracking Method With a Robotic Flexible Endoscope Based on dVRK Platform},
 volume = {PP},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@Article{Lu2022TowardIA,
 author = {Bo Lu and Bin Li and Wei Chen and Yueming Jin and Zixu Zhao and Q. Dou and P. Heng and Yunhui Liu},
 booktitle = {IEEE Transactions on Automation Science and Engineering},
 journal = {IEEE Transactions on Automation Science and Engineering},
 pages = {3794-3808},
 title = {Toward Image-Guided Automated Suture Grasping Under Complex Environments: A Learning-Enabled and Optimization-Based Holistic Framework},
 volume = {19},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/6edf9e85d10f3342aae626dfa68583132834edaf},
  doi = {10.1109/tase.2018.2887129},
  abstract = {IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING is published by the IEEE Robotics and Automation Society. All members of the IEEE are eligible for membership in the Society and will receive this TRANSACTIONS upon payment of the annual Society membership fee of $9.00 plus an annual subscription fee of $50.00. For information on joining, write to the lEEE Service Center at the address below. Member copies of Transactions/Journals are for personal use only.},
}

@Article{Lu2022AUM,
 author = {Bo Lu and Bin Li and Q. Dou and Yunhui Liu},
 booktitle = {IEEE/ASME transactions on mechatronics},
 journal = {IEEE/ASME Transactions on Mechatronics},
 pages = {5124-5135},
 title = {A Unified Monocular Camera-Based and Pattern-Free Hand-to-Eye Calibration Algorithm for Surgical Robots With RCM Constraints},
 volume = {27},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/66f6e36718e68024806b8eeba8c9b1b7781dd08d},
  doi = {10.1109/tmech.2020.2967763},
  abstract = {This TransacTions is a joint publication of the IEEE and ASME. For membership and subscription information and pricing, please visit www.ieee.org/membership-catalog. Member copies of Transactions/Journals are for personal use only. IEEE/ASME TRANSACTIONS ON MECHATRONICS MANAGEMENT COMMITTEE Chairman: Xiaobo Tan (asME DscD), Treasurer: HirosHi FujiMoTo (iEs), Secretary: Kyujin cHo (ras), jun uEDa (asME DscD), aaron Dollar (ras), and MicHaEl ruDErMan (iEs)},
}

@Article{Haiderbhai2022RobustST,
 author = {Mustafa Haiderbhai and R. Gondokaryono and T. Looi and James M. Drake and L. Kahrs},
 booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},
 journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 pages = {3429-3435},
 title = {Robust Sim2Real Transfer with the da Vinci Research Kit: A Study On Camera, Lighting, and Physics Domain Randomization},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/dda867de88aff74f70fb9b2de0387297183afea5},
  doi = {10.1109/iros47612.2022.9981344},
  abstract = {The 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) will be held on October 23–27, 2022 in The Kyoto International Conference Center, Kyoto, Japan. The IROS is one of the largest and most impacting robotics research conferences worldwide. It provides an international forum for the international robotics research community to explore the frontier of science and technology in intelligent robots and smart machines. The theme of IROS 2022 is “Embodied AI for a Symbiotic Society’’. In addition to technical sessions and multi-media presentations, IROS conferences also hold panel discussions, forums, workshops, tutorials, exhibits, and technical tours to enrich the fruitful discussions among conference attendees.},
}

@Article{Zheng2022TowardCA,
 author = {Y. Zheng and Marzieh Ershad and A. M. Fey},
 booktitle = {International Conference on Biomedical Robotics and Biomechatronics},
 journal = {2022 9th IEEE RAS/EMBS International Conference for Biomedical Robotics and Biomechatronics (BioRob)},
 pages = {1-8},
 title = {Toward Correcting Anxious Movements Using Haptic Cues on the Da Vinci Surgical Robot},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/7ed8f84a096dec775b806ebaa524dcefd1803ffb},
  doi = {10.1109/TNSRE.2008.2008280}
}

@Article{Varier2022AMBFRLAR,
 author = {Vignesh Manoj Varier and Dhruv Kool Rajamani and Farid Tavakkolmoghaddam and A. Munawar and G. Fischer},
 booktitle = {International Symposium on Medical Robotics},
 journal = {2022 International Symposium on Medical Robotics (ISMR)},
 pages = {1-8},
 title = {AMBF-RL: A real-time simulation based Reinforcement Learning toolkit for Medical Robotics},
 year = {2022},
  semanticscholar = {https://www.semanticscholar.org/paper/4894fe427abf78532272ec0c0226e694c889290d},
  doi = {10.1109/ISMR67322.2025}
}

@inproceedings{Zhang2022,
  title = {Learning Based Estimation of 7 DOF Instrument and Grasping Forces on the da Vinci Research Kit},
  author = {Zhang, Jintan and Yilmaz, Nural and Tumerdem, Ugur and Kazanzides, Peter},

  doi = {10.1109/ISMR48347.2022.9807554},
  year = {2022},
  date = {2022-04-01},
  urldate = {2022-04-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/a0ca57669a8be1a2c4aa0ec5c882a1406ab30575},
  abstract = {Positron Emission Tomography (PET) enables functional imaging of deep brain structures, but the bulk and weight of current systems preclude their use during many natural human activities, such as locomotion. The proposed long-term solution is to construct a robotic system that can support an imaging system surrounding the subject’s head, and then move the system to accommodate natural motion. This requires a system to measure the motion of the head with respect to the imaging ring, for use by both the robotic system and the image reconstruction software. We report here the design and experimental evaluation of a parallel string encoder mechanism for sensing this motion. Our preliminary results indicate that the measurement system may achieve accuracy within 0.5mm, especially for small motions, with improved accuracy possible through kinematic calibration.},
}

@ARTICLE{kossowsky2022predicting,
  author={Kossowsky, Hanna and Nisky, Ilana},
  journal={IEEE Transactions on Medical Robotics and Bionics},
  title={Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks},
  year={2022},
  volume={4},
  number={2},
  pages={391-402},
research_field={TR},
  data_type={RI and KD}
}

@article{Yilmaz2022IJCARS,
  title = {Transfer of learned dynamics between different surgical robots and operative configurations},
  author = {Yilmaz, Nural and Zhang, Jintan and Kazanzides, Peter and Tumerdem, Ugur},

  doi = {10.1007/s11548-022-02601-7},
  year = {2022},
  date = {2022-04-01},
  urldate = {2022-04-01},
  journal = {Intl. Journal of Computer Assisted Radiology and Surgery (IJCARS)},
  volume = {17},
  pages = {903-910},
  publisher = {Springer},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/7d2dad9a09cf672fb249f61263a35433a6394662}
}

@Article{Özgüner2021VisuallyGN,
 author = {Orhan Özgüner and Thomas Shkurti and Su Lu and W. Newman and M. C. Çavuşoğlu},
 booktitle = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
 journal = {2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},
 pages = {242-248},
 title = {Visually Guided Needle Driving and Pull for Autonomous Suturing},
 year = {2021},
  semanticscholar = {https://www.semanticscholar.org/paper/13d72b70a8970800ebb7f261e07d50a711fff24a},
  doi = {10.1109/case49439.2021}
}

@inproceedings{Zhao2021,
  archivePrefix = {arXiv},
  arxivId = {2103.12988},
  author = {Zhao, Zixu and Jin, Yueming and Lu, Bo and Ng, Chi-Fai and Dou, Qi and Liu, Yun-Hui and Heng, Pheng-Ann},

  booktitle = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  eprint = {2103.12988},
  month = {mar},
  title = {{One to Many: Adaptive Instrument Segmentation via Meta Learning and Dynamic Online Adaptation in Robotic Surgical Video}},
  url = {http://arxiv.org/abs/2103.12988},
  year = {2021},
  research_field={IM},
  data_type={RI},
  semanticscholar = {https://www.semanticscholar.org/paper/967af3930617d1ce227fb769400b57c63dc71aec},
  doi = {10.1109/icra48506.2021}
}

@INPROCEEDINGS{9661536,
  author={Richter, Florian and Funk, Emily K. and Seo Park, Won and Orosco, Ryan K. and Yip, Michael C.},

  booktitle={2021 International Symposium on Medical Robotics (ISMR)},
  title={From Bench to Bedside: The First Live Robotic Surgery on the dVRK to Enable Remote Telesurgery with Motion Scaling},
  year={2021},
  volume={},
  number={},
  pages={1-7},
  keywords={Technological innovation;Translational research;Hospitals;Animals;Surgery;Robots},
  doi={10.1109/ISMR48346.2021.9661536},
  ieeexplore = {https://ieeexplore.ieee.org/abstract/document/9661536},
  research_field={TR},
  semanticscholar = {https://www.semanticscholar.org/paper/2ab58942e069700e6fe11dc4a3fdc15bf4c02412},
  abstract = {Innovations from surgical robotic research rarely translates to live surgery due to the significant difference between the lab and a live environment. Live environments require considerations that are often overlooked during early stages of research such as surgical staff, surgical procedure, and the challenges of working with live tissue. One such example is the da Vinci Research Kit (dVRK) which is used by over 40 robotics research groups and represents an open-sourced version of the da Vinci ® Surgical System. Despite dVRK being available for nearly a decade and the ideal candidate for translating research to practice on over 5,000 da Vinci ® Systems used in hospitals around the world, not one live surgery has been conducted with it. In this paper, we address the challenges, considerations, and solutions for translating surgical robotic research from bench-to-bedside. This is explained from the perspective of a remote telesurgery scenario where motion scaling solutions previously experimented in a lab setting are translated to a live pig surgery. This study presents results from the first ever use of a dVRK in a live animal and discusses how the surgical robotics community can approach translating their research to practice.},
}

@article{Attanasio2021,
  author = {Attanasio, Aleks and Alberti, Chiara and Scaglioni, Bruno and Marahrens, Nils and Frangi, Alejandro F. and Leonetti, Matteo and Biyani, Chandra Shekhar and {De Momi}, Elena and Valdastri, Pietro},

  doi = {10.1109/TMRB.2021.3054326},
  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  month = {feb},
  number = {1},
  title = {{A Comparative Study of Spatio-Temporal U-Nets for Tissue Segmentation in Surgical Robotics}},
  volume = {3},
  year = {2021},
  research_field={IM},
  data_type={RI},
  semanticscholar = {https://www.semanticscholar.org/paper/9bdcbb04224ab1a99bb8fcc629e2e3d3d088d990},
  abstract = {In surgical robotics, the ability to achieve high levels of autonomy is often limited by the complexity of the surgical scene. Autonomous interaction with soft tissues requires machines able to examine and understand the endoscopic video streams in real-time and identify the features of interest. In this work, we show the first example of spatio-temporal neural networks, based on the U-Net, aimed at segmenting soft tissues in endoscopic images. The networks, equipped with Long Short-Term Memory and Attention Gate cells, can extract the correlation between consecutive frames in an endoscopic video stream, thus enhancing the segmentation’s accuracy with respect to the standard U-Net. Initially, three configurations of the spatio-temporal layers are compared to select the best architecture. Afterwards, the parameters of the network are optimised and finally the results are compared with the standard U-Net. An accuracy of 83.77% ± 2.18% and a precision of 78.42% ± 7.38% are achieved by implementing both Long Short Term Memory (LSTM) convolutional layers and Attention Gate blocks. The results, although originated in the context of surgical tissue retraction, could benefit many autonomous tasks such as ablation, suturing and debridement.},
}

@article{Munawar2021,
  author = {Munawar, Adnan and Wu, Jie Ying and Taylor, Russell H. and Kazanzides, Peter and Fischer, Gregory S.},

  doi = {10.1109/LRA.2021.3062604},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  month = {apr},
  number = {2},
  title = {{A Framework for Customizable Multi-User Teleoperated Control}},
  volume = {6},
  year = {2021},
  research_field={HW},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/97e6cc631d17f7c7132e24b974b1cb3c7611cc87},
  abstract = {Traditional teleoperation (leader/follower) systems primarily focus on one operator controlling one remote robot, but as robots become ubiquitous, there is an increasing need for multiple operators, including autonomous agents, to collaboratively control multiple robots. However, existing teleoperation frameworks do not inherently support the variety of possible collaborations, such as multiple operators, each with an input device (leader), controlling a robot and camera or different degrees of freedom of a single robot (follower). The same concept applies to teleoperating robots in a simulation environment through physical input devices. In this letter, we extend our novel simulation framework that is capable of incorporating multiple input devices asynchronously with a real-time dynamic simulation to incorporate a customizable shared control. For this purpose, we have identified and implemented a sufficient set of coordinate frames to encapsulate the pairing of multiple leaders, followers and cameras in a shared asynchronous manner with force feedback. We demonstrate the utility of this framework in accelerating user training, ease of learning, and enhanced task completion times through shared control by a supervisor.},
}

@article{Cai2021,
  author = {Cai, Yuanpei and Choi, Pangfai and Hui, Chiu-Wai Vincent and Taylor, Russell and Au, Kwok Wai Samuel},

  doi = {10.1109/TMECH.2021.3058174},
  issn = {1083-4435},
  journal = {IEEE/ASME Transactions on Mechatronics},
  title = {{A Task Space Virtual Fixture Architecture for Tele-operated Surgical System with Slave Joint Limit Constraints}},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/f54d773b2bc0c8c58aec6b6c78df21c85ed9edc2},
  abstract = {Unintended motion is one of the major causes of intraoperative injuries in teleoperated surgeries. Due to the large workspace discrepancy between master and slave manipulators, the tip of the slave may deviate from the intended master command without prior notice, when the slave is moved beyond its joint limits. Conventional solutions such as the constrained-optimization-based virtual fixture (VF) method suffer from nonintuitive tip motion management and unnatural haptics in high-dimensional systems. To this end, we propose a task space virtual fixture (TSVF) architecture to systematically address those issues by forbidden-region VF design. It decomposes the high-dimensional task space into low-dimensional task subspaces according to its inherent topology. In each subspace, we design a human-centric TSVF geometry and controller to manage the tip behavior by exploiting the nonlinear kinematics mapping. This architecture builds a real-time TSVF system with natural and predictable haptics. To showcase this concept, we design and implement the proposed TSVF system for the state-of-the-art surgical system, da Vinci Research Kit. Simulations, experiments, and human-factor user study verify its effectiveness and intuitiveness. In the user study, our proposed TSVF system demonstrates the most easy-to-understand tip behavior and shows a significant positive effect over haptics likeability.}
}

@ARTICLE{DEttorre2021,
  author={D’Ettorre, Claudia and Mariani, Andrea and Stilli, Agostino and Rodriguez y Baena, Ferdinando and Valdastri, Pietro and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H. and Fischer, Gregory S. and DiMaio, Simon P. and Menciassi, Arianna and Stoyanov, Danail},

  journal={IEEE Robotics & Automation Magazine},
  title={Accelerating Surgical Robotics Research: A Review of 10 Years With the da Vinci Research Kit},
  year={2021},
  volume={28},
  number={4},
  pages={56-78},
  keywords={Robots;Surgery;Automation;Cameras;Robot vision systems;Instruments;Tools},
  doi={10.1109/MRA.2021.3101646},
  ieeexplore = {https://ieeexplore.ieee.org/document/9531355},
  research_field={RE}
}

@inproceedings{Fu2021,
  archivePrefix = {arXiv},
  arxivId = {2103.08119},
  author = {Fu, Guanhao and Azimi, Ehsan and Kazanzides, Peter},

  booktitle = {arXiv preprint},
  eprint = {2103.08119},
  title = {{Mobile Teleoperation: Evaluation of Wireless Wearable Sensing of the Operator's Arm Motion}},
  year = {2021},
  url = {https://arxiv.org/abs/2103.08119},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  doi = {10.1145/3613904.3642858},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@article{d2021autonomous,
  title={Autonomous pick-and-place using the dVRK},
  author={D’Ettorre, Claudia and Stilli, Agostino and Dwyer, George and Tran, Maxine and Stoyanov, Danail},

  journal={International Journal of Computer Assisted Radiology and Surgery},
  volume={16},
  number={7},
  pages={1141--1149},
  year={2021},
  publisher={Springer},
  doi = {10.1007/s11548-021-02397-y},
  semanticscholar = {https://www.semanticscholar.org/paper/3f2b07fd67179c4181a096f245248696a510db43},
  abstract = {Robotic-assisted partial nephrectomy (RAPN) is a tissue-preserving approach to treating renal cancer, where ultrasound (US) imaging is used for intra-operative identification of tumour margins and localisation of blood vessels. With the da Vinci Surgical System (Sunnyvale, CA), the US probe is inserted through an auxiliary access port, grasped by the robotic tool and moved over the surface of the kidney. Images from US probe are displayed separately to the surgical site video within the surgical console leaving the surgeon to interpret and co-registers information which is challenging and complicates the procedural workflow. We introduce a novel software architecture to support a hardware soft robotic rail designed to automate intra-operative US acquisition. As a preliminary step towards complete task automation, we automatically grasp the rail and position it on the tissue surface so that the surgeon is then able to manipulate manually the US probe along it. A preliminary clinical study, involving five surgeons, was carried out to evaluate the potential performance of the system. Results indicate that the proposed semi-autonomous approach reduced the time needed to complete a US scan compared to manual tele-operation. Procedural automation can be an important workflow enhancement functionality in future robotic surgery systems. We have shown a preliminary study on semi-autonomous US imaging, and this could support more efficient data acquisition.},
}

@inproceedings{meli2021autonomous,
  title={Autonomous tissue retraction with a biomechanically informed logic based framework},
  author={Meli, Daniele and Tagliabue, Eleonora and Dall’Alba, Diego and Fiorini, Paolo},

  booktitle={2021 International Symposium on Medical Robotics (ISMR)},
  pages={1--7},
  year={2021},
  organization={IEEE},
  research_field={AU and SS},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/d5db8a867ed9c47287580b7db2ec1b276031b143},
  doi = {10.1109/ismr48346.2021.9661573},
  abstract = {Autonomy in robot-assisted surgery is essential to reduce surgeons’ cognitive load and eventually improve the overall surgical outcome. A key requirement for autonomy in a safety-critical scenario as surgery lies in the generation of interpretable plans that rely on expert knowledge. Moreover, the Autonomous Robotic Surgical System (ARSS) must be able to reason on the dynamic and unpredictable anatomical environment, and quickly adapt the surgical plan in case of unexpected situations. In this paper, we present a modular Framework for Robot-Assisted Surgery (FRAS) in deformable anatomical environments. Our framework integrates a logic module for task-level interpretable reasoning, a biomechanical simulation that complements data from real sensors, and a situation awareness module for context interpretation. The framework performance is evaluated on simulated soft tissue retraction, a common surgical task to remove the tissue hiding a region of interest. Results show that the framework has the adaptability required to successfully accomplish the task, handling dynamic environmental conditions and possible failures, while guaranteeing the computational efficiency required in a real surgical scenario. The framework is made publicly available.},
}

@article{Wu2021,
  author = {Wu, Jie Ying and Tamhane, Aniruddha and Kazanzides, Peter and Unberath, Mathias},

  doi = {10.1007/s11548-021-02343-y},
  issn = {1861-6410},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  month = {mar},
  title = {{Cross-modal self-supervised representation learning for gesture and skill recognition in robotic surgery}},
  year = {2021},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/0093de51220636684b9bd511ddd3946a46836ef7}
}

@article{Tagliabue2021,
  author = {Tagliabue, Eleonora and Dall'Alba, Diego and Pfeiffer, Micha and Piccinelli, Marco and Marin, Riccardo and Castellani, Umberto and Speidel, Stefanie and Fiorini, Paolo},

  doi = {10.1109/LRA.2021.3060655},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  month = {apr},
  number = {2},
  title = {{Data-Driven Intra-Operative Estimation of Anatomical Attachments for Autonomous Tissue Dissection}},
  volume = {6},
  year = {2021},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/68c946bf268de5b03f05b356dbf97ef2c23916d2},
  abstract = {The execution of surgical tasks by an Autonomous Robotic System (ARS) requires an up-to-date model of the current surgical environment, which has to be deduced from measurements collected during task execution. In this work, we propose to automate tissue dissection tasks by introducing a convolutional neural network, called BA-Net, to predict the location of attachment points between adjacent tissues. BA-Net identifies the attachment areas from a single partial view of the deformed surface, without any a-priori knowledge about their location. The proposed method guarantees a very fast prediction time, which makes it ideal for intra-operative applications. Experimental validation is carried out on both simulated and real world phantom data of soft tissue manipulation performed with the da Vinci Research Kit (dVRK). The obtained results demonstrate that BA-Net provides robust predictions at varying geometric configurations, material properties, distributions of attachment points and grasping point locations. The estimation of attachment points provided by BA-Net improves the simulation of the anatomical environment where the system is acting, leading to a median simulation error below 5 mm in all the tested conditions. BA-Net can thus further support an ARS by providing a more robust test bench for the robotic actions intra-operatively, in particular when replanning is needed. The method and collected dataset are available at https://gitlab.com/altairLab/banet.},
}

@article{Yasin2021,
  title = {Evaluation of Hybrid Control and Palpation Assistance for Situational Awareness in Telemanipulated Task Execution},
  author = {Yasin, Rashid and Chalasani, Preetham and Zevallos, Nicolas and Shahbazi, Mahya and Li, Zhaoshuo and Deguet, Anton and Kazanzides, Peter and Choset, Howie and Taylor, Russell H and Simaan, Nabil},

  year = {2021},
  date = {2021-02-01},
  journal = {IEEE Trans. on Medical Robotics and Bionics},
  volume = {3},
  number = {1},
  pages = {31-43},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/e064660d114c01271825555a2f1224a9faef071c},
  doi = {10.1109/TMRB.2020.3042992},
  abstract = {The use of intelligent feedback modalities to control and react to interaction forces during surgical procedures is an important factor in enabling safe and precise surgery. We explore the use of a model-mediated telemanipulation framework to enhance a user’s situational awareness using assistive virtual fixtures and semi-automated task execution for safe and intuitive environment interaction during robotic laparoscopic surgery. The framework allows stiffness mapping with semi-autonomous excitation, hybrid position-force control, and model updates during soft geometry contact. A 24-person study was carried out at 3 sites in simulated ablation and palpation of phantom anatomy. Compared to methods lacking intelligent feedback and guidance, the proposed framework improved task execution metrics (force regulation, completion time, path-following error) and reduced user effort.}
}

@article{Gonzalez2021,
  author = {Gonzalez, Glebys T. and Kaur, Upinder and Rahma, Masudur and Venkatesh, Vishnunandan and Sanchez, Natalia and Hager, Gregory and Xue, Yexiang and Voyles, Richard and Wachs, Juan},

  journal = {Military Medicine},
  title = {{From the Dexterous Surgical Skill to the Battlefield - A Robotics Exploratory Study}},
  volume = {186},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/2a9bc46c1bcaf00950bd768038236a02b35986c6},
  doi = {10.1093/milmed/usaa253},
  abstract = {INTRODUCTION
Short response time is critical for future military medical operations in austere settings or remote areas. Such effective patient care at the point of injury can greatly benefit from the integration of semi-autonomous robotic systems. To achieve autonomy, robots would require massive libraries of maneuvers collected with the goal of training machine learning algorithms. Although this is attainable in controlled settings, obtaining surgical data in austere settings can be difficult. Hence, in this article, we present the Dexterous Surgical Skill (DESK) database for knowledge transfer between robots. The peg transfer task was selected as it is one of the six main tasks of laparoscopic training. In addition, we provide a machine learning framework to evaluate novel transfer learning methodologies on this database.


METHODS
A set of surgical gestures was collected for a peg transfer task, composed of seven atomic maneuvers referred to as surgemes. The collected Dexterous Surgical Skill dataset comprises a set of surgical robotic skills using the four robotic platforms: Taurus II, simulated Taurus II, YuMi, and the da Vinci Research Kit. Then, we explored two different learning scenarios: no-transfer and domain-transfer. In the no-transfer scenario, the training and testing data were obtained from the same domain; whereas in the domain-transfer scenario, the training data are a blend of simulated and real robot data, which are tested on a real robot.


RESULTS
Using simulation data to train the learning algorithms enhances the performance on the real robot where limited or no real data are available. The transfer model showed an accuracy of 81% for the YuMi robot when the ratio of real-tosimulated data were 22% to 78%. For the Taurus II and the da Vinci, the model showed an accuracy of 97.5% and 93%, respectively, training only with simulation data.


CONCLUSIONS
The results indicate that simulation can be used to augment training data to enhance the performance of learned models in real scenarios. This shows potential for the future use of surgical data from the operating room in deployable surgical robots in remote areas.},
}

@article{VanAmsterdam2021,
  author = {van Amsterdam, Beatrice and Clarkson, Matthew J. and Stoyanov, Danail},

  doi = {10.1109/TBME.2021.3054828},
  issn = {0018-9294},
  journal = {IEEE Transactions on Biomedical Engineering},
  month = {jun},
  number = {6},
  title = {{Gesture Recognition in Robotic Surgery: A Review}},
  volume = {68},
  year = {2021},
  research_field={RE},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/9bd76023b5dae31eb29eb34b414d6fe8799dfcc3},
  abstract = {Objective: Surgical activity recognition is a fundamental step in computer-assisted interventions. This paper reviews the state-of-the-art in methods for automatic recognition of fine-grained gestures in robotic surgery focusing on recent data-driven approaches and outlines the open questions and future research directions. Methods: An article search was performed on 5 bibliographic databases with the following search terms: robotic, robot-assisted, JIGSAWS, surgery, surgical, gesture, fine-grained, surgeme, action, trajectory, segmentation, recognition, parsing. Selected articles were classified based on the level of supervision required for training and divided into different groups representing major frameworks for time series analysis and data modelling. Results: A total of 52 articles were reviewed. The research field is showing rapid expansion, with the majority of articles published in the last 4 years. Deep-learning-based temporal models with discriminative feature extraction and multi-modal data integration have demonstrated promising results on small surgical datasets. Currently, unsupervised methods perform significantly less well than the supervised approaches. Conclusion: The development of large and diverse open-source datasets of annotated demonstrations is essential for development and validation of robust solutions for surgical gesture recognition. While new strategies for discriminative feature extraction and knowledge transfer, or unsupervised and semi-supervised approaches, can mitigate the need for data and labels, they have not yet been demonstrated to achieve comparable performance. Important future research directions include detection and forecast of gesture-specific errors and anomalies. Significance: This paper is a comprehensive and structured analysis of surgical gesture recognition methods aiming to summarize the status of this rapidly evolving field.},
}

@article{Gultekin2021,
  author = {G{\"{u}}ltekin, İsmail Burak and Karab{\"{u}}k, Emine and K{\"{o}}se, Mehmet Faruk and G{\"{u}}ltekin, İsmail Burak and Karab{\"{u}}k, Emine and K{\"{o}}se, Mehmet Faruk},

  doi = {10.4274/jtgga.galenos.2021.2020.0187},
  issn = {1309-0399},
  journal = {Journal of the Turkish-German Gynecological Association},
  month = {mar},
  number = {1},
  title = {{Hey Siri! Perform a type 3 hysterectomy. Please watch out for the ureter!” What is autonomous surgery and what are the latest developments?}},
  volume = {22},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/dd0f4deb0e21dedd80766908b57a7e709326e37f},
  abstract = {As a result of major advances in deep learning algorithms and computer processing power, there have been important developments in the fields of medicine and robotics. Although fully autonomous surgery systems where human impact will be minimized are still a long way off, systems with partial autonomy have gradually entered clinical use. In this review, articles on autonomous surgery classified and summarized, with the aim of informing the reader about questions such as “What is autonomic surgery?” and in which areas studies are progressing.},
}

@Article{Wilcox2021LearningTL,
 author = {Albert Wilcox and J. Kerr and Brijen Thananjeyan and Jeffrey Ichnowski and M. Hwang and Samuel Paradis and Danyal M. Fer and Ken Goldberg},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2022 International Conference on Robotics and Automation (ICRA)},
 pages = {9637-9643},
 title = {Learning to Localize, Grasp, and Hand Over Unmodified Surgical Needles},
 year = {2021}
}

@Article{Loschi2021AnOT,
 author = {Filippo Loschi and Nicola Piccinelli and D. Dall’Alba and R. Muradore and P. Fiorini and C. Secchi},
 booktitle = {IEEE International Conference on Robotics and Automation},
 journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
 pages = {12449-12455},
 title = {An Optimized Two-Layer Approach for Efficient and Robustly Stable Bilateral Teleoperation},
 year = {2021},
  semanticscholar = {https://www.semanticscholar.org/paper/6ab1777c05dcc70b1951934843fb3f7166a91f1f},
  doi = {10.7210/JRSJ.10.552},
}

@Article{Barragan2021SACHETSSC,
 author = {Juan Antonio Barragan and Daniela Chanci and Denny Yu and J. Wachs},
 booktitle = {IEEE International Symposium on Robot and Human Interactive Communication},
 journal = {2021 30th IEEE International Conference on Robot & Human Interactive Communication (RO-MAN)},
 pages = {1243-1248},
 title = {SACHETS: Semi-Autonomous Cognitive Hybrid Emergency Teleoperated Suction},
 year = {2021},
  semanticscholar = {https://www.semanticscholar.org/paper/f2063842143232152d8404b56ada4f66b955ec80}
}

@article{Lin2021,
  author = {Lin, Hongbin and Gao, Qian and Chu, Xiangyu and Dou, Qi and Deguet, Anton and Kazanzides, Peter and Au, K. W. Samuel},

  doi = {10.1109/LRA.2021.3062351},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  month = {apr},
  number = {2},
  title = {{Learning Deep Nets for Gravitational Dynamics With Unknown Disturbance Through Physical Knowledge Distillation: Initial Feasibility Study}},
  volume = {6},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/168c2a83ce165501c7fc79e921d82ab2edee55ea},
  abstract = {Learning high-performance deep neural networks for dynamic modeling of high Degree-Of-Freedom (DOF) robots remains challenging due to the sampling complexity. Typical unknown system disturbance caused by unmodeled dynamics (such as internal compliance, cables) further exacerbates the problem. In this letter, a novel framework characterized by both high data efficiency and disturbance-adapting capability is proposed to address the problem of modeling gravitational dynamics using deep nets in feedforward gravity compensation control for high-DOF master manipulators with unknown disturbance. In particular, Feedforward Deep Neural Networks (FDNNs) are learned from both prior knowledge of an existing analytical model and observation of the robot system by Knowledge Distillation (KD). Through extensive experiments in high-DOF master manipulators with significant disturbance, we show that our method surpasses a standard Learning-from-Scratch (LfS) approach in terms of data efficiency and disturbance adaptation. Our initial feasibility study has demonstrated the potential of outperforming the analytical teacher model as the training data increases.},
}

@article{Ghalamzan2021,
  archivePrefix = {arXiv},
  arxivId = {2103.07938},
  author = {Ghalamzan, Amir},

  eprint = {2103.07938},
  journal = {arXiv preprint},
  month = {mar},
  title = {{Learning needle insertion from sample task executions}},
  url = {http://arxiv.org/abs/2103.07938},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/c9f4a09d5e30010bd08f8e8f0f4e98705b827bad},
  abstract = {Automating a robotic task, e.g., robotic suturing can be very complex and time-consuming. Learning a task model to autonomously perform the task is invaluable making the technology, robotic surgery, accessible for a wider community. The data of robotic surgery can be easily logged where the collected data can be used to learn task models. This will result in reduced time and cost of robotic surgery in which a surgeon can supervise the robot operation or give high-level commands instead of low-level control of the tools. We present a data-set of needle insertion in soft tissue with two arms where Arm 1 inserts the needle into the tissue and Arm 2 actively manipulate the soft tissue to ensure the desired and actual exit points are the same. This is important in real-surgery because suturing without active manipulation of tissue may yield failure of the suturing as the stitch may not grip enough tissue to resist the force applied for the suturing. We present a needle insertion dataset including 60 successful trials recorded by 3 pair of stereo cameras. Moreover, we present Deep-robot Learning from Demonstrations that predicts the desired state of the robot at the time step after t (which the optimal action taken at t yields) by looking at the video of the past time steps, i.e. n step time history where N is the memory time window, of the task execution. The experimental results illustrate our proposed deep model architecture is outperforming the existing methods. Although the solution is not yet ready to be deployed on a real robot, the results indicate the possibility of future development for real robot deployment.}
}

@inproceedings{Hasan2021,
  author = {Hasan, S. M. Kamrul and Simon, Richard A. and Linte, Cristian A.},

  booktitle = {Medical Imaging 2021: Image-Guided Procedures, Robotic Interventions, and Modeling},
  doi = {10.1117/12.2580668},
  editor = {Linte, Cristian A. and Siewerdsen, Jeffrey H.},
  isbn = {9781510640252},
  month = {feb},
  publisher = {SPIE},
  title = {{Segmentation and removal of surgical instruments for background scene visualization from endoscopic/laparoscopic video}},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/6eb535a14bf9e57ff5b7cf5b10348ae1921830da},
  abstract = {Surgical tool segmentation is becoming imperative to provide detailed information during intra-operative execution. These tools can obscure surgeons' dexterity control due to narrow working space and visual field-of-view, which increases the risk of complications resulting from tissue injuries (e.g. tissue scars and tears). This paper demonstrates a novel application of segmenting and removing surgical instruments from laparoscopic/endoscopic video using digital inpainting algorithms. To segment the surgical instruments, we use a modified U-Net architecture (U-NetPlus) composed of a pre-trained VGG11 or VGG16 encoder and redesigned decoder. The decoder is modified by replacing the transposed convolution operation with an up-sampling operation based on nearest-neighbor (NN) interpolation. This modification removes the artifacts generated by the transposed convolution, and, furthermore, these new interpolation weights require no learning for the upsampling operation. The tool removal algorithms use the tool segmentation mask and either the instrument-free reference frames or previous instrument-containing frames to fill-in (i.e., inpaint) the instrument segmentation mask with the background tissue underneath. We have demonstrated the performance of the proposed surgical tool segmentation/removal algorithms on a robotic instrument dataset from the MICCAI 2015 EndoVis Challenge. We also showed successful performance of the tool removal algorithm from synthetically generated surgical instruments-containing videos obtained by embedding a moving surgical tool into surgical tool-free videos. Our application successfully segments and removes the surgical tool to unveil the background tissue view otherwise obstructed by the tool, producing visually comparable results to the ground truth.},
}

@inproceedings{Kohlgrueber2021,
  title = {Model-based Design and Digital Implementation to Improve Control of the da Vinci Research Kit Telerobotic Surgical System},
  author = {Kohlgrueber, Stefan and Kim, Yeongmi and Kazanzides, Peter},

  doi = {10.1109/ICRA48506.2021.9560842},
  year = {2021},
  date = {2021-06-01},
  urldate = {2021-06-01},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  pages = {12435-12441},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/2d9b66a6a63df458183b00e663bb476ff06238ce},
  abstract = {The da Vinci Research Kit (dVRK) was introduced in 2012 to provide an affordable, open-source platform for research in robotic minimally-invasive surgery. It provides access to all levels of control but, until now, has relied on an analog controller for the motor current, which cannot easily be customized to improve performance. This paper aims to implement the low-level control digitally and to improve the overall control performance. To enable model-based controller design, the system is first identified using measurements provided by the encoders and internal electronics. The digital current controller is then implemented on the existing field programmable gate array (FPGA). Experiments demonstrate that the new digital current controller yields superior performance compared to the original analog design. In addition, the identified system model is used to design an improved position controller that is also implemented on the FPGA and provides better trajectory tracking than the position controller currently implemented on the control PC. The comparison between simulation and measurement, for both the current and position control, verifies the validity of the model based on the system identification, enabling utilization for future adaptations. The improved low-level control enlarges the possibilities for more accurate operation and the achieved digital implementation enables researchers worldwide to easily adapt the low-level control in future versions of the dVRK.}
}

@article{Huang2021a,
  author = {Huang, Jingbin and Liu, Fei and Richter, Florian and Yip, Michael C.},

  journal = {arXiv preprint},
  title = {{Model-Predictive Control of Blood Suction for Surgical Hemostasis using Differentiable Fluid Simulations}},
  year = {2021},
  research_field={},
  data_type={}
}

@article{Huang2021b,
  archivePrefix = {arXiv},
  arxivId = {2102.01436},
  author = {Huang, Jingbin and Liu, Fei and Richter, Florian and Yip, Michael C.},

  eprint = {2102.01436},
  journal = {arXiv preprint},
  month = {feb},
  title = {{Model-Predictive Control of Blood Suction for Surgical Hemostasis using Differentiable Fluid Simulations}},
  year = {2021},
  url = {https://arxiv.org/abs/2102.01436},
  research_field={AU},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/ae7034217bb850a0989aef5745d1269e24cd7726},
  doi = {10.1109/ICRA48506.2021.9561624},
  abstract = {Recent developments in surgical robotics have led to new advancements in the automation of surgical sub-tasks such as suturing, soft tissue manipulation, tissue tensioning and cutting. However, integration of dynamics to optimize these control policies for the variety of scenes encountered in surgery remains unsolved. Towards this effort, we investigate the integration of differentiable fluid dynamics to optimizing a suction tool’s trajectory to clear the surgical field from blood as fast as possible. The fully differentiable fluid dynamics is integrated with a novel suction model for effective model predictive control of the tool. The differentiability of the fluid model is crucial because we utilize the gradients of the fluid states with respect to the suction tool position to optimize the trajectory. Through a series of experiments, we demonstrate how, by incorporating fluid models, the trajectories generated by our method can perform as good as or better than handcrafted human-intuitive suction policies. We also show that our method is adaptable and can work in different cavity conditions while using a single handcrafted strategy fails.},
}

@article{Caccianiga2021,
  author = {Caccianiga, Guido and Mariani, Andrea and {Galli de Paratesi}, Chiara and Menciassi, Arianna and {De Momi}, Elena},

  doi = {10.1109/LRA.2021.3063967},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  title = {{Multi-sensory Guidance and Feedback for Simulation-based Training in Robot Assisted Surgery: a Preliminary Comparison of Visual, Haptic, and Visuo-Haptic}},
  year = {2021},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a63da0819790a9cb542321bcbef5f63c16443701},
  abstract = {Nowadays, robot assisted surgery training relies more and more on computer-based simulation. However, the application of such training technologies is still limited to the early stages of practical training. To broaden the usefulness of simulators, multi-sensory feedback augmentation has been recently investigated. This study aims at combining initial predictive (guidance) and subsequent error-based (feedback) training augmentation in the visual and haptic domain. 32 participants performed 30 repetitions of a virtual reality task resembling needle-driving by using the surgeon console of the da Vinci Research Kit. These trainees were randomly and equally divided into four groups: one group had no training augmentation, while the other groups underwent visual, haptic and visuo-haptic augmentation, respectively. Results showed a significant improvement, initially introduced by guidance, in the task completion capabilities of all the experimental groups against control. In terms of accuracy, the experimental groups outperformed the control group at the end of training. Specifically, visual guidance and haptic feedback played a significant role in error reduction. Further investigations on long term learning could better delineate the optimal combination of guidance and feedback in these sensory domains.},
}

@article{NagyneElek2021,
  author = {{Nagyn{\'{e}} Elek}, Ren{\'{a}}ta and Haidegger, Tam{\'{a}}s},

  doi = {10.3390/s21082666},
  issn = {1424-8220},
  journal = {Sensors},
  month = {apr},
  number = {8},
  title = {{Non-Technical Skill Assessment and Mental Load Evaluation in Robot-Assisted Minimally Invasive Surgery}},
  volume = {21},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/326ea42ae3b6614b0926c24d83913717f13c16a7},
  abstract = {BACKGROUND: Sensor technologies and data collection practices are changing and improving quality metrics across various domains. Surgical skill assessment in Robot-Assisted Minimally Invasive Surgery (RAMIS) is essential for training and quality assurance. The mental workload on the surgeon (such as time criticality, task complexity, distractions) and non-technical surgical skills (including situational awareness, decision making, stress resilience, communication, leadership) may directly influence the clinical outcome of the surgery. METHODS: A literature search in PubMed, Scopus and PsycNet databases was conducted for relevant scientific publications. The standard PRISMA method was followed to filter the search results, including non-technical skill assessment and mental/cognitive load and workload estimation in RAMIS. Publications related to traditional manual Minimally Invasive Surgery were excluded, and also the usability studies on the surgical tools were not assessed. RESULTS: 50 relevant publications were identified for non-technical skill assessment and mental load and workload estimation in the domain of RAMIS. The identified assessment techniques ranged from self-rating questionnaires and expert ratings to autonomous techniques, citing their most important benefits and disadvantages. CONCLUSIONS: Despite the systematic research, only a limited number of articles was found, indicating that non-technical skill and mental load assessment in RAMIS is not a well-studied area. Workload assessment and soft skill measurement do not constitute part of the regular clinical training and practice yet. Meanwhile, the importance of the research domain is clear based on the publicly available surgical error statistics. Questionnaires and expert-rating techniques are widely employed in traditional surgical skill assessment; nevertheless, recent technological development in sensors and Internet of Things-type devices show that skill assessment approaches in RAMIS can be much more profound employing automated solutions. Measurements and especially big data type analysis may introduce more objectivity and transparency to this critical domain as well. SIGNIFICANCE: Non-technical skill assessment and mental load evaluation in Robot-Assisted Minimally Invasive Surgery is not a well-studied area yet; while the importance of this domain from the clinical outcome’s point of view is clearly indicated by the available surgical error statistics.},
}

@article{NagyneElek2021a,
  author = {{Nagyn{\'{e}} Elek}, Ren{\'{a}}ta and Haidegger, Tam{\'{a}}s},

  doi = {10.3390/s21082666},
  issn = {1424-8220},
  journal = {Sensors},
  month = {apr},
  number = {8},
  title = {{Non-Technical Skill Assessment and Mental Load Evaluation in Robot-Assisted Minimally Invasive Surgery}},
  volume = {21},
  year = {2021},
  research_field={TR},
  data_type={ED},
  semanticscholar = {https://www.semanticscholar.org/paper/326ea42ae3b6614b0926c24d83913717f13c16a7},
  abstract = {BACKGROUND: Sensor technologies and data collection practices are changing and improving quality metrics across various domains. Surgical skill assessment in Robot-Assisted Minimally Invasive Surgery (RAMIS) is essential for training and quality assurance. The mental workload on the surgeon (such as time criticality, task complexity, distractions) and non-technical surgical skills (including situational awareness, decision making, stress resilience, communication, leadership) may directly influence the clinical outcome of the surgery. METHODS: A literature search in PubMed, Scopus and PsycNet databases was conducted for relevant scientific publications. The standard PRISMA method was followed to filter the search results, including non-technical skill assessment and mental/cognitive load and workload estimation in RAMIS. Publications related to traditional manual Minimally Invasive Surgery were excluded, and also the usability studies on the surgical tools were not assessed. RESULTS: 50 relevant publications were identified for non-technical skill assessment and mental load and workload estimation in the domain of RAMIS. The identified assessment techniques ranged from self-rating questionnaires and expert ratings to autonomous techniques, citing their most important benefits and disadvantages. CONCLUSIONS: Despite the systematic research, only a limited number of articles was found, indicating that non-technical skill and mental load assessment in RAMIS is not a well-studied area. Workload assessment and soft skill measurement do not constitute part of the regular clinical training and practice yet. Meanwhile, the importance of the research domain is clear based on the publicly available surgical error statistics. Questionnaires and expert-rating techniques are widely employed in traditional surgical skill assessment; nevertheless, recent technological development in sensors and Internet of Things-type devices show that skill assessment approaches in RAMIS can be much more profound employing automated solutions. Measurements and especially big data type analysis may introduce more objectivity and transparency to this critical domain as well. SIGNIFICANCE: Non-technical skill assessment and mental load evaluation in Robot-Assisted Minimally Invasive Surgery is not a well-studied area yet; while the importance of this domain from the clinical outcome’s point of view is clearly indicated by the available surgical error statistics.},
}

@article{Di2021,
  archivePrefix = {arXiv},
  arxivId = {2104.06348},
  author = {Di, James and Xu, Mingwei and Das, Nikhil and Yip, Michael C.},

  eprint = {2104.06348},
  journal = {arXiv preprint},
  month = {apr},
  title = {{Optimal Multi-Manipulator Arm Placement for Maximal Dexterity during Robotics Surgery}},
  year = {2021},
  url = {https://arxiv.org/abs/2104.06348},
  research_field={HW},
  data_type={KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/f258f65e531564ad74c358405a99843efdc358fe},
  doi = {10.1109/ICRA48506.2021.9561570},
  abstract = {Robot arm placements are oftentimes a limitation in surgical preoperative procedures, relying on trained staff to evaluate and decide on the optimal positions for the arms. Given new and different patient anatomies, it can be challenging to make an informed choice, leading to more frequently colliding arms or limited manipulator workspaces. In this paper, we develop a method to generate the optimal manipulator base positions for the multi-port da Vinci surgical system that minimizes self-collision and environment-collision, and maximizes the surgeon’s reachability inside the patient. Scoring functions are defined for each criterion so that they may be optimized over. Since for multi-manipulator setups, a large number of free parameters are available to adjust the base positioning of each arm, a challenge becomes how one can expediently assess possible setups. We thus also propose methods that perform fast queries of each measure with the use of a proxy collision-checker. We then develop an optimization method to determine the optimal position using the scoring functions. We evaluate the optimality of the base positions for the robot arms on canonical trajectories, and show that the solution yielded by the optimization program can satisfy each criterion. The metrics and optimization strategy are generalizable to other surgical robotic platforms so that patient-side manipulator positioning may be optimized and solved.},
}

@article{Abdelaal2021,
  author = {Abdelaal, Alaa Eldin and Liu, Jordan and Hong, Nancy and Hager, Gregory D. and Salcudean, Septimiu E.},

  doi = {10.1109/LRA.2021.3060402},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  month = {apr},
  number = {2},
  title = {{Parallelism in Autonomous Robotic Surgery}},
  volume = {6},
  year = {2021},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/ba6b797b53310d8502512d19865322cc3be714d4},
  abstract = {Robots can perform multiple tasks in parallel. This work is about leveraging this capability in automating multilateral surgical subtasks. In particular, we explore, in a simulation study, the benefits of considering this parallelism capability in developing execution models for autonomous robotic surgery. We apply our work to two surgical subtask categories: (i) coupled-motion subtasks, where multiple robot arms share the same resources to perform the subtask, and (ii) decoupled-motion subtasks, where each robot arm executes its part of the task independently from the others. We propose and develop parallel execution models for the surgical debridement subtask, a representative of the first category, and the multi-throw suturing subtask, a representative of the second one. Comparing these parallel execution models to the state-of-the-art ones shows significant reductions in the subtasks completion time by at least 40%. In 20 trials, our results show that our proposed model for the surgical debridement subtask, that uses hierarchical concurrent state machines, provides a parallel execution framework that is efficient while greatly reducing collisions between the arms compared to a naive parallel execution model without coordination. We also show how applying parallelism can lead to execution models that go beyond the normal practice of human surgeons. We finally propose the notion of “automation for surgical manual execution” where we argue that autonomous robotic surgery research can be used as a tool for surgeons to discover novel manual execution models that can significantly improve their surgical practice.}
}

@ARTICLE{sharon2021rate,
  author={Sharon, Yarden and Jarc, Anthony M. and Lendvay, Thomas S. and Nisky, Ilana},
  journal={IEEE Transactions on Medical Robotics and Bionics},
  title={Rate of Orientation Change as a New Metric for Robot-Assisted and Open Surgical Skill Evaluation},
  year={2021},
  volume={3},
  number={2},
  pages={414-425},
  research_field={TR},
data_type={RI and KD}
}

@article{Sharon2021,
  author = {Sharon, Yarden and Jarc, Anthony M. and Lendvay, Thomas S. and Nisky, Ilana},

  doi = {10.1109/TMRB.2021.3073209},
  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  title = {{Rate of Orientation Change as a New Metric for Robot-Assisted and Open Surgical Skill Evaluation}},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/ffbeb4ffd52539d5c8a8e6ce9dae794da1672e63},
  abstract = {Surgeons’ technical skill directly impacts patient outcomes. To date, the angular motion of the instruments has been largely overlooked in objective skill evaluation. To fill this gap, we have developed metrics for surgical skill evaluation that are based on the orientation of surgical instruments. We tested our new metrics on two datasets with different conditions: (1) a dataset of experienced robotic surgeons and nonmedical users performing needle-driving on a dry lab model, and (2) a small dataset of suturing movements performed by surgeons training on a porcine model. We evaluated the performance of our new metrics (angular displacement and the rate of orientation change) alongside the performances of classical metrics (task time and path length). We calculated each metric on different segments of the movement. Our results highlighted the importance of segmentation rather than calculating the metrics on the entire movement. Our new metric, the rate of orientation change, showed statistically significant differences between experienced surgeons and nonmedical users / novice surgeons, which were consistent with the classical task time metric. The rate of orientation change captures technical aspects that are taught during surgeons’ training, and together with classical metrics can lead to a more comprehensive discrimination of skills.},
}

@article{Thananjeyan2021,
  author = {Thananjeyan, Brijen and Balakrishna, Ashwin and Nair, Suraj and Luo, Michael and Srinivasan, Krishnan and Hwang, Minho and Gonzalez, Joseph E. and Ibarz, Julian and Finn, Chelsea and Goldberg, Ken},

  doi = {10.1109/LRA.2021.3070252},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  month = {jul},
  number = {3},
  title = {{Recovery RL: Safe Reinforcement Learning With Learned Recovery Zones}},
  volume = {6},
  year = {2021},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/431dc05ac25510de6264084434254cca877f9ab3},
  abstract = {Safety remains a central obstacle preventing widespread use of RL in the real world: learning new tasks in uncertain environments requires extensive exploration, but safety requires limiting exploration. We propose Recovery RL, an algorithm which navigates this tradeoff by (1) leveraging offline data to learn about constraint violating zones before policy learning and (2) separating the goals of improving task performance and constraint satisfaction across two policies: a task policy that only optimizes the task reward and a recovery policy that guides the agent to safety when constraint violation is likely. We evaluate Recovery RL on 6 simulation domains, including two contact-rich manipulation tasks and an image-based navigation task, and an image-based obstacle avoidance task on a physical robot. We compare Recovery RL to 5 prior safe RL methods which jointly optimize for task performance and safety via constrained optimization or reward shaping and find that Recovery RL outperforms the next best prior method across all domains. Results suggest that Recovery RL trades off constraint violations and task successes 2–20 times more efficiently in simulation domains and 3 times more efficiently in physical experiments. See https://tinyurl.com/rl-recovery for videos and supplementary material.},
}

@inproceedings{WuISMR2021a,
  title = {Robot Force Estimation with Learned Intraoperative Correction},
  author = {Wu, Jie Ying and Yilmaz, Nural and Tumerdem, Ugur and Kazanzides, Peter},

  doi = {10.1109/ISMR48346.2021.9661568},
  year = {2021},
  date = {2021-11-01},
  urldate = {2021-11-01},
  booktitle = {IEEE Intl. Symp. on Medical Robotics (ISMR)},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/b15f228283837b5de87e3733ed3504c13b0af592},
  abstract = {Measurement of environment interaction forces during robotic minimally-invasive surgery would enable haptic feedback to the surgeon, thereby solving one long-standing limitation. Estimating this force from existing sensor data avoids the challenge of retrofitting systems with force sensors, but is difficult due to mechanical effects such as friction and compliance in the robot mechanism. We have previously shown that neural networks can be trained to estimate the internal robot joint torques, thereby enabling estimation of external forces on the da Vinci Research Kit (dVRK). In this work, we extend the method to estimate external Cartesian forces and torques, and also present a two-step approach to adapt to the specific surgical setup by compensating for forces due to the interactions between the instrument shaft and cannula seal and between the trocar and patient body. Experiments show that this approach provides estimates of external forces and torques within a mean root-mean-square error (RMSE) of 1.8N and 0.1Nm, respectively. Furthermore, the two-step approach can add as little as 5 minutes to the surgery setup time, with about 4 minutes to collect intraoperative training data and 1 minute to train the second-step network.}
}

@article{Richter2021,
  archivePrefix = {arXiv},
  arxivId = {2102.06235},
  author = {Richter, Florian and Lu, Jingpei and Orosco, Ryan K. and Yip, Michael C.},

  eprint = {2102.06235},
  journal = {arXiv preprint},
  month = {feb},
  title = {{Robotic Tool Tracking under Partially Visible Kinematic Chain: A Unified Approach}},
  url = {http://arxiv.org/abs/2102.06235},
  year = {2021},
  research_field={IM},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/76882ba81760f9c40bd1ad0195ad7ecb1f2f361d},
  doi = {10.1109/tro.2021.3111441},
  abstract = {Anytime a robot manipulator is controlled via visual feedback, the transformation between the robot and camera frame must be known. However, in the case where cameras can only capture a portion of the robot manipulator in order to better perceive the environment being interacted with, there is greater sensitivity to errors in calibration of the base-to-camera transform. A secondary source of uncertainty during robotic control are inaccuracies in joint angle measurements which can be caused by biases in positioning and complex transmission effects such as backlash and cable stretch. In this work, we bring together these two sets of unknown parameters into a unified problem formulation when the kinematic chain is partially visible in the camera view. We prove that these parameters are nonidentifiable implying that explicit estimation of them is infeasible. To overcome this, we derive a smaller set of parameters we call lumped error since it lumps together the errors of calibration and joint angle measurements. A particle filter method is presented and tested in simulation and on two real world robots to estimate the lumped error and show the efficiency of this parameter reduction.},
}

@inproceedings{xu2021surrol,
  title={Surrol: An open-source reinforcement learning centered and dvrk compatible platform for surgical robot learning},
  author={Xu, Jiaqi and Li, Bin and Lu, Bo and Liu, Yun-Hui and Dou, Qi and Heng, Pheng-Ann},

  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={1821--1828},
  year={2021},
  organization={IEEE},
  ieeexplore = {https://ieeexplore.ieee.org/abstract/document/9635867},
  research_field={AU and SS and TR},
  semanticscholar = {https://www.semanticscholar.org/paper/6f4ca4a3fd6071787083d067cf420e468e930d62},
  doi = {10.1109/IROS51168.2021.9635867},
  abstract = {Autonomous surgical execution relieves tedious routines and surgeon’s fatigue. Recent learning-based methods, especially reinforcement learning (RL) based methods, achieve promising performance for dexterous manipulation, which usually requires the simulation to collect data efficiently and reduce the hardware cost. The existing learning-based simulation platforms for medical robots suffer from limited scenarios and simplified physical interactions, which degrades the real-world performance of learned policies. In this work, we designed SurRoL, an RL-centered simulation platform for surgical robot learning compatible with the da Vinci Research Kit (dVRK). The designed SurRoL integrates a user-friendly RL library for algorithm development and a real-time physics engine, which is able to support more PSM/ECM scenarios and more realistic physical interactions. Ten learning-based surgical tasks are built in the platform, which are common in the real autonomous surgical execution. We evaluate SurRoL using RL algorithms in simulation, provide in-depth analysis, deploy the trained policies on the real dVRK, and show that our SurRoL achieves better transferability in the real world.},
}

@Article{Gao2021AHL,
 author = {Qian Gao and Ning Tan and Zhenglong Sun},
 booktitle = {The international journal of medical robotics + computer assisted surgery : MRCAS},
 journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
 title = {A hybrid learning‐based hysteresis compensation strategy for surgical robots},
 volume = {17},
 year = {2021},
  semanticscholar = {https://www.semanticscholar.org/paper/2e30f3b395f00a583cf20d167d80dc97e394ec95},
  doi = {10.1002/rcs.2489},
  abstract = {Computer‐assisted Surgery system (CAS) is an effective medical imaging simulation tool, which is widely used in preoperative planning of surgery. The objective of this study is to investigate the clinical application of CAS in pediatric mediastinal tumor resection.}
}

@article{meli2021unsupervised,
  title={Unsupervised identification of surgical robotic actions from small non-homogeneous datasets},
  author={Meli, Daniele and Fiorini, Paolo},

  journal={IEEE Robotics and Automation Letters},
  volume={6},
  number={4},
  pages={8205--8212},
  year={2021},
  publisher={IEEE},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a1e7eef4baf3a26d7128618c03238860309d242e},
  doi = {10.1109/LRA.2021.3104880},
  abstract = {Robot-assisted surgery is an established clinical practice. The automatic identification of surgical actions is needed for a range of applications, including performance assessment of trainees and surgical process modeling for autonomous execution and monitoring. However, supervised action identification is not feasible, due to the burden of manually annotating recordings of potentially complex and long surgical executions. Moreover, often few example executions of a surgical procedure can be recorded. This letter proposes a novel fast algorithm for unsupervised identification of surgical actions in a standard surgical training task, the ring transfer, executed with da Vinci Research Kit. Exploiting kinematic and semantic visual features automatically extracted from a very limited dataset of executions, we are able to significantly outperform state-of-the-art results on a dataset of non-expert executions (58% vs. 24% F1-score), and improve performance in the presence of noise, short actions and non-homogeneous workflows, i.e. non repetitive action sequences.},
}

@inproceedings{Varier2020,
  author = {Varier, Vignesh Manoj and Rajamani, Dhruv Kool and Goldfarb, Nathaniel and Tavakkolmoghaddam, Farid and Munawar, Adnan and Fischer, Gregory S},

  booktitle = {2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  doi = {10.1109/RO-MAN47096.2020.9223543},
  isbn = {978-1-7281-6075-7},
  publisher = {IEEE},
  title = {{Collaborative Suturing: A Reinforcement Learning Approach to Automate Hand-off Task in Suturing for Surgical Robots}},
  year = {2020},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/ea46d2f21b3bb2e43549b2e6bd210b8e5b162a99},
  abstract = {Over the past decade, Robot-Assisted Surgeries (RAS), have become more prevalent in facilitating successful operations. Of the various types of RAS, the domain of collaborative surgery has gained traction in medical research. Prominent examples include providing haptic feedback to sense tissue consistency, and automating sub-tasks during surgery such as cutting or needle hand-off - pulling and reorienting the needle after insertion during suturing. By fragmenting suturing into automated and manual tasks the surgeon could essentially control the process with one hand and also circumvent workspace restrictions imposed by the control interface present at the surgeon's side during the operation. This paper presents an exploration of a discrete reinforcement learning-based approach to automate the needle hand-off task. Users were asked to perform a simple running suture using the da Vinci Research Kit. The user trajectory was learnt by generating a sparse reward function and deriving an optimal policy using Q-learning. Trajectories obtained from three learnt policies were compared to the user defined trajectory. The results showed a root-mean-square error of [0.0044mm, 0.0027mm, 0.0020mm] in ℝ3. Additional trajectories from varying initial positions were produced from a single policy to simulate repeated passes of the hand-off task.}
}

@inproceedings{Su2020a,
  author = {Su, Yun-Hsuan and Munawar, Adnan and Deguet, Anton and Lewis, Andrew and Lindgren, Kyle and Li, Yangming and Taylor, Russell H. and Fischer, Gregory S. and Hannaford, Blake and Kazanzides, Peter},

  booktitle = {2020 Fourth IEEE International Conference on Robotic Computing (IRC)},
  doi = {10.1109/IRC.2020.00014},
  publisher = {IEEE},
  title = {{Collaborative Robotics Toolkit (CRTK): Open Software Framework for Surgical Robotics Research}},
  year = {2020},
  research_field={HW},
  data_type={SD},
  semanticscholar = {https://www.semanticscholar.org/paper/54551156fdb5ff1806491cc1a1369899ca2390b5},
  abstract = {Robot-assisted minimally invasive surgery has made a substantial impact in operating rooms over the past few decades with their high dexterity, small tool size, and impact on adoption of minimally invasive techniques. In recent years, intelligence and different levels of surgical robot autonomy have emerged thanks to the medical robotics endeavors at numerous academic institutions and leading surgical robot companies. To accelerate interaction within the research community and prevent repeated development, we propose the Collaborative Robotics Toolkit (CRTK), a common API for the RAVEN-II and da Vinci Research Kit (dVRK) - two open surgical robot platforms installed at more than 40 institutions worldwide. CRTK has broadened to include other robots and devices, including simulated robotic systems and industrial robots. This common API is a community software infrastructure for research and education in cutting edge human-robot collaborative areas such as semi-autonomous teleoperation and medical robotics. This paper presents the concepts, design details and the integration of CRTK with physical robot systems and simulation platforms.}
}

@inproceedings{Molnar2020,
  author = {Molnar, Cecilia and Nagy, Tamas D. and Elek, Renata Nagyne and Haidegger, Tamas},

  booktitle = {2020 IEEE 18th International Symposium on Intelligent Systems and Informatics (SISY)},
  doi = {10.1109/SISY50555.2020.9217086},
  isbn = {978-1-7281-7352-8},
  publisher = {IEEE},
  title = {{Visual servoing-based camera control for the da Vinci Surgical System}},
  year = {2020},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/bcb04ef6c47aa6eab1fa7aedd2435028778fe02e},
  abstract = {Minimally Invasive Surgery (MIS)–which is a very beneficial technique to the patient but can be challenging to the surgeon–includes endoscopic camera handling by an assistant (traditional MIS) or a robotic arm under the control of the operator (Robot-Assisted MIS, RAMIS). Since in the case of RAMIS the endoscopic image is the sole sensory input, it is essential to keep the surgical tools in the field-of-view of the camera for patient safety reasons. Based on the endoscopic images, the movement of the endoscope holder arm can be automated by visual servoing techniques, which can reduce the risk of medical error. In this paper, we propose a marker-based visual servoing technique for automated camera positioning in the case of RAMIS. The method was validated on the research-enhanced da Vinci Surgical System. The implemented method is available at: https://github.com/ABC-iRobotics/irob-saf/tree/visua1_servoing},
}

@inproceedings{Yilmaz2020,
  author = {Yilmaz, Nural and Wu, Jie Ying and Kazanzides, Peter and Tumerdem, Ugur},

  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA40945.2020.9197445},
  isbn = {978-1-7281-7395-5},
  publisher = {IEEE},
  title = {{Neural Network based Inverse Dynamics Identification and External Force Estimation on the da Vinci Research Kit}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e66026db91fbab60592001e2378e45887c7c6513},
  abstract = {Most current surgical robotic systems lack the ability to sense tool/tissue interaction forces, which motivates research in methods to estimate these forces from other available measurements, primarily joint torques. These methods require the internal joint torques, due to the robot inverse dynamics, to be subtracted from the measured joint torques. This paper presents the use of neural networks to estimate the inverse dynamics of the da Vinci surgical robot, which enables estimation of the external environment forces. Experiments with motions in free space demonstrate that the neural networks can estimate the internal joint torques within 10% normalized rootmean-square error (NRMSE), which outperforms model-based approaches in the literature. Comparison with an external force sensor shows that the method is able to estimate environment forces within about 10% NRMSE.}
}

@inproceedings{Munawar2020,
  author = {Munawar, Adnan and Srishankar, Nishan and Fischer, Gregory S},

  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {9781728173955},
  title = {{An Open-Source Framework for Rapid Development of Interactive Soft-Body Simulations for Real-Time Training}},
  year = {2020},
  research_field={TR},
  data_type={DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a88c357bf3045b5d438ee972f5bf8c284291c7f0},
  doi = {10.1109/icra40945.2020}
}

@article{Black2020,
  author = {Black, David G and Hosseinabadi, Amir H Hadi and Salcudean, Septimiu E},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{6-DOF Force Sensing for the Master Tool Manipulator of the da Vinci Surgical System}},
  volume = {5},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/dba4e60dc28bc9ce99428b026a0b71f3ea5e3141},
  doi = {10.1109/LRA.2020.2970944},
  abstract = {We integrated a force/torque sensor into the wrist of the Master Tool Manipulator (MTM) of the da Vinci Standard Surgical system. The added sensor can be used to monitor the surgeon interaction forces and to improve the haptic experience. The proposed mechanical design is expected to have little effect on the surgeon's operative experience and is simple and inexpensive to implement. We also developed a software package that allows for seamless integration of the force sensor into the da Vinci Research Kit (dVRK) and the Robot Operating System (ROS). The complete mechanical and electrical modifications, as well as the software packages are discussed. Two example applications of impedance control at the MTM and joystick control of the PSM are presented to demonstrate the successful integration of the sensor into the MTM and the interface to the dVRK.}
}

@article{Wu2020a,
  author = {Wu, Gloria C Y and Podolsky, Dale J and Looi, Thomas and Kahrs, Lueder A and Drake, James M and Forrest, Christopher R},

  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  publisher = {IEEE},
  title = {{A 3 mm Wristed Instrument for the da Vinci Robot: Setup, Characterization, and Phantom Tests for Cleft Palate Repair}},
  year = {2020},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/f208bae8d3e5a626986da2012388524473da71bc},
  doi = {10.1109/TMRB.2020.2977737},
  abstract = {Cleft palate is a congenital defect that affects approximately 1 in 800 births worldwide. A robotic approach for cleft palate repair is desired due to the potential ergonomic, vision and accessibility improvements in the small workspace of the oral cavity. This paper presents a 3 mm, 4-degree-of-freedom instrument for use with the widely available da Vinci system. The pin-jointed wrist design features cable guide channels in place of pulleys, significantly reducing the size of the wrist. Additionally, a novel cam mechanism minimizes any potential cable slack resulting from the lack of pulleys. The accuracy of the novel wrist was found to be 1.4° for motion in the same direction and the slack reducing capability of the cam mechanism was verified. Despite the 11.9° of hysteresis, there was no noticeable impact to teleoperation as the instrument was used to successfully perform suturing with the da Vinci Research Kit. Furthermore, contact between the tools and the oral aperture during surgery is significantly lower for the 3 mm instrument when compared to current 8 mm instruments. In summary, the novel instrument has the potential to better facilitate the adoption of a robotic approach.}
}

@article{Wang2020,
  author = {Wang, Ziheng and Kasman, Michael and Martinez, Marco and Rege, Robert and Zeh, Herbert and Scott, Danny and Fey, Ann Majewicz},

  issn = {2424-905X},
  journal = {Journal of Medical Robotics Research},
  publisher = {World Scientific Publishing Company},
  title = {{A Comparative Human-Centric Analysis of Virtual Reality and Dry Lab Training Tasks on the da Vinci Surgical Platform}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/bdbef3bb025ab0fcd5b69ee6d1a6e1ee9515f8f0},
  doi = {10.1142/s2424905x19420078},
  abstract = {There is a growing, widespread trend of adopting robot-assisted minimally invasive surgery (RMIS) in clinical care. Dry lab robot training and virtual reality simulation are commonly used to train surgical residents; however, it is unclear whether both types of training are equivalent or can be interchangeable and still achieve the same results in terms of training outcomes. In this paper, we take the first step in comparing the effects of physical and simulated surgical training tasks on human operator kinematics and physiological response to provide a richer understanding of exactly how the user interacts with the actual or simulated surgical robot. Four subjects, with expertise levels ranging from novice to expert surgeon, were recruited to perform three surgical tasks — Continuous Suture, Pick and Place, Tubes, with three repetitions — on two training platforms: (1) the da Vinci Si Skills Simulator and (2) da Vinci S robot, in a randomized order. We collected physiological response and kinematic movement data through body-worn sensors for a total of 72 individual experimental trials. A range of expertise was chosen for this experiment to wash out inherent differences based on expertise and only focus on inherent differences between the virtual reality and dry lab platforms. Our results show significant differences ([Formula: see text]-[Formula: see text]) between tasks done on the simulator and surgical robot. Specifically, robotic tasks resulted in significantly higher muscle activation and path length, and significantly lower economy of volume. The individual tasks also had significant differences in various kinematic and physiological metrics, leading to significant interaction effects between the task type and training platform. These results indicate that the presence of the robotic system may make surgical training tasks more difficult for the human operator. Thus, the potentially detrimental effects of virtual reality training alone are an important topic for future investigation.}
}

@inproceedings{Tran2020,
  title = {A Deep Learning Approach to Intrinsic Force Sensing on the da Vinci Surgical Robot},
  author = {Tran, Nam and Wu, Jie Ying and Deguet, Anton and Kazanzides, Peter},

  year = {2020},
  date = {2020-11-01},
  booktitle = {IEEE Intl. Conf. on Robotic Computing},
  pages = {25-32},
  keywords = {dvrk, machine-learning},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/a2fc15e7937f14e53a6c1077d5b0a10cdb234b9c},
  doi = {10.1109/IRC.2020.00011},
  abstract = {In robot-assisted minimally-invasive surgery (RAMIS), force estimation remains a challenging issue. We seek to estimate external forces based on available measurements from the joint encoders and motor currents. To this end, we propose a deep learning approach for end-to-end force estimation on the da Vinci Surgical System that is trained using data collected by both moving an instrument in free space and by palpating a tissue phantom that has an embedded force sensor for ground truth. The trained neural network provides reasonable force estimates (within about 1N to 2N precision given a full range of 10N) and is generalizable to other regions of the robot workspace. We further show that our proposed system can provide useful haptic feedback in a pilot study to differentiate stiffness in various tissue phantoms.}
}

@article{Lu2020,
  author = {Lu, Bo and Chen, Wei and Jin, Yue-Ming and Zhang, Dandan and Dou, Qi and Chu, Henry K. and Heng, Pheng-Ann and Liu, Yun-Hui},

  journal = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{A Learning-Driven Framework with Spatial Optimization For Surgical Suture Thread Reconstruction and Autonomous Grasping Under Multiple Topologies and Environmental Noises}},
  year = {2020},
  research_field={AU},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/efb94e389706c6307d42e70e33599771c9283f0c},
  doi = {10.1109/IROS45743.2020.9341445},
  abstract = {Surgical knot tying is one of the most fundamental and important procedures in surgery, and a high-quality knot can significantly benefit the postoperative recovery of the patient. However, a longtime operation may easily cause fatigue to surgeons, especially during the tedious wound closure task. In this paper, we present a vision-based method to automate the suture thread grasping, which is a sub-task in surgical knot tying and an intermediate step between the stitching and looping manipulations. To achieve this goal, the acquisition of a suture’s three-dimensional (3D) information is critical. Towards this objective, we adopt a transfer-learning strategy first to fine-tune a pre-trained model by learning the information from large legacy surgical data and images obtained by the onsite equipment. Thus, a robust suture segmentation can be achieved regardless of inherent environment noises. We further leverage a searching strategy with termination policies for a suture’s sequence inference based on the analysis of multiple topologies. Exact results of the pixel-level sequence along a suture can be obtained, and they can be further applied for a 3D shape reconstruction using our optimized shortest path approach. The grasping point considering the suturing criterion can be ultimately acquired. Experiments regarding the suture 2D segmentation and ordering sequence inference under environmental noises were extensively evaluated. Results related to the automated grasping operation were demonstrated by simulations in V-REP and by robot experiments using Universal Robot (UR) together with the da Vinci Research Kit (dVRK) adopting our learning-driven framework.},
}

@article{Brancadoro2020,
  author = {Brancadoro, Margherita and Dimitri, Mattia and Boushaki, Mohamed Nassim and Staderini, Fabio and Sinibaldi, Edoardo and Capineri, Lorenzo and Cianchi, Fabio and {Biffi Gentili}, Guido and Menciassi, Arianna},

  issn = {1364-5706},
  journal = {Minimally Invasive Therapy & Allied Technologies},
  publisher = {Taylor & Francis},
  title = {{A novel microwave tool for robotic liver resection in minimally invasive surgery}},
  year = {2020},
  research_field={HW},
  data_type={KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/9365d4896bf62f8f21292c356e6c0ccf1fe37906},
  doi = {10.1080/13645706.2020.1749083},
  abstract = {Abstract Introduction During the last two decades, many surgical procedures have evolved from open surgery to minimally invasive surgery (MIS). This limited invasiveness has motivated the development of robotic assistance platforms to obtain better surgical outcomes. Nowadays, the da Vinci robot is a commercial tele-robotic platform widely used for different surgical applications. Material and methods In this work, the da Vinci Research Kit (dVRK), namely the research version of the da Vinci, is used to manipulate a novel microwave device in a teleoperation scenario. The dVRK provides an open source platform, so that the novel microwave tool, dedicated to prevention bleeding during hepatic resection surgery, is mechanically integrated on the slave side, while the software interface is adapted in order to correctly control tool pose. Tool integration is validated through in-vitro and ex-vivo tests performed by expert surgeons, meanwhile the coagulative efficacy of the developed tool in a perfused liver model was proved in in-vivo tests. Results and conclusions An innovative microwave tool for liver robotic resection has been realized and integrated into a surgical robot. The tool can be easily operated through the dVRK without limiting the intuitive and friendly use, and thus easily reaching the hemostasis of vessels.}
}

@article{Zhang2020a,
  author = {Zhang, Dandan and Liu, Jindong and Gao, Anzhu and Yang, Guang-Zhong},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{An Ergonomic Shared Workspace Analysis Framework for the Optimal Placement of a Compact Master Control Console}},
  volume = {5},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e984a36b39cf76efe10122d38442766517cbcbad},
  doi = {10.1109/LRA.2020.2974428},
  abstract = {Master-Slave control is commonly used for Robot-Assisted Minimally Invasive Surgery (RAMIS). The configuration, as well as the placement of the master manipulators, can influence the remote control performance. An ergonomic shared workspace analysis framework is proposed in this letter. Combined with the workspace of the master manipulators and the human arms, the human-robot interaction workspace can be generated. The optimal master robot placement can be determined based on three criteria: 1) interaction workspace volume, 2) interaction workspace quality, and 3) intuitiveness for slave robot control. Experimental verification of the platform is conducted on a da Vinci Research Kit (dVRK). An in-house compact master manipulator (Hamlyn CRM) is used as the master robot and the da Vinci robot is used as the slave robot. Comparisons are made between with and without using design optimization to validate the effectiveness of the ergonomic shared workspace analysis technique. Results indicate that the proposed ergonomic shared workspace analysis can improve the performance of teleoperation in terms of task completion time and the number of clutching required during operation.}
}

@article{Caccianiga2020,
  author = {Caccianiga, Guido and Mariani, Andrea and {De Momi}, Elena and Cantarero, Gabriela and Brown, Jeremy D},

  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  publisher = {IEEE},
  title = {{An Evaluation of Inanimate and Virtual Reality Training for Psychomotor Skill Development in Robot-Assisted Minimally Invasive Surgery}},
  year = {2020},
  research_field={TR},
  data_type={KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/afbb60612267eddc0e81a7fbb633bc099744eae6},
  doi = {10.1109/TMRB.2020.2990692},
  abstract = {Robot-assisted minimally invasive surgery (RAMIS) is gaining widespread adoption in many surgical specialties, despite the lack of a standardized training curriculum. Current training approaches rely heavily on virtual reality simulators, in particular for basic psychomotor and visuomotor skill development. It is not clear, however, whether training in virtual reality is equivalent to inanimate model training. In this manuscript, we seek to compare virtual reality training to inanimate model training, with regard to skill learning and skill transfer. Using a custom-developed needle-driving training task with inanimate and virtual analogs, we investigated the extent to which N=18 participants improved their skill on a given platform post-training, and transferred that skill to the opposite platform. Results indicate that the two approaches are not equivalent, with more salient skill transfer after inanimate training than virtual training. These findings support the claim that training with real physical models is the gold standard, and suggest more inanimate model training be incorporated into training curricula for early psychomotor skill development.}
}

@article{Mariani2020a,
  author = {Mariani, Andrea and Colaci, Giorgia and {Da Col}, Tommaso and Sanna, Nicole and Vendrame, Eleonora and Menciassi, Arianna and {De Momi}, Elena},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{An Experimental Comparison Towards Autonomous Camera Navigation to Optimize Training in Robot Assisted Surgery}},
  volume = {5},
  year = {2020},
  research_field={TR},
  data_type={KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/3c6bff93a4542c116174b429df3977eca1913267},
  doi = {10.1109/LRA.2020.2965067},
  abstract = {Robot-Assisted Surgery enhances vision and it can restore depth perception, but it introduces the need for learning how to tele-operatively control both the surgical tools and the endoscope. Together with the complexity of selecting the optimal viewpoint to carry out the procedure, this requires distinct training. This work proposes an autonomous camera navigation during the initial stages of training in order to optimize the learning of these skills. A user study involving 26 novice participants was carried out using the master console of the da Vinci Research Kit and a virtual reality training environment. The subjects were randomly divided into two groups: the control group that manually controlled the camera as in the current practice and the experimental group that underwent the autonomous navigation. After training, the time-accuracy metrics of the users who underwent autonomous camera navigation were significantly higher with respect to the control group. Additionally, autonomous camera navigation seemed to be capable to provide an imprinting about endoscope management.}
}

@article{Fontanelli2020,
  author = {Fontanelli, Giuseppe Andrea and Buonocore, Luca Rosario and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},

  issn = {1083-4435},
  journal = {IEEE/ASME Transactions on Mechatronics},
  publisher = {IEEE},
  title = {{An External Force Sensing System for Minimally Invasive Robotic Surgery}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/1963a7371ab606afa050269828cda11cb1790eb4},
  doi = {10.1109/TMECH.2020.2979027},
  abstract = {Minimally invasive robotic surgery (MIRS) has revolutionized surgical procedures. However, compared to classic laparoscopy, the surgeon must rely only on visual perception because of the lack of force feedback. In this article, a new noninvasive force feedback system is proposed and evaluated. Extending the work by Fontanelli (2017), where preliminary results were presented, a solution based on a novel force sensor placed in the terminal part of the trocar is shown in detail. With respect to the state of the art, our system allows measuring the interaction forces between the surgical instrument and the environment inside the patient's body without any changes to the instrument structure and with full adaptability to different robotic platforms and surgical tools. Using a commercial force-torque sensor as ground truth, the static and dynamic characterization of the sensor is provided together with an extensive experimental validation. Finally, a simple and intuitive application of the proposed sensing system in a realistic surgical scenario is presented.}
}

@inproceedings{Hashempour2020a,
  archivePrefix = {arXiv},
  arxivId = {2012.02458},
  author = {Hashempour, Hamidreza and Nazari, Kiyanoush and Zhong, Fangxun and Esfahani, Amir Masoud Ghalamzan},

  booktitle = {arXiv},
  eprint = {2012.02458},
  issn = {23318422},
  title = {{A data-set of piercing needle through deformable objects for deep learning from demonstrations}},
  year = {2020},
  url = {https://arxiv.org/abs/2012.02458},
  research_field={AU},
  data_type={KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/f4327b978dec52f16b089c222c43543f8ecf4717},
  doi = {10.5860/choice.189890},
  abstract = {The innovative preprint repository, arXiv, was created in the early 1990s to improve access to scientific research. arXiv contains millions of Open Access articles in physics, mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering
 and systems science, and economics. All articles are available for free download on the open web. Often research findings are available on arXiv before they are published in a peer-reviewed journal. arXiv relies on a collaborative support business model where institutions that most heavily
 utilize arXiv contribute financially. Support also comes from Cornell University and the Simons Foundation.}
}

@inproceedings{Alambeigi2020,
  author = {Alambeigi, Farshid and Wang, Zerui and Liu, Yun-Hui and Taylor, Russell H. and Armand, Mehran},

  booktitle = {arXiv preprint},
  title = {{A Versatile Data-Driven Framework for Model-Independent Control of Continuum Manipulators Interacting With Obstructed Environments With Unknown Geometry and Stiffness}},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  doi = {10.1145/3613904.3642858},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@inproceedings{Paradis2020a,
  author = {Paradis, Samuel and Hwang, Minho and Thananjeyan, Brijen and Ichnowski, Jeffrey and Seita, Daniel and Fer, Danyal and Low, Thomas and Gonzalez, Joseph E. and Goldberg, Ken},

  booktitle = {arXiv preprint},
  title = {{Intermittent Visual Servoing: Efficiently Learning Policies Robust to Instrument Changes for High-precision Surgical Manipulation}},
  year = {2020},
  research_field={AU},
  data_type={KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  doi = {10.1145/3613904.3642858},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@article{Richter2020,
  author = {Richter, Florian and Shen, Shihao and Liu, Fei and Huang, Jingbin and Funk, Emily K. and Orosco, Ryan K. and Yip, Michael C.},

  journal = {arXiv preprint},
  title = {{Autonomous Robotic Suction to Clear the Surgical Field for Hemostasis using Image-based Blood Flow Detection}},
  year = {2020},
  research_field={AU},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/57a0705beed92d510701b2d7c8032111709897d2},
  doi = {10.1109/LRA.2021.3056057},
  abstract = {Autonomous robotic surgery has seen significant progression over the last decade with the aims of reducing surgeon fatigue, improving procedural consistency, and perhaps one day take over surgery itself. However, automation has not been applied to the critical surgical task of controlling tissue and blood vessel bleeding–known as hemostasis. The task of hemostasis covers a spectrum of bleeding sources and a range of blood velocity, trajectory, and volume. In an extreme case, an un-controlled blood vessel fills the surgical field with flowing blood. In this work, we present the first, automated solution for hemostasis through development of a novel probabilistic blood flow detection algorithm and a trajectory generation technique that guides autonomous suction tools towards pooling blood. The blood flow detection algorithm is tested in both simulated scenes and in a real-life trauma scenario involving a hemorrhage that occurred during thyroidectomy. The complete solution is tested in a physical lab setting with the da Vinci Research Kit (dVRK) and a simulated surgical cavity for blood to flow through. The results show that our automated solution has accurate detection, a fast reaction time, and effective removal of the flowing blood. Therefore, the proposed methods are powerful tools to clearing the surgical field which can be followed by either a surgeon or future robotic automation developments to close the vessel rupture.},
}

@article{Zhan2020,
  author = {Zhan, Jian and Cartucho, Joao and Giannarou, Stamatia},

  journal = {arXiv preprint},
  title = {{Autonomous Tissue Scanning under Free-Form Motion for Intraoperative Tissue Characterisation}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/84b35a7c6a35bbd284a0de4aa414244f1f161b4e},
  doi = {10.1109/ICRA40945.2020.9197294},
  abstract = {In Minimally Invasive Surgery (MIS), tissue scanning with imaging probes is required for subsurface visualisation to characterise the state of the tissue. However, scanning of large tissue surfaces in the presence of motion is a challenging task for the surgeon. Recently, robot-assisted local tissue scanning has been investigated for motion stabilisation of imaging probes to facilitate the capturing of good quality images and reduce the surgeon’s cognitive load. Nonetheless, these approaches require the tissue surface to be static or translating with periodic motion. To eliminate these assumptions, we propose a visual servoing framework for autonomous tissue scanning, able to deal with free-form tissue motion. The 3D structure of the surgical scene is recovered, and a feature-based method is proposed to estimate the motion of the tissue in real-time. The desired scanning trajectory is manually defined on a reference frame and continuously updated using projective geometry to follow the tissue motion and control the movement of the robotic arm. The advantage of the proposed method is that it does not require the learning of the tissue motion prior to scanning and can deal with free-form motion. We deployed this framework on the da Vinci®surgical robot using the da Vinci Research Kit (dVRK) for Ultrasound tissue scanning. Our framework can be easily extended to other probe-based imaging modalities.},
}

@incollection{Taylor2020,
  author = {Taylor, Russell H and Kazanzides, Peter and Fischer, Gregory S and Simaan, Nabil},

  booktitle = {Biomedical Information Technology},
  publisher = {Elsevier},
  title = {{Medical robotics and computer-integrated interventional medicine}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/aec2473601284a3e8b9dde86539af318fa201ac3}
}

@article{Ozguner2020,
  author = {Ozguner, Orhan and Shkurti, Thomas and Huang, Siqi and Hao, Ran and Jackson, Russell C. and Newman, Wyatt S. and Cavusoglu, M. Cenk},

  doi = {10.1109/TASE.2020.2986503},
  issn = {1545-5955},
  journal = {IEEE Transactions on Automation Science and Engineering},
  number = {4},
  title = {{Camera-Robot Calibration for the Da Vinci Robotic Surgery System}},
  volume = {17},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/b703b865ad8090966883596da80b2198f913c88c},
  abstract = {The development of autonomous or semiautonomous surgical robots stands to improve the performance of existing teleoperated equipment but requires fine hand-eye calibration between the free-moving endoscopic camera and patient-side manipulator arms (PSMs). A novel method of solving this problem for the da Vinci robotic surgical system and kinematically similar systems is presented. First, a series of image-processing and optical-tracking operations are performed to compute the coordinate transformation between the endoscopic camera view frame and an optical-tracking marker permanently affixed to the camera body. Then, the kinematic properties of the PSM are exploited to compute the coordinate transformation between the kinematic base frame of the PSM and an optical marker permanently affixed thereto. Using these transformations, it is then possible to compute the spatial relationship between the PSM and the endoscopic camera using only one tracker snapshot of the two markers. The effectiveness of this calibration is demonstrated by successfully guiding the PSM end-effector to points of interest identified through the camera. Additional tests on a surgical task, namely, grasping a surgical needle, are also performed to validate the proposed method. The resulting visually guided robot positioning accuracy is better than the earlier hand-eye calibration results reported in the literature for the da Vinci system while supporting the intraoperative update of the calibration and requiring only devices that are already commonly used in the surgical environment. Note to Practitioners—The problem of hand-eye calibration for the da Vinci robotic surgical system and kinematically similar systems is addressed in this article. Existing approaches have insufficient accuracy to automate low-level surgical subtasks and often require external patterns or subjective human intervention, none of which are applicable to practical robotic minimally invasive surgery (RMIS) scenarios. This article breaks down the calibration procedure into systematic steps to reduce error accumulation. Most of the time-consuming steps are performed offline, allowing them to be retained between movements. Each time the passive joints of the manipulator or the endoscope move, all that needs to be done is to refresh the transformation between the fixed markers. This key idea enables intraoperative updates of the hand-eye calibration to be performed online without sacrificing precision. The calibration method presented here demonstrates that the achieved accuracy is sufficient for automating basic surgical manipulation tasks, such as grasping a suturing needle. The hand-eye calibration will be incorporated into a visually guided manipulation framework to perform high-precision autonomous surgical tasks.},
}

@article{Orosco2020,
  author = {Orosco, Ryan K and Lurie, Benjamin and Matsuzaki, Tokio and Funk, Emily K and Divi, Vasu and Holsinger, F Christopher and Hong, Steven and Richter, Florian and Das, Nikhil and Yip, Michael},

  doi = {10.1007/s00464-020-07681-7},
  isbn = {0046402007681},
  issn = {1432-2218},
  journal = {Surgical Endoscopy},
  publisher = {Springer US},
  title = {{Compensatory motion scaling for time ‑ delayed robotic surgery}},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/30a554a02e651a215c1f816b32efa9538d9731ff}
}

@article{Eslamian2020,
  author = {Eslamian, Shahab and Reisner, Luke A and Pandya, Abhilash K},

  issn = {1478-5951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {2},
  publisher = {Wiley Online Library},
  title = {{Development and evaluation of an autonomous camera control algorithm on the da Vinci Surgical System}},
  volume = {16},
  year = {2020},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a0f43c66bf5f795a1d01f82e0c76513bf6650b16},
  doi = {10.1002/rcs.2036},
  abstract = {Manual control of the camera arm in telerobotic surgical systems requires the surgeon to repeatedly interrupt the flow of the surgery. During surgery, there are instances when one or even both tools can drift out of the field of view. These issues may lead to increased workload and potential errors.}
}

@article{Hwang2020,
  author = {Hwang, Minho and Thananjeyan, Brijen and Paradis, Samuel and Seita, Daniel and Ichnowski, Jeffrey and Fer, Danyal and Low, Thomas and Goldberg, Ken},

  doi = {10.1109/LRA.2020.3010746},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {4},
  title = {{Efficiently Calibrating Cable-Driven Surgical Robots With RGBD Fiducial Sensing and Recurrent Neural Networks}},
  volume = {5},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/1e6bee245e605a039aa6d2d80f1477b61fcbaec7},
  abstract = {Automation of surgical subtasks using cable-driven robotic surgical assistants (RSAs) such as Intuitive Surgical's da Vinci Research Kit (dVRK) is challenging due to imprecision in control from cable-related effects such as cable stretching and hysteresis. We propose a novel approach to efficiently calibrate such robots by placing a 3D printed fiducial coordinate frames on the arm and end-effector that is tracked using RGBD sensing. To measure the coupling and history-dependent effects between joints, we analyze data from sampled trajectories and consider 13 approaches to modeling. These models include linear regression and LSTM recurrent neural networks, each with varying temporal window length to provide compensatory feedback. With the proposed method, data collection of 1800 samples takes 31 minutes and model training takes under 1 minute. Results on a test set of reference trajectories suggest that the trained model can reduce the mean tracking error of physical robot from 2.96 mm to 0.65 mm. Results on the execution of open-loop trajectories of the FLS peg transfer surgeon training task suggest that the best model increases success rate from 39.4% to 96.7%, producing performance comparable to that of an expert surgical resident. Supplementary materials, including code and 3D-printable models, are available at https://sites.google.com/berkeley.edu/surgical-calibration.},
}

@article{Zhang2020,
  author = {Zhang, Dandan and Liu, Jindong and Zhang, Lin and Yang, Guang-Zhong},

  issn = {1861-6429},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  number = {3},
  publisher = {Springer},
  title = {{Hamlyn CRM: a compact master manipulator for surgical robot remote control}},
  volume = {15},
  year = {2020},
  research_field={HW},
  data_type={KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/46ffc8aab655fa9c27b471a89fb5d479f7f3a84e},
  doi = {10.1007/s11548-019-02112-y},
  abstract = {Purpose Compact master manipulators have inherent advantages, since they can have practical deployment within the general surgical environments easily and bring benefits to surgical training. To assess the advantages of compact master manipulators for surgical skills training and the performance of general robot-assisted surgical tasks, Hamlyn Compact Robotic Master (Hamlyn CRM) is built up and evaluated in this paper. Methods A compact structure for the master manipulator is proposed. A novel sensing system is designed while stable real-time motion tracking can be realized by fusing the information from multiple sensors. User studies were conducted based on a ring transfer task and a needle passing task to explore a suitable mapping strategy for the compact master manipulator to control a surgical robot remotely. The overall usability of the Hamlyn CRM is verified based on the da Vinci Research Kit (dVRK). The master manipulators of the dVRK control console are used as the reference Results Motion tracking experiments verified that the proposed system can track the operators’ hand motion precisely. As for the master–slave mapping strategy, user studies proved that the combination of the position relative mapping mode and the orientation absolute mapping mode is suitable for Robot-Assisted Minimally Invasive Surgery (RAMIS), while key parameters for mapping are selected. Conclusion Results indicated that the Hamlyn CRM can serve as a compact master manipulator for surgical training and has potential applications for RAMIS.},
}

@article{Zhong2020,
  author = {Zhong, Fangxun and Wang, Zerui and Chen, Wei and He, Kejing and Wang, Yaqing and Liu, Yun-Hui},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Hand-Eye Calibration of Surgical Instrument for Robotic Surgery Using Interactive Manipulation}},
  volume = {5},
  year = {2020},
  research_field={IM},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/25b8542c00d4992d82cac797972676a18d8397e7},
  doi = {10.1109/LRA.2020.2967685},
  abstract = {Conventional robot hand-eye calibration methods are impractical for localizing robotic instruments in minimally-invasive surgeries under intra-corporeal workspace after pre-operative set-up. In this letter, we present a new approach to autonomously calibrate a robotic instrument relative to a monocular camera without recognizing calibration objects or salient features. The algorithm leverages interactive manipulation (IM) of the instrument for tracking its rigid-body motion behavior subject to the remote center-of-motion constraint. An adaptive controller is proposed to regulate the IM-induced instrument trajectory, using visual feedback, within a 3D plane which is observable from both the robot base and the camera. The eye-to-hand orientation and position are then computed via a dual-stage process allowing parameter estimation in low-dimensional spaces. The method does not require the exact knowledge of instrument model or large-scale data collection. Results from simulations and experiments on the da Vinci Research Kit are demonstrated via a laparoscopy resembled set-up using the proposed framework.}
}

@incollection{Lasso2020,
  author = {Lasso, Andras and Kazanzides, Peter},

  booktitle = {Handbook of Medical Image Computing and Computer Assisted Intervention},
  publisher = {Elsevier},
  title = {{System integration}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/2e9c9324cfaf0d7a6fa90611deeed1fa32629040},
  doi = {10.1016/c2017-0-04608-6}
}

@incollection{Penza2020,
  author = {Penza, Veronica and Moccia, Sara and {De Momi}, Elena and Mattos, Leonardo S},

  booktitle = {Handbook of Robotic and Image-Guided Surgery},
  pages = {223--237},
  publisher = {Elsevier},
  title = {{Enhanced Vision to Improve Safety in Robotic Surgery}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/88f3cb632a00f51ca00e441ecb892ba6b98402fa},
  doi = {10.1016/c2017-0-01316-2}
}

@article{Saracino2020,
  author = {Saracino, A. and {Oude Vrielink}, T. J. C. and Menciassi, Arianna and Sinibaldi, E. and Mylonas, G. P.},

  doi = {10.1109/TBME.2020.2987646},
  issn = {0018-9294},
  journal = {IEEE Transactions on Biomedical Engineering},
  title = {{Haptic intracorporeal palpation using a cable-driven parallel robot: a user study}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/3cd0d8f6efb2c6f030432663f094950ba004bde1},
  abstract = {Objective: Intraoperative palpation is a surgical gesture jeopardized by the lack of haptic feedback which affects robotic minimally invasive surgery. Restoring the force reflection in teleoperated systems may improve both surgeons’ performance and procedures’ outcome. Methods: A force-based sensing approach was developed, based on a cable-driven parallel manipulator with anticipated seamless and low-cost integration capabilities in teleoperated robotic surgery. No force sensor on the end-effector is used, but tissue probing forces are estimated from measured cable tensions. A user study involving surgical trainees (n = 22) was conducted to experimentally evaluate the platform in two palpation-based test-cases on silicone phantoms. Two modalities were compared: visual feedback alone and both visual + haptic feedbacks available at the master site. Results: Surgical trainees’ preference for the modality providing both visual and haptic feedback is corroborated by both quantitative and qualitative metrics. Hard nodules detection sensitivity improves (94.35 ± 9.1% vs 76.09 ± 19.15% for visual feedback alone), while also exerting smaller forces (4.13 ± 1.02 N vs 4.82 ± 0.81 N for visual feedback alone) on the phantom tissues. At the same time, the subjective perceived workload decreases. Conclusion: Tissue-probe contact forces are estimated in a low cost and unique way, without the need of force sensors on the end-effector. Haptics demonstrated an improvement in the tumor detection rate, a reduction of the probing forces, and a decrease in the perceived workload for the trainees. Significance: Relevant benefits are demonstrated from the usage of combined cable-driven parallel manipulators and haptics during robotic minimally invasive procedures. The translation of robotic intraoperative palpation to clinical practice could improve the detection and dissection of cancer nodules.},
}

@article{Li2020,
  author = {Li, Zhaoshuo and Shahbazi, Mahya and Patel, Niravkumar and O'Sullivan, Eimear and Zhang, Haojie and Vyas, Khushi and Chalasani, Preetham and Deguet, Anton and Gehlbach, Peter L and Iordachita, Iulian},

  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  publisher = {IEEE},
  title = {{Hybrid Robot-assisted Frameworks for Endomicroscopy Scanning in Retinal Surgeries}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/029f5b0626af6f0a0f4a3f080fdc851e5cdfa3a2},
  doi = {10.1109/TMRB.2020.2988312},
  abstract = {High-resolution real-time intraocular imaging of retina at the cellular level is very challenging due to the vulnerable and confined space within the eyeball as well as the limited availability of appropriate modalities. A probe-based confocal laser endomicroscopy (pCLE) system, can be a potential imaging modality for improved diagnosis. The ability to visualize the retina at the cellular level could provide information that may predict surgical outcomes. The adoption of intraocular pCLE scanning is currently limited due to the narrow field of view and the micron-scale range of focus. In the absence of motion compensation, physiological tremors of the surgeons hand and patient movements also contribute to the deterioration of the image quality. Therefore, an image-based hybrid control strategy is proposed to mitigate the above challenges. The proposed hybrid control strategy enables a shared control of the pCLE probe between surgeons and robots to scan the retina precisely, with the absence of hand tremors and with the advantages of an image-based auto-focus algorithm that optimizes the quality of pCLE images. The hybrid control strategy is deployed on two frameworks—cooperative and teleoperated. Better image quality, smoother motion, and reduced workload are all achieved in a statistically significant manner with the hybrid control frameworks.},
}

@inproceedings{Ginesi2020,
  author = {Ginesi, Michele and Meli, Daniele and Roberti, Andrea and Sansonetto, Nicola and Fiorini, Paolo},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{Autonomous task planning and situation awareness in robotic surgery}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Li2020c,
  author = {Li, Zhaoshuo and Gordon, Alex and Looi, Thomas and Drake, James and Forrest, Christopher and Taylor, Russell H.},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{Anatomical Mesh-Based Virtual Fixtures for Surgical Robots}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Tagliabue2020,
  author = {Tagliabue, Eleonora and Pore, Ameya and Alba, Diego Dall and Magnabosco, Enrico and Piccinelli, Marco and Fiorini, Paolo},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{Soft Tissue Simulation Environment to Learn Manipulation Tasks in Autonomous Robotic Surgery *}},
  year = {2020},
  research_field={AU},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Seita2020a,
  author = {Seita, Daniel and Ganapathi, Aditya and Hoque, Ryan and Hwang, Minho and Cen, Edward and Tanwani, Ajay Kumar and Balakrishna, Ashwin and Thananjeyan, Brijen and Ichnowski, Jeffrey and Jamali, Nawid and Yamane, Katsu and Iba, Soshi and Canny, John and Goldberg, Ken},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{Deep Imitation Learning of Sequential Fabric Smoothing From an Algorithmic Supervisor}},
  year = {2020},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Col2020,
  author = {Col, Tommaso Da and Mariani, Andrea and Deguet, Anton and Menciassi, Arianna and Kazanzides, Peter and Momi, Elena De},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{SCAN : System for Camera Autonomous Navigation in Robotic-Assisted Surgery}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Qian2020,
  author = {Qian, Long and Song, Chengzhi and Jiang, Yiwei and Luo, Qi and Ma, Xin and Chiu, Philip Waiyan},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{FlexiVision : Teleporting the Surgeon ' s Eyes via Robotic Flexible Endoscope and Head-Mounted Display}},
  year = {2020},
  research_field={AU},
  data_type={KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Bombieri2020,
  author = {Bombieri, Marco and Alba, Diego Dall and Ramesh, Sanat and Menegozzo, Giovanni and Schneider, Caitlin and Fiorini, Paolo},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems (IROS)},
  title = {{Joints-Space Metrics for Automatic Robotic Surgical Gestures Classification}},
  year = {2020},
  research_field={TR},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a44d6eb172747c6cb7e333334a0e59a0564a99ec},
  doi = {10.1109/IROS60139.2025}
}

@inproceedings{Attanasio2020,
  author = {Attanasio, Aleks and Scaglioni, Bruno and Leonetti, Matteo and Frangi, Alejandro F. and Cross, William and Biyani, Chandra Shekhar and Valdastri, Pietro},

  booktitle = {IEEE Robotics and Automation Letters},
  number = {4},
  title = {{Autonomous Tissue Retraction in Robotic Assisted Minimally Invasive Surgery - A Feasibility Study}},
  volume = {5},
  year = {2020},
  research_field={AU},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/ee56d8dc8d5900716d763d1cc94f3d226d8f9a25},
  doi = {10.1109/LRA.2020.2974710},
  abstract = {In this letter, we present a conditional generative adversarial network-based model for real-time underwater image enhancement. To supervise the adversarial training, we formulate an objective function that evaluates the perceptual image quality based on its global content, color, local texture, and style information. We also present EUVP, a large-scale dataset of a paired and an unpaired collection of underwater images (of ‘poor’ and ‘good’ quality) that are captured using seven different cameras over various visibility conditions during oceanic explorations and human-robot collaborative experiments. In addition, we perform several qualitative and quantitative evaluations which suggest that the proposed model can learn to enhance underwater image quality from both paired and unpaired training. More importantly, the enhanced images provide improved performances of standard models for underwater object detection, human pose estimation, and saliency prediction. These results validate that it is suitable for real-time preprocessing in the autonomy pipeline by visually-guided underwater robots. The model and associated training pipelines are available at https://github.com/xahidbuffon/funie-gan.},
}

@article{Roberti2020b,
  author = {Roberti, Andrea and Piccinelli, Nicola and Meli, Daniele and Muradore, Riccardo and Fiorini, Paolo},

  doi = {10.1109/TMRB.2020.3033670},
  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  title = {{Improving Rigid 3-D Calibration for Robotic Surgery}},
  volume = {2},
  year = {2020},
  research_field={IM},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2584f49ec0ebc1796bdf60a4856b59b15bf0a2fb},
  abstract = {Autonomy is the next frontier of research in robotic surgery and its aim is to improve the quality of surgical procedures in the next future. One fundamental requirement for autonomy is advanced perception capability through vision sensors. In this article, we propose a novel calibration technique for a surgical scenario with a da Vinci® Research Kit (dVRK) robot. Camera and robotic arms calibration are necessary to precise position and emulate expert surgeon. The novel calibration technique is tailored for RGB-D cameras. Different tests performed on relevant use cases prove that we significantly improve precision and accuracy with respect to state of the art solutions for similar devices on a surgical-size setups. Moreover, our calibration method can be easily extended to standard surgical endoscope used in real surgical scenario.},
}

@article{Minelli2020,
  author = {Minelli, Marco and Sozzi, Alessio and Rossi, Giacomo De and Ferraguti, Federica and Setti, Francesco and Muradore, Riccardo and Bonf, Marcello and Secchi, Cristian},

  isbn = {9781728162119},
  journal = {2020 IEEE International Workshop on Intelligent Robots and Systems (IROS)},
  title = {{Integrating Model Predictive Control and Dynamic Waypoints Generation for Motion Planning in Surgical Scenario}},
  year = {2020},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/92e75215524119301f04e92361b8eb40e17040b2},
  doi = {10.1109/IROS45743.2020.9341673},
  abstract = {In this paper we present a novel strategy for motion planning of autonomous robotic arms in Robotic Minimally Invasive Surgery (R-MIS). We consider a scenario where several laparoscopic tools must move and coordinate in a shared environment. The motion planner is based on a Model Predictive Controller (MPC) that predicts the future behavior of the robots and allows to move them avoiding collisions between the tools and satisfying the velocity limitations. In order to avoid the local minima that could affect the MPC, we propose a strategy for driving it through a sequence of waypoints. The proposed control strategy is validated on a realistic surgical scenario.},
}

@incollection{Colleoni2020,
  author = {Colleoni, Emanuele and Edwards, Philip and Stoyanov, Danail},

  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention - MICCAI},
  doi = {10.1007/978-3-030-59716-0_67},
  publisher = {Medical Image Computing and Computer Assisted Intervention},
  title = {{Synthetic and Real Inputs for Tool Segmentation in Robotic Surgery}},
  year = {2020},
  research_field={IM},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/04f42c6607d97d0e685cf33fd1d4635554eddfd1},
  abstract = {Semantic tool segmentation in surgical videos is important for surgical scene understanding and computer-assisted interventions as well as for the development of robotic automation. The problem is challenging because different illumination conditions, bleeding, smoke and occlusions can reduce algorithm robustness. At present labelled data for training deep learning models is still lacking for semantic surgical instrument segmentation and in this paper we show that it may be possible to use robot kinematic data coupled with laparoscopic images to alleviate the labelling problem. We propose a new deep learning based model for parallel processing of both laparoscopic and simulation images for robust segmentation of surgical tools. Due to the lack of laparoscopic frames annotated with both segmentation ground truth and kinematic information a new custom dataset was generated using the da Vinci Research Kit (dVRK) and is made available.}
}

@article{Wu2020,
  author = {Wu, Jie Ying and Kazanzides, Peter and Unberath, Mathias},

  issn = {1861-6410},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  publisher = {Springer},
  title = {{Leveraging vision and kinematics data to improve realism of biomechanic soft tissue simulation for robotic surgery}},
  year = {2020},
  research_field={SS},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/0d452fbd66ba6f76a1242258d0a04f685d0687c2},
  doi = {10.1007/s11548-020-02139-6},
  abstract = {Purpose Surgical simulations play an increasingly important role in surgeon education and developing algorithms that enable robots to perform surgical subtasks. To model anatomy, finite element method (FEM) simulations have been held as the gold standard for calculating accurate soft tissue deformation. Unfortunately, their accuracy is highly dependent on the simulation parameters, which can be difficult to obtain. Methods In this work, we investigate how live data acquired during any robotic endoscopic surgical procedure may be used to correct for inaccurate FEM simulation results. Since FEMs are calculated from initial parameters and cannot directly incorporate observations, we propose to add a correction factor that accounts for the discrepancy between simulation and observations. We train a network to predict this correction factor. Results To evaluate our method, we use an open-source da Vinci Surgical System to probe a soft tissue phantom and replay the interaction in simulation. We train the network to correct for the difference between the predicted mesh position and the measured point cloud. This results in 15–30% improvement in the mean distance, demonstrating the effectiveness of our approach across a large range of simulation parameters. Conclusion We show a first step towards a framework that synergistically combines the benefits of model-based simulation and real-time observations. It corrects discrepancies between simulation and the scene that results from inaccurate modeling parameters. This can provide a more accurate simulation environment for surgeons and better data with which to train algorithms.},
}

@article{Abdelaal2020,
  author = {Abdelaal, Alaa Eldin and Mathur, Prateek and Salcudean, Septimiu E},

  issn = {2573-5144},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  publisher = {Annual Reviews},
  title = {{Robotics In Vivo: A Perspective on Human–Robot Interaction in Surgical Robotics}},
  volume = {3},
  year = {2020},
  research_field={RE},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/1211f289d081e173384a568520df293d3c73fdba},
  doi = {10.1146/annurev-control-091219-013437},
  abstract = {This article reviews recent work on surgical robots that have been used or tested in vivo, focusing on aspects related to human–robot interaction. We present the general design requirements that should be considered when developing such robots, including the clinical requirements and the technologies needed to satisfy them. We also discuss the human aspects related to the design of these robots, considering the challenges facing surgeons when using robots in the operating room, and the safety issues of such systems. We then survey recent work in seven different surgical settings: urology and gynecology, orthopedic surgery, cardiac surgery, head and neck surgery, neurosurgery, radiotherapy, and bronchoscopy. We conclude with the open problems and recommendations on how to move forward in this research area.}
}

@misc{Hoque2020,
  archivePrefix = {arXiv},
  arxivId = {2003.09044},
  author = {Hoque, Ryan and Seita, Daniel and Balakrishna, Ashwin and Ganapathi, Aditya and Tanwani, Ajay Kumar and Jamali, Nawid and Yamane, Katsu and Iba, Soshi and Goldberg, Ken},

  booktitle = {Robotics: Science and Systems (RSS)},
  doi = {10.15607/rss.2020.xvi.034},
  eprint = {2003.09044},
  issn = {23318422},
  title = {{VisuoSpatial foresight for multi-step, multi-task fabric manipulation}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/369aea426e4602ab0963da50c1ee88aa08754fdf},
  abstract = {Robotic fabric manipulation has applications in home robotics, textiles, senior care and surgery. Existing fabric manipulation techniques, however, are designed for specific tasks, making it difficult to generalize across different but related tasks. We extend the Visual Foresight framework to learn fabric dynamics that can be efficiently reused to accomplish different fabric manipulation tasks with a single goal-conditioned policy. We introduce VisuoSpatial Foresight (VSF), which builds on prior work by learning visual dynamics on domain randomized RGB images and depth maps simultaneously and completely in simulation. We experimentally evaluate VSF on multi-step fabric smoothing and folding tasks against 5 baseline methods in simulation and on the da Vinci Research Kit (dVRK) surgical robot without any demonstrations at train or test time. Furthermore, we find that leveraging depth significantly improves performance. RGBD data yields an 80% improvement in fabric folding success rate over pure RGB data. Code, data, videos, and supplementary material are available at this https URL.},
}

@article{Thananjeyan2020,
  author = {Thananjeyan, Brijen and Balakrishna, Ashwin and Rosolia, Ugo and Li, Felix and McAllister, Rowan and Gonzalez, Joseph E and Levine, Sergey and Borrelli, Francesco and Goldberg, Ken},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Safety Augmented Value Estimation from Demonstrations (SAVED): Safe Deep Model-Based RL for Sparse Cost Robotic Tasks}},
  volume = {5},
  year = {2020},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/51fe965c579a689d1dc466f2313227a07eb22126},
  doi = {10.1109/LRA.2020.2976272},
  abstract = {Reinforcement learning (RL) for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations (SAVED), which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than <inline-formula><tex-math notation="LaTeX">$\text\{5\}\%$</tex-math></inline-formula> of the time while SAVED has a success rate of over <inline-formula><tex-math notation="LaTeX">$\text\{75\}\%$</tex-math></inline-formula> in the first 50 training iterations. Code and supplementary material is available at <uri>https://tinyurl.com/saved-rl</uri>.},
}

@article{Mariani2020,
  author = {Mariani, Andrea and Pellegrini, Edoardo and {De Momi}, Elena},

  journal = {IEEE Transactions on Biomedical Engineering},
  publisher = {IEEE},
  title = {{Skill-oriented and Performance-driven Adaptive Curricula for Training in Robot-Assisted Surgery using Simulators: a Feasibility Study}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/fa4a74bd7f1370b4f435a77444a83a10c130346d},
  doi = {10.1109/TBME.2020.3011867},
  abstract = {Objective: Virtual Reality (VR) simulators represent a remarkable educational opportunity in order to acquire and refine surgical practical skills. Nevertheless, there exists no consensus regarding a standard curriculum of simulation-based training. This study introduces an automatic, adaptive curriculum where the training session is real-time scheduled on the basis of the trainee's performances. Methods: An experimental study using the master console of the da Vinci Research Kit (Intuitive Surgical Inc., Sunnyvale, US) was carried out to test this approach. Tasks involving fundamental skills of robotic surgery were designed and simulated in VR. Twelve participants without medical background along with twelve medical residents were randomly and equally divided into two groups: a control group, self-managing the training session, and an experimental group, undergoing the proposed adaptive training. Results: The performances of the experimental users were significantly better with respect to the ones of the control group after training (non-medical: p < 0.01; medical: p = 0.02). This trend was analogous in the non-medical and medical populations and no significant difference was identified between these two classes (even in the baseline assessment). Conclusion: The analysis of the learning of the involved surgical skills highlighted how the proposed adaptive training managed to better identify and compensate for the trainee's gaps. The absence of initial difference between the non-medical and medical users underlines that robotic surgical devices require specific training before clinical practice. Significance: This feasibility study could pave the way towards the improvement of simulation-based training curricula.}
}

@article{Huan2020,
  author = {Huan, Yu and Tamadon, Izadyar and Scatena, Cristian and Cela, Vito and Naccarato, Antonio Giuseppe},

  doi = {10.1109/TBME.2020.2996965},
  journal = {IEEE Transactions on Biomedical Engineering},
  title = {{Soft Graspers for Safe and Effective Tissue Clutching in Minimally Invasive Surgery}},
  volume = {9294},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/9942b975ec6d8fd9c13e4bc7833005e4a64d128c},
  abstract = {Objective: Surgical graspers must be safe, not to damage tissue, and effective, to establish a stable contact for operation. For conventional rigid graspers, these requirements are conflicting and tissue damage is often induced. We thus proposed novel soft graspers, based on morphing jaws that increase contact area with clutching force. Methods: We introduced two soft jaw concepts: DJ and CJ. They were designed (using analytical and numerical models) and prototyped (10 mm diameter, 10 mm span). Corresponding graspers were obtained by integrating the jaws into a conventional tool used in the dVRK surgical robotics platform. Morphing performance was experimentally characterized. Jaw-tissue interaction was quantitatively assessed through damage indicators obtained from ex vivo tests and histological analysis, also comparing DJ, CJ and dVRK rigid jaws. Soft graspers were demonstrated through ex vivo tests on dVRK. Ex vivo tests and related analysis were devised/performed with medical doctors. Results: Design goal was achieved for both soft jaws: by morphing, contact area exceeded by 20–30% the maximum area allowed by encumbrance specifications to rigid jaws. Experimental characterization was in good agreement with model predictions (error ≈ 4%). Damage indicators showed differences amongst DJ, CJ and dVRK jaws (ANOVA p-value  =  0.0005): damage was one order of magnitude lower for soft graspers (each pairwise comparison was statistically significant). Conclusion: We proposed and demonstrated soft graspers potentially less harmful to tissue than conventional graspers. Significance: Beyond minimally invasive surgery, the proposed concepts and design methodology can foster the development of graspers for soft robotics.}
}

@article{Li2020a,
  author = {Li, Yang and Richter, Florian and Lu, Jingpei and Funk, Emily K and Orosco, Ryan K and Zhu, Jianke and Yip, Michael C},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{SuPer: A Surgical Perception Framework for Endoscopic Tissue Manipulation with Surgical Robotics}},
  volume = {5},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/4f885053ad0cd16ddcd6d65192caaebae411173f},
  doi = {10.1109/LRA.2020.2970659},
  abstract = {Traditional control and task automation have been successfully demonstrated in a variety of structured, controlled environments through the use of highly specialized modeled robotic systems in conjunction with multiple sensors. However, the application of autonomy in endoscopic surgery is very challenging, particularly in soft tissue work, due to the lack of high-quality images and the unpredictable, constantly deforming environment. In this letter, we propose a novel surgical perception framework, SuPer, for surgical robotic control. This framework continuously collects 3D geometric information that allows for mapping a deformable surgical field while tracking rigid instruments within the field. To achieve this, a model-based tracker is employed to localize the surgical tool with a kinematic prior in conjunction with a model-free tracker to reconstruct the deformable environment and provide an estimated point cloud as a mapping of the environment. The proposed framework was implemented on the da Vinci Surgical System in real-time with an end-effector controller where the target configurations are set and regulated through the framework. Our proposed framework successfully completed soft tissue manipulation tasks with high accuracy. The demonstration of this novel framework is promising for the future of surgical autonomy. In addition, we provide our dataset for further surgical research.**Website: https://www.sites.google.com/ucsd.edu/super-framework.},
}

@article{Hwang2020a,
  author = {Hwang, Minho and Thananjeyan, Brijen and Seita, Daniel and Ichnowski, Jeffrey and Paradis, Samuel and Fer, Danyal and Low, Thomas and Goldberg, Ken},

  journal = {arXiv preprint},
  title = {{Superhuman Surgical Peg Transfer Using Depth-Sensing and Deep Recurrent Neural Networks}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/5d58c24b6a1d7a093415121dcf245a3559f243b8}
}

@article{Bahar2020,
  author = {Bahar, Lidor and Sharon, Yarden and Nisky, Ilana},

  issn = {1662-5218},
  journal = {Frontiers in Neurorobotics},
  publisher = {Frontiers},
  title = {{Surgeon-Centered Analysis of Robot-Assisted Needle Driving Under Different Force Feedback Conditions}},
  volume = {13},
  year = {2020},
  research_field={TR},
  data_type={KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/dc6493ff295ee6c6f9a646ec2e4ea7c063e81e5b},
  doi = {10.3389/fnbot.2019.00108},
  abstract = {Robotic assisted minimally invasive surgery (RAMIS) systems present many advantages to the surgeon and patient over open and standard laparoscopic surgery. However, haptic feedback, which is crucial for the success of many surgical procedures, is still an open challenge in RAMIS. Understanding the way that haptic feedback affects performance and learning can be useful in the development of haptic feedback algorithms and teleoperation control systems. In this study, we examined the performance and learning of inexperienced participants under different haptic feedback conditions in a task of surgical needle driving via a soft homogeneous deformable object—an artificial tissue. We designed an experimental setup to characterize their movement trajectories and the forces that they applied on the artificial tissue. Participants first performed the task in an open condition, with a standard surgical needle holder, followed by teleoperation in one of three feedback conditions: (1) no haptic feedback, (2) haptic feedback based on position exchange, and (3) haptic feedback based on direct recording from a force sensor, and then again with the open needle holder. To quantify the effect of different force feedback conditions on the quality of needle driving, we developed novel metrics that assess the kinematics of needle driving and the tissue interaction forces, and we combined our novel metrics with classical metrics. We analyzed the final teleoperated performance in each condition, the improvement during teleoperation, and the aftereffect of teleoperation on the performance when using the open needle driver. We found that there is no significant difference in the final performance and in the aftereffect between the 3 conditions. Only the two conditions with force feedback presented statistically significant improvement during teleoperation in several of the metrics, but when we compared directly between the improvements in the three different feedback conditions none of the effects reached statistical significance. We discuss possible explanations for the relative similarity in performance. We conclude that we developed several new metrics for the quality of surgical needle driving, but even with these detailed metrics, the advantage of state of the art force feedback methods to tasks that require interaction with homogeneous soft tissue is questionable.},
}

@article{Chua2020,
  author = {Chua, Zonghe and Jarc, Anthony M. and Wren, Sherry and Nisky, Ilana and Okamura, Allison M.},

  doi = {10.1109/TMRB.2020.3023005},
  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  title = {{Task Dynamics of Prior Training Influence Visual Force Estimation Ability During Teleoperation}},
  year = {2020},
  research_field={TR},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2158ac5446a898563fa984e8f60554671f2abf4a},
  abstract = {The lack of haptic feedback in teleoperation is a potential barrier to safe handling of soft materials, yet in Robot-assisted Minimally Invasive Surgery (RMIS), haptic feedback is often unavailable. Due to its availability in open and laparoscopic surgery, surgeons with such experience potentially possess learned models of tissue stiffness that might promote good force estimation abilities during RMIS. To test if prior haptic experience leads to improved force estimation ability in teleoperation, 33 naive participants were assigned to one of three training conditions: manual manipulation, teleoperation with force feedback, or teleoperation without force feedback, and learned to tension a silicone sample to a set of forces. They were then asked to perform the tension task, and a previously unencountered palpation task, to a different set of forces under teleoperation without force feedback. Compared to the teleoperation groups, the manual group had higher force error in the tension task outside the range of forces they had trained on, but showed better speed-accuracy functions in the palpation task at low force levels. This suggests that the dynamics of the training modality affect force estimation ability during teleoperation, with the prior haptic experience accessible if formed under the same dynamics as the task.},
}

@article{Leporini2020,
  author = {Leporini, Alice and Oleari, Elettra and Landolfo, Carmela and Sanna, Alberto and Larcher, Alessandro and Gandaglia, Giorgio and Fossati, Nicola and Muttin, Fabio and Capitanio, Umberto and Montorsi, Francesco and Salonia, Andrea and Minelli, Marco and Ferraguti, Federica and Secchi, Cristian and Farsoni, Saverio and Sozzi, Alessio and Bonfe, Marcello and Sayols, Narcis and Hernansanz, Albert and Casals, Alicia and Hertle, Sabine and Cuzzolin, Fabio and Dennison, Andrew and Melzer, Andreas and Kronreif, Gernot and Siracusano, Salvatore and Falezza, Fabio and Setti, Francesco and Muradore, Riccardo},

  doi = {10.1109/TMRB.2020.2990286},
  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  number = {2},
  title = {{Technical and Functional Validation of a Teleoperated Multirobots Platform for Minimally Invasive Surgery}},
  volume = {2},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/42644e0ea548b08b4aa66a56e6cffaf2d4077e64},
  abstract = {Nowadays Robotic assisted Minimally Invasive Surgeries (R-MIS) are the elective procedures for treating highly accurate and scarcely invasive pathologies, thanks to their ability to empower surgeons’ dexterity and skills. The research on new Multi-Robots Surgery (MRS) platform is cardinal to the development of a new SARAS surgical robotic platform, which aims at carrying out autonomously the assistants tasks during R-MIS procedures. In this work, we will present the SARAS MRS platform validation protocol, framed in order to assess: (i) its technical performances in purely dexterity exercises, and (ii) its functional performances. The results obtained show a prototype able to put the users in the condition of accomplishing the tasks requested (both dexterity- and surgical-related), even with reasonably lower performances respect to the industrial standard. The main aspects on which further improvements are needed result to be the stability of the end effectors, the depth perception and the vision systems, to be enriched with dedicated virtual fixtures. The SARAS’ aim is to reduce the main surgeon’s workload through the automation of assistive tasks which would benefit both surgeons and patients by facilitating the surgery and reducing the operation time.},
}

@article{Liu2020a,
  author = {Liu, Huan and Selvaggio, Mario and Ferrentino, Pasquale and Moccia, Rocco and Pirozzi, Salvatore and Bracale, Umberto and Ficuciello, Fanny},

  doi = {10.1109/TMECH.2020.3022782},
  issn = {1083-4435},
  journal = {IEEE/ASME Transactions on Mechatronics},
  title = {{The MUSHA Hand II: A Multi-Functional Hand for Robot-Assisted Laparoscopic Surgery}},
  year = {2020},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/af53cb2e2e264546fa0d168ea524a0d06802c8a2},
  abstract = {Although substantial progresses have been made in robot-assisted laparoscopic surgery, the graspers for existing surgical systems generally remain nonsensorized forceps design with limited functions. This article presents the design, development, and preliminary evaluation of the MUSHA Hand II, a multifunctional hand with force sensors for robot-assisted laparoscopic surgery. The proposed hand has three snake-like underactuated fingers that can be folded into a $\phi$12 mm cylindrical form. Each finger has a three-axis force sensor, to provide force information. After been deployed into an abdominal cavity, the hand can be configured to either grasper mode, retractor mode, or palpation mode for different tasks. Underactuated finger design enhances the adaptivity in grasping and the compliance in interaction with the environment. In addition, fingertip force sensors can be utilized for palpation to obtain a real-time stiffness map of organs. Using the da Vinci Research Kit as a robotic testbed, the functionality of the hand has been demonstrated and experiments have been conducted, including robotic palpation and organ manipulation. The results suggest that the hand can effectively enhance the functionality of a robotic surgical system and overcome the limits on force sensing introduced by the use of robots in laparoscopic surgery.}
}

@article{Wang2020a,
  author = {Wang, Chongyun and Komninos, Charalampos and Andersen, Stephanie and D'Ettorre, Claudia and Dwyer, George and Maneas, Efthymios and Edwards, Philip and Desjardins, Adrien and Stilli, Agostino and Stoyanov, Danail},

  issn = {1861-6429},
  journal = {International journal of computer assisted radiology and surgery},
  publisher = {Springer},
  title = {{Ultrasound 3D reconstruction of malignant masses in robotic-assisted partial nephrectomy using the PAF rail system: a comparison study.}},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/5ee158e176091938b84854839c7156cae98599ba},
  doi = {10.1007/s11548-020-02149-4},
  abstract = {Purpose In robotic-assisted partial nephrectomy (RAPN), the use of intraoperative ultrasound (IOUS) helps to localise and outline the tumours as well as the blood vessels within the kidney. The aim of this work is to evaluate the use of the pneumatically attachable flexible (PAF) rail system for US 3D reconstruction of malignant masses in RAPN. The PAF rail system is a novel device developed and previously presented by the authors to enable track-guided US scanning. Methods We present a comparison study between US 3D reconstruction of masses based on: the da Vinci Surgical System kinematics, single- and stereo-camera tracking of visual markers embedded on the probe. An US-realistic kidney phantom embedding a mass is used for testing. A new design for the US probe attachment to enhance the performance of the kinematic approach is presented. A feature extraction algorithm is proposed to detect the margins of the targeted mass in US images. Results To evaluate the performance of the investigated approaches the resulting 3D reconstructions have been compared to a CT scan of the phantom. The data collected indicates that single camera reconstruction outperformed the other approaches, reconstructing with a sub-millimetre accuracy the targeted mass. Conclusions This work demonstrates that the PAF rail system provides a reliable platform to enable accurate US 3D reconstruction of masses in RAPN procedures. The proposed system has also the potential to be employed in other surgical procedures such as hepatectomy or laparoscopic liver resection.},
}

@article{Jo2020,
  author = {Jo, Yeeun and Kim, Yoon Jae and Cho, Minwoo and Lee, Chiwon and Kim, Myungjoon and Moon, Hye-Min and Kim, Sungwan},

  issn = {1598-6446},
  journal = {International Journal of Control, Automation and Systems},
  number = {1},
  publisher = {Springer},
  title = {{Virtual Reality-based Control of Robotic Endoscope in Laparoscopic Surgery}},
  volume = {18},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/75d936fd8c83fdb1fca2dafb93c8ac3d0c05707c},
  doi = {10.1007/s12555-019-0244-9}
}

@article{Moccia2020,
  author = {Moccia, Rocco and Iacono, Cristina and Siciliano, Bruno and Ficuciello, Fanny},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Vision-based dynamic virtual fixtures for tools collision avoidance in robotic surgery}},
  volume = {5},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/4524caf974eb70df7bec1eda98bb71c7ca6cb56d},
  doi = {10.1109/LRA.2020.2969941},
  abstract = {In robot-aided surgery, during the execution of typical bimanual procedures such as dissection, surgical tools can collide and create serious damage to the robot or tissues. The da Vinci robot is one of the most advanced and certainly the most widespread robotic system dedicated to minimally invasive surgery. Although the procedures performed by da Vinci-like surgical robots are teleoperated, potential collisions between surgical tools are a very sensitive issue declared by surgeons. Shared control techniques based on Virtual Fixtures (VF) can be an effective way to help the surgeon prevent tools collision. This letter presents a surgical tools collision avoidance method that uses Forbidden Region Virtual Fixtures. Tool clashing is avoided by rendering a repulsive force to the surgeon. To ensure the correct definition of the VF, a marker-less tool tracking method, using deep neural network architecture for tool segmentation, is adopted. The use of direct kinematics for tools collision avoidance is affected by tools position error introduced by robot component elasticity during tools interaction with the environment. On the other hand, kinematics information can help in case of occlusions of the camera. Therefore, this work proposes an Extended Kalman Filter (EKF) for pose estimation which ensures a more robust application of VF on the tool, coupling vision and kinematics information. The entire pipeline is tested in different tasks using the da Vinci Research Kit system.}
}

@article{Ma2020,
  author = {Ma, Xin and Song, Chengzhi and Chiu, Philip Waiyan and Li, Zheng},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Visual Servo of a 6-DOF Robotic Stereo Flexible Endoscope Based on da Vincix Research Kit (dVRK) System}},
  volume = {5},
  year = {2020},
  research_field={HW},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/be0cdde41dc1e65998f356992fd884e7ff92bf6e},
  doi = {10.1109/LRA.2020.2965863},
  abstract = {Endoscopes play an important role in minimally invasive surgery (MIS). Due to the advantages of less occupied motion space and enhanced safety, flexible endoscopes are drawing more and more attention. However, the structure of the flexible section makes it difficult for surgeons to manually rotate and guide the view of endoscopes. To solve these problems, we developed a 6-DOF robotic stereo flexible endoscope (RSFE) based on the da Vinci Research Kit (dVRK). Then an image-based endoscope guidance method with depth information is proposed for the RSFE. With this method, the view and insertion depth of the RSFE can be adjusted by tracking the surgical instruments automatically. Additionally, an image-based view rotation control method is proposed, with which the rotation of the view can be controlled by tracking two surgical instruments. The experimental results show that the proposed methods control the direction and rotation of the view of the flexible endoscope faster than the manual control method. Lastly, an ex vivo experiment is performed to demonstrate the feasibility of the proposed control method and system.}
}

@inproceedings{Ginesi2019,
  author = {Ginesi, Michele and Meli, Daniele and Nakawala, Hirenkumar and Roberti, Andrea and Fiorini, Paolo},

  booktitle = {2019 19th International Conference on Advanced Robotics (ICAR)},
  isbn = {1728124670},
  publisher = {IEEE},
  title = {{A knowledge-based framework for task automation in surgery}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/8037d8cc267f6ad7129de2ea6f71513a69c0bc29},
  doi = {10.1109/icar46387.2019}
}

@inproceedings{Mikic2019,
  author = {Mikic, Marko and Francis, Peter and Looi, Thomas and Gerstle, J Ted and Drake, James},

  booktitle = {2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  isbn = {1538613115},
  publisher = {IEEE},
  title = {{Bone Conduction Headphones for Force Feedback in Robotic Surgery}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/1473f421ade8ac1b0f9db1e4195d7707515478ae},
  doi = {10.1109/embc43219.2019}
}

@inproceedings{Hamedani2019,
  author = {Hamedani, Mohammad Hossein and Selvaggio, Mario and Rahimkhani, Mahtab and Ficuciello, Fanny and Sadeghian, Hamid and Zekri, Maryam and Sheikholeslam, Farid},

  booktitle = {2019 7th International Conference on Robotics and Mechatronics (ICRoM)},
  isbn = {1728166047},
  publisher = {IEEE},
  title = {{Robust Dynamic Surface Control of da Vinci Robot Manipulator Considering Uncertainties: A Fuzzy Based Approach}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/5bcb6fa9e22dc4bd9200fa1acfefd80a6aea0925},
  doi = {10.1109/icrom48714.2019}
}

@inproceedings{Sundaresan2019,
  author = {Sundaresan, Priya and Thananjeyan, Brijen and Chiu, Johnathan and Fer, Danyal and Goldberg, Ken},

  booktitle = {2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)},
  isbn = {1728103568},
  publisher = {IEEE},
  title = {{Automated Extraction of Surgical Needles from Tissue Phantoms}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/59acaaa887009df13c9a31715cab6aa26b4c1f4b},
  doi = {10.1109/case41385.2019},
}

@inproceedings{Li2019a,
  author = {Li, Xiang and Wang, Zerui and Liu, Yun-Hui},

  booktitle = {2019 IEEE International Conference on Real-time Computing and Robotics (RCAR)},
  isbn = {1728137268},
  publisher = {IEEE},
  title = {{Sequential Robotic Manipulation for Active Shape Control of Deformable Linear Objects}},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/4a7f50847cc8689ce804e1397f7faec1370f56fc},
  doi = {10.1109/rcar47638.2019}
}

@inproceedings{Shin2019b,
  author = {Shin, Changyeob and Ferguson, Peter Walker and Pedram, Sahba Aghajani and Ma, Ji and Dutson, Erik P. and Rosen, Jacob},

  booktitle = {2019 IEEE International Conference On Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2019.8794159},
  title = {{Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control}},
  year = {2019},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/d9ad92812d61709a9bf35b09078361d8bffd3f7a},
  abstract = {Tissue manipulation is a frequently used fundamental subtask of any surgical procedures, and in some cases it may require the involvement of a surgeon’s assistant. The complex dynamics of soft tissue as an unstructured environment is one of the main challenges in any attempt to automate the manipulation of it via a surgical robotic system. Two AI learning based model predictive control algorithms using vision strategies are proposed and studied: (1) reinforcement learning and (2) learning from demonstration. Comparison of the performance of these AI algorithms in a simulation setting indicated that the learning from demonstration algorithm can boost the learning policy by initializing the predicted dynamics with given demonstrations. Furthermore, the learning from demonstration algorithm is implemented on a Raven IV surgical robotic system and successfully demonstrated feasibility of the proposed algorithm using an experimental approach. This study is part of a profound vision in which the role of a surgeon will be redefined as a pure decision maker whereas the vast majority of the manipulation will be conducted autonomously by a surgical robotic system. A supplementary video can be found at: http://bionics.seas.ucla.edu/research/surgeryproject17.html},
}

@inproceedings{Nagy2019a,
  author = {Nagy, Tam{\'{a}}s D and Ukhrenkov, Nikita and Drexler, Daniel A and Tak{\'{a}}cs, {\'{A}}rp{\'{a}}d and Haidegger, Tam{\'{a}}s},

  booktitle = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
  isbn = {1728145694},
  publisher = {IEEE},
  title = {{Enabling quantitative analysis of situation awareness: system architecture for autonomous vehicle handover studies}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/ead4dcd0293d0ccdf7649082cd4ec151e0c9660d},
  doi = {10.1109/smc43495.2019}
}

@inproceedings{Munawar2019,
  author = {Munawar, Adnan and Wang, Yan and Gondokaryono, Radian and Fischer, Gregory S},

  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1728140048},
  publisher = {IEEE},
  title = {{A real-time dynamic simulator and an associated front-end representation format for simulating complex robots and environments}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90}
}

@inproceedings{Zhang2019d,
  author = {Zhang, Dandan and Liu, Jindong and Zhang, Lin and Yang, Guang-Zhong},

  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1728140048},
  publisher = {IEEE},
  title = {{Design and verification of a portable master manipulator based on an effective workspace analysis framework}},
  year = {2019},
  research_field={HW},
  data_type={RI and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90}
}

@inproceedings{Munawar2019a,
  author = {Munawar, Adnan and Fischer, Gregory S},

  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1728140048},
  publisher = {IEEE},
  title = {{An Asynchronous Multi-Body Simulation Framework for Real-Time Dynamics, Haptics and Learning with Application to Surgical Robots}},
  year = {2019},
  research_field={AU},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90}
}

@inproceedings{Moccia2019,
  author = {Moccia, Rocco and Selvaggio, Mario and Villani, Luigi and Siciliano, Bruno and Ficuciello, Fanny},

  booktitle = {2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1728140048},
  publisher = {IEEE},
  title = {{Vision-based virtual fixtures generation for robotic-assisted polyp dissection procedures}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/f79ea000a747fecbbb8a40ed199a79cb02534b90}
}

@inproceedings{Richter2019b,
  author = {Richter, Florian and Zhang, Yifei and Zhi, Yuheng and Orosco, Ryan K and Yip, Michael C},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Augmented reality predictive displays to help mitigate the effects of delayed telesurgery}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Tsai2019,
  author = {Tsai, Ya-Yen and Huang, Bidan and Guo, Yao and Yang, Guang-Zhong},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Transfer Learning for Surgical Task Segmentation}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Itzkovich2019,
  author = {Itzkovich, Danit and Sharon, Yarden and Jarc, Anthony and Refaely, Yael and Nisky, Ilana},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Using Augmentation to Improve the Robustness to Rotation of Deep Learning Segmentation in Robotic-Assisted Surgical Data}},
  year = {2019},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Marinho2019,
  author = {Marinho, Murilo M and Adorno, Bruno V and Harada, Kanako and Deie, Kyoichi and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H and Mitsuishi, Mamoru},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{A unified framework for the teleoperation of surgical robots in constrained workspaces}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{VanAmsterdam2019,
  author = {van Amsterdam, Beatrice and Nakawala, Hirenkumar and {De Momi}, Elena and Stoyanov, Danail},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Weakly Supervised Recognition of Surgical Gestures}},
  year = {2019},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Qian2019b,
  author = {Qian, Long and Deguet, Anton and Wang, Zerui and Liu, Yun-Hui and Kazanzides, Peter},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Augmented reality assisted instrument insertion and tool manipulation for the first assistant in robotic surgery}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Pryor2019,
  author = {Pryor, Will and Vagvolgyi, Balazs P and Gallagher, William J and Deguet, Anton and Leonard, Simon and Whitcomb, Louis L and Kazanzides, Peter},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Experimental Evaluation of Teleoperation Interfaces for Cutting of Satellite Insulation}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Banach2019,
  author = {Banach, Artur and Leibrandt, Konrad and Grammatikopoulou, Maria and Yang, Guang-Zhong},

  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  isbn = {153866027X},
  publisher = {IEEE},
  title = {{Active contraints for tool-shaft collision avoidance in minimally invasive surgery}},
  year = {2019},
  research_field={HW},
  data_type={RI and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/babd88f02ff8d131f1907785fc6fd0ff2da4a1e0},
  doi = {10.1109/icra39644.2019}
}

@inproceedings{Thananjeyan2019c,
  author = {Thananjeyan, Brijen and Tanwani, Ajay and Ji, Jessica and Fer, Danyal and Patel, Vatsal and Krishnan, Sanjay and Goldberg, Ken},

  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  doi = {10.1109/ISMR.2019.8710194},
  isbn = {978-1-5386-7825-1},
  publisher = {IEEE},
  title = {{Optimizing Robot-Assisted Surgery Suture Plans to Avoid Joint Limits and Singularities}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/27930fb5246613500e56b21f010ec04262099463},
  abstract = {Laparoscopic robots such as the da Vinci Research Kit encounter joint limits and singularities during procedures, leading to errors and prolonged operating times. We propose the Circle Suture Placement Problem to optimize the location and direction of four evenly-spaced stay sutures on surgical mesh for robot-assisted hernia surgery. We present an algorithm for this problem that runs in 0.4 seconds on a desktop equipped with commodity hardware. Simulated results integrating data from expert surgeon demonstrations suggest that optimizing over both suture position and direction increases dexterity reward by 11%-57% over baseline algorithms that optimize over either suture position or direction only.}
}

@inproceedings{Zhang2019b,
  author = {Zhang, Ada and Guo, Liheng and Jarc, Anthony M.},

  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  doi = {10.1109/ISMR.2019.8710177},
  isbn = {978-1-5386-7825-1},
  publisher = {IEEE},
  title = {{Prediction of task-based, surgeon efficiency metrics during robotic-assisted minimally invasive surgery}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/59dafb5aaa0a2e9660180540ef2f6d4ce5c3fac6},
  abstract = {We present a machine learning algorithm for predicting surgeon performance on future tasks given performance on previous tasks during robotic-assisted surgery. Performance is estimated via six heuristics, called metrics, which are derived from da Vinci Surgical System data streams and which have previously demonstrated significance in differentiating expertise. We use a boosted ensemble of regression trees, which learns a regression from 193 system data features to one of the six target metrics. Predictions from our algorithm are evaluated on whether they achieve significantly lower error than two intelligent guesses: (1) guessing a surgeon performs the future task exactly as how she performed the previous task (GUESS_PREV) and (2) guessing a surgeon will have average performance on the future task (GUESS_MEAN). We show that our algorithm can predict metrics of future tasks, significantly $(p\lt 0.05)$ outperforming GUESS_PREV on 19 out of 36 metrics and task pairs and GUESS_MEAN on 8 out of 36 metrics and task pairs. This work serves as a first step towards pre-operative performance estimation, where a surgeon can proactively improve performance rather than relying solely on post-operative feedback}
}

@inproceedings{Pique2019a,
  author = {Piqu{\'{e}}, Francesco and Boushaki, Mohamed N and Brancadoro, Margherita and {De Momi}, Elena and Menciassi, Arianna},

  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  isbn = {153867825X},
  publisher = {IEEE},
  title = {{Dynamic modeling of the da Vinci research kit arm for the estimation of interaction wrench}},
  year = {2019},
  research_field={},
  data_type={}
}

@inproceedings{Menegozzo2019,
  author = {Menegozzo, Giovanni and Dall'Alba, Diego and Zandon{\`{a}}, Chiara and Fiorini, Paolo},

  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  isbn = {153867825X},
  publisher = {IEEE},
  title = {{Surgical gesture recognition with time delay neural network based on kinematic data}},
  year = {2019},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/8aa5230692467f704ac8decfeaf514dc93ba09f2},
  doi = {10.1109/ismr44918.2019}
}

@inproceedings{Setti2019,
  author = {Setti, Francesco and Oleari, Elettra and Leporini, Alice and Trojaniello, Diana and Sanna, Alberto and Capitanio, Umberto and Montorsi, Francesco and Salonia, Andrea and Muradore, Riccardo},

  booktitle = {2019 International Symposium on Medical Robotics (ISMR)},
  doi = {10.1109/ISMR.2019.8710209},
  isbn = {978-1-5386-7825-1},
  publisher = {IEEE},
  title = {{A Multirobots Teleoperated Platform for Artificial Intelligence Training Data Collection in Minimally Invasive Surgery}},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/2ce4415da0f510e2468ada95b88a2f96fd557cc4},
  abstract = {Dexterity and perception capabilities of surgical robots may soon be improved by cognitive functions that can support surgeons in decision making and performance monitoring, and enhance the impact of automation within the operating rooms. Nowadays, the basic elements of autonomy in robotic surgery are still not well understood and their mutual interaction is unexplored. Current classification of autonomy encompasses six basic levels: Level 0: no autonomy; Level 1: robot assistance; Level 2: task autonomy; Level 3: conditional autonomy; Level 4: high autonomy. Level 5: full autonomy. The practical meaning of each level and the necessary technologies to move from one level to the next are the subject of intense debate and development. In this paper, we discuss the first outcomes of the European funded project Smart Autonomous Robotic Assistant Surgeon (SARAS). SARAS will develop a cognitive architecture able to make decisions based on pre-operative knowledge and on scene understanding via advanced machine learning algorithms. To reach this ambitious goal that allows us to reach Level 1 and 2, it is of paramount importance to collect reliable data to train the algorithms. We will present the experimental setup to collect the data for a complex surgical procedure (Robotic Assisted Radical Prostatectomy) on very sophisticated manikins (i.e. phantoms of the inflated human abdomen). The SARAS platform allows the main surgeon and the assistant to teleoperate two independent two-arm robots. The data acquired with this platform (videos, kinematics, audio) will be used in our project and will be released (with annotations) for research purposes.}
}

@inproceedings{Dimitri2019,
  author = {Dimitri, M and Gentili, G B and Staderini, F and Brancadoro, M and Menciassi, A and Coratti, A and Cianchi, F and Corvi, A and Capineri, L},

  booktitle = {2019 PhotonIcs & Electromagnetics Research Symposium-Spring (PIERS-Spring)},
  isbn = {172813403X},
  publisher = {IEEE},
  title = {{Developrnent of a Robotic Surgical System of Thermal Ablation and Microwave Coagulation}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/6c007bbfd098515b358a57d27972c3d510b1e26f},
  doi = {10.1109/piers-spring46901.2019}
}

@inproceedings{Ferro2019a,
  author = {Ferro, Marco and Brunori, Damiano and Magistri, Federico and Saiella, Lorenzo and Selvaggio, Mario and Fontanelli, Giuseppe Andrea},

  booktitle = {2019 Third IEEE International Conference on Robotic Computing (IRC)},
  doi = {10.1109/IRC.2019.00093},
  isbn = {978-1-5386-9245-5},
  publisher = {IEEE},
  title = {{A Portable da Vinci Simulator in Virtual Reality}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/84f62b19c9095754a0e64db3a05718298c20ae41},
  abstract = {Research activity in Minimally Invasive Robotic Surgery (MIRS) has gained a considerable momentum in the last years, due to the availability of reliable and clinically relevant research platforms like the da Vinci Research Kit (dVRK). However, despite the wide sharing of the dVRK in the research community, access to the platform remains limited because of high maintenance costs and difficulty in replacing components. In this work we complete a robotic simulator of the dVRK, previously developed by our group, with cheap haptic interfaces and an Oculus Rift to replicate and extend the functionalities of the Master console. The complete system represents an efficient, safe and low-cost tool, useful to design and validate new surgical instruments and control strategies, as well as provide an easyto-access educational tool to students.}
}

@article{Chrysilla2019,
  author = {Chrysilla, Grace and Eusman, Nickolas and Deguet, Anton and Kazanzides, Peter},

  journal = {Acta Polytechnica Hungarica},
  number = {8},
  title = {{A Compliance Model to Improve the Accuracy of the da Vinci Research Kit (dVRK)}},
  volume = {16},
  year = {2019},
  research_field={HW},
  data_type={KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/922b22f7dda3b65e5b68148f10644ff24914cb06},
  doi = {10.12700/aph.16.8.2019.8.4},
  abstract = {: The da Vinci surgical robot is widely used for minimally-invasive surgery. It inserts multiple articulated instruments through small incisions into the patient. The robot system contains encoders to measure joint displacements which, when combined with the kinematic model of the robot, measures the instrument position and orientation. But, the accuracy of these measurements is affected by non-kinematic errors, such as bending of the instruments due to applied loads. We develop a compliance model that relates displacement of the first two joints of the da Vinci Patient Side Manipulator (PSM) to lateral forces applied to the instrument shaft. This model enables us to compensate for these errors based on the measured joint efforts, which are derived from the measured motor currents. We perform experiments with the open-source da Vinci Research Kit (dVRK) to estimate the model parameters and to evaluate the accuracy improvement that results from application of this model. Preliminary results indicate that the model-based correction can reduce instrument position error due to externally-applied forces.},
}

@article{Wang2019c,
  author = {Wang, Yan and Gondokaryono, Radian and Munawar, Adnan and Fischer, Gregory Scott},

  doi = {10.1109/LRA.2019.2927947},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  title = {{A Convex Optimization-based Dynamic Model Identification Package for the da Vinci Research Kit}},
  year = {2019},
  research_field={SS},
  data_type={DD},
  semanticscholar = {https://www.semanticscholar.org/paper/71d6271db34dde1616d2c5a1b0d1c26fba51fa81},
  abstract = {The da Vinci Research Kit (dVRK) is a teleoperated surgical robotic system. For dynamic simulations and model-based control, the dynamic model of the dVRK is required. We present an open-source dynamic model identification package for the dVRK, capable of modeling the parallelograms, springs, counterweight, and tendon couplings, which are inherent to the dVRK. A convex optimization-based method is used to identify the dynamic parameters of the dVRK subject to physical consistency. Experimental results show the effectiveness of the modeling and the robustness of the package. Although this software package is originally developed for the dVRK, it is feasible to apply it on other similar robots.},
}

@article{Nagy2019,
  author = {Nagy, Tam{\'{a}}s D{\'{a}}niel and Haidegger, Tam{\'{a}}s},

  issn = {1785-8860},
  journal = {Acta Polytechnica Hungarica},
  pages = {61--78},
  publisher = {{\'{O}}buda University},
  title = {{A DVRK-based framework for surgical subtask automation}},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and SD}
}

@article{Lin2019,
  author = {Lin, Hongbin and Hui, Chiu-Wai Vincent and Wang, Yan and Deguet, Anton and Kazanzides, Peter and Au, K W Samuel},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {4},
  publisher = {IEEE},
  title = {{A Reliable Gravity Compensation Control Strategy for dVRK Robotic Arms With Nonlinear Disturbance Forces}},
  volume = {4},
  year = {2019},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/16913ea312e4307d409031f6f64c04cdaebf3499},
  doi = {10.1109/LRA.2019.2927953},
  abstract = {External disturbance forces caused by nonlinear springy electrical cables in the master tool manipulator (MTM) of the da Vinci Research Kit (dVRK) limits the usage of the existing gravity compensation methods. Significant motion drifts at the MTM tip are often observed when the MTM is located far from its identification trajectory, preventing the usage of these methods for the entire workspace reliably. In this letter, we propose a general and systematic framework to address the problems of the gravity compensation for the MTM of the dVRK. Particularly, high-order polynomial models were used to capture the highly nonlinear disturbance forces and integrated with the multi-step least square estimation framework. This method allows us to identify the parameters of both the gravitational and disturbance forces for each link sequentially, preventing residual error passing among the links of the MTM with uneven mass distribution. A corresponding gravity compensation controller was developed to compensate the gravitational and disturbance forces. The method was validated with extensive experiments in the majority of the manipulator's workspace, showing significant performance enhancements over existing methods. Finally, a deliverable software package in MATLAB and C++ was integrated with dVRK and published in the dVRK community for open-source research and development.},
}

@article{Qian2019a,
  author = {Qian, Long and Wu, Jie Ying and DiMaio, Simon and Navab, Nassir and Kazanzides, Peter},

  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  publisher = {IEEE},
  title = {{A Review of Augmented Reality in Robotic-Assisted Surgery}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/760105262d700ad2b0dfca8430cb4c8ee97f7ae6},
  doi = {10.1109/TMRB.2019.2957061},
  abstract = {Augmented reality (AR) and robotic-assisted surgery (RAS) are both rapidly evolving technologies in recent years. RAS systems, such as the da Vinci Surgical System, aim to improve surgical precision and dexterity, as well as access to minimally-invasive procedures, while AR provides an advanced interface to enhance user perception. Combining the features of both, AR-integrated RAS has become an appealing concept with increased interest among the academic community. In this paper, we review the existing literature about AR-integrated RAS. We discuss the hardware components, application paradigms and clinical relevance of the literature. The concept of AR-integrated RAS has been shown to be feasible for various procedures. Encouraging preliminary results include reduced sight diversion and improved situation awareness. Special techniques, e.g., activation-on-demand, are taken into consideration to address visual clutter of the AR interface and ensure that the system is fail-safe. Although AR-integrated RAS is not yet mature, we believe that if the current trend of development continues, it will soon demonstrate its clinical value.}
}

@article{Song2019,
  author = {Song, Chengzhi and Ma, Xin and Xia, Xianfeng and Chiu, Philip Wai Yan and Chong, Charing Ching Ning and Li, Zheng},

  issn = {1432-2218},
  journal = {Surgical endoscopy},
  publisher = {Springer},
  title = {{A robotic flexible endoscope with shared autonomy: a study of mockup cholecystectomy}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/d41d9f68bd45e963dfb506be08fefc0d63e8dc95},
  doi = {10.1007/s00464-019-07241-8}
}

@article{Pandya2019a,
  author = {Pandya, Abhilash and Eslamian, Shahab and Ying, Hao and Nokleby, Matthew and Reisner, Luke A.},

  doi = {10.3390/robotics8010009},
  issn = {2218-6581},
  journal = {Robotics},
  number = {1},
  title = {{A Robotic Recording and Playback Platform for Training Surgeons and Learning Autonomous Behaviors Using the da Vinci Surgical System}},
  volume = {8},
  year = {2019},
  research_field={TR},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/197a57d17f2c2f19b72bd93aac6ca8f6bd2f8f09},
  abstract = {This paper describes a recording and playback system developed using a da Vinci Standard Surgical System and research kit. The system records stereo laparoscopic videos, robot arm joint angles, and surgeon–console interactions in a synchronized manner. A user can then, on-demand and at adjustable speeds, watch stereo videos and feel recorded movements on the hand controllers of entire procedures or sub procedures. Currently, there is no reported comprehensive ability to capture expert surgeon movements and insights and reproduce them on hardware directly. This system has important applications in several areas: (1) training of surgeons, (2) collection of learning data for the development of advanced control algorithms and intelligent autonomous behaviors, and (3) use as a “black box” for retrospective error analysis. We show a prototype of such an immersive system on a clinically-relevant platform along with its recording and playback fidelity. Lastly, we convey possible research avenues to create better systems for training and assisting robotic surgeons.},
}

@article{Avinash2019b,
  author = {Avinash, Apeksha and Abdelaal, Alaa Eldin and Mathur, Prateek and Salcudean, Septimiu E.},

  doi = {10.1007/s11548-019-01955-9},
  issn = {1861-6410},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  number = {7},
  title = {{A “pickup” stereoscopic camera with visual-motor aligned control for the da Vinci surgical system: a preliminary study}},
  volume = {14},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/0521167e315f4426987cceba8c8396c4faaf800f}
}

@article{Li2019,
  author = {Li, Weibing and Song, Chengzhi and Li, Zheng},

  issn = {0278-0046},
  journal = {IEEE Transactions on Industrial Electronics},
  publisher = {IEEE},
  title = {{An Accelerated Recurrent Neural Network for Visual Servo Control of a Robotic Flexible Endoscope with Joint Limit Constraint}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/d7c57e7af91d5b46c7cda8308ed4a54dca35afb2},
  doi = {10.1109/TIE.2019.2959481},
  abstract = {In this article, a recurrent neural network (RNN) is accelerated and applied to visual servo control of a physically constrained robotic flexible endoscope. The robotic endoscope consists of a patient side manipulator of the da Vinci Research Kit platform and a flexible endoscope working as an end-effector. To automate the robotic endoscope, kinematic modeling for visual servoing is conducted, leading to a quadratic programming (QP) control framework incorporating kinematic and physical constraints of the robotic endoscope. To solve the QP problem and realize the vision-based control, an RNN accelerated to finite-time convergence by a sign-bipower activation function (SBPAF) is proposed. The finite-time convergence of the RNN is theoretically proved in the sense of Lyapunov, showing that the SBPAF activated RNN exhibits a faster convergence speed as compared with its predecessor. To validate the efficacy of the RNN model and the control framework, simulations are performed using a simulated flexible endoscope in the robot operating system. Physical experiment is then further performed to verify the feasibility of the RNN model and the control framework. Both simulation and experimental results demonstrate that the proposed RNN solution is effective to achieve visual servoing and handle physical limits of the robotic endoscope simultaneously.}
}

@article{Gondokaryono2019,
  author = {Gondokaryono, Radian A and Agrawal, Ankur and Munawar, Adnan and Nycz, Christopher J and Fischer, Gregory S},

  journal = {Acta Polytechnica Hungarica},
  number = {8},
  title = {{An Approach to Modeling Closed-Loop Kinematic Chain Mechanisms, Applied to Simulations of the da Vinci Surgical System}},
  volume = {16},
  year = {2019},
  research_field={SS},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/4439636218c25289c1e736563eb1c936ee07561e},
  doi = {10.12700/aph.16.8.2019.8.3},
  abstract = {Open-sourced kinematic models of the da Vinci Surgical System have previously been developed using serial chains for forward and inverse kinematics. However, these models do not describe the motion of every link in the closed-loop mechanism of the da Vinci manipulators; knowing the kinematics of all components in motion is essential for the foundation of modeling the system dynamics and implementing representative simulations. This paper proposes a modeling method of the closed-loop kinematics, using the existing da Vinci kinematics and an optical motion capture link length calibration. Resulting link lengths and DH parameters are presented and used as the basis for ROS-based simulation models. The models were simulated in RViz visualization simulation and Gazebo dynamics simulation. Additionally, the closed-loop kinematic chain was verified by comparing the remote center of motion location of simulation with the hardware. Furthermore, the dynamic simulation resulted in satisfactory joint stability and performance. All models and simulations are provided as an open-source package.},
}

@article{Ma2019b,
  author = {Ma, Xin and Song, Chengzhi and Chiu, Philip Waiyan and Li, Zheng},

  doi = {10.1109/LRA.2019.2895273},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  title = {{Autonomous Flexible Endoscope for Minimally Invasive Surgery With Enhanced Safety}},
  volume = {4},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/96fcfac950605112f51ae230f0cfe3448dc1d8f1},
  abstract = {Automation in robotic surgery has become an increasingly attractive topic. Although full automation remains fictional, task autonomy and conditional autonomy are highly achievable. Apart from the performance of task fulfillment, one major concern in robotic surgery is safety. In this paper, we present a flexible endoscope that can help to guide the minimally invasive surgical operations automatically. It is developed based on the tendon-driven continuum mechanism and is integrated with the da Vinci research kit. In total, the proposed flexible endoscope has six degree-of-freedoms. Visual servoing is adopted to automatically track the surgical instruments. During the tracking, optimal control method is used to minimize the motion and space occupation of the flexible endoscope, which will improve the safety of both the robot system and the assistants nearby. Compared with the existing rigid endoscope, both the experimental results and the user study results show that the proposed flexible endoscope has advantages of being safer and less space occupation without reducing its comfort level.}
}

@article{Haidegger2019a,
  author = {Haidegger, Tamas},

  doi = {10.1109/TMRB.2019.2913282},
  issn = {2576-3202},
  journal = {IEEE Transactions on Medical Robotics and Bionics},
  number = {2},
  title = {{Autonomy for Surgical Robots: Concepts and Paradigms}},
  volume = {1},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/0af00e43658fe5dacaec577a943619351b8ad230},
  abstract = {Robot-assisted and computer-integrated surgery provides innovative, minimally invasive solutions to heal complex injuries and diseases. The dominant portion of these surgical interventions has been performed with master–slave teleoperation systems, which are not capable of autonomous task execution or cognitive decision making. Much of the most advanced technologies foundered on the drawing boards or at the research labs for a long time, partially due to the fact that the surgical domain is resistant to the introduction of new hazards via the increased complexity of novel solutions. It has been seen with similar heavily regulated areas that internationally accepted standards can facilitate the adoption of new technologies in a safe manner. This paper reviews the existing autonomous capabilities of surgical robots, and investigates the major barriers of development presented by the lack of autonomy benchmarks and standards. The emerging safety standard environment is presented, as a key enabling factor to the commercialization of autonomous surgical robots. A practical scale is introduced to assess the level of autonomy of current and future surgical robots. Regarding the forthcoming robotic platforms, it is crucial to improve the transparency of the regulatory environment, streamline the standardization framework, and increase the social acceptance.}
}

@article{Ficuciello2019a,
  author = {Ficuciello, Fanny and Tamburrini, Guglielmo and Arezzo, Alberto and Villani, Luigi and Siciliano, Bruno},

  doi = {10.1515/pjbr-2019-0002},
  issn = {2081-4836},
  journal = {Paladyn, Journal of Behavioral Robotics},
  number = {1},
  title = {{Autonomy in surgical robots and its meaningful human control}},
  volume = {10},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/285dd7840ee2b5f80edb8ae551527f49e858652f},
  abstract = {Abstract This article focuses on ethical issues raised by increasing levels of autonomy for surgical robots. These ethical issues are explored mainly by reference to state-ofart case studies and imminent advances in Minimally Invasive Surgery (MIS) and Microsurgery. In both area, surgicalworkspace is limited and the required precision is high. For this reason, increasing levels of robotic autonomy can make a significant difference there, and ethically justified control sharing between humans and robots must be introduced. In particular, from a responsibility and accountability perspective suitable policies for theMeaningfulHuman Control (MHC) of increasingly autonomous surgical robots are proposed. It is highlighted how MHC should be modulated in accordance with various levels of autonomy for MIS and Microsurgery robots. Moreover, finer MHC distinctions are introduced to deal with contextual conditions concerning e.g. soft or rigid anatomical environments.},
}

@article{Colleoni2019a,
  author = {Colleoni, Emanuele and Moccia, Sara and Du, Xiaofei and {De Momi}, Elena and Stoyanov, Danail},

  doi = {10.1109/LRA.2019.2917163},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  title = {{Deep Learning Based Robotic Tool Detection and Articulation Estimation With Spatio-Temporal Layers}},
  volume = {4},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/0a566ec0eabf0e9ce602d757f8e98da80d0fce69},
  abstract = {Surgical-tool joint detection from laparoscopic images is an important but challenging task in computer-assisted minimally invasive surgery. Illumination levels, variations in background and the different number of tools in the field of view, all pose difficulties to algorithm and model training. Yet, such challenges could be potentially tackled by exploiting the temporal information in laparoscopic videos to avoid per frame handling of the problem. In this letter, we propose a novel encoder–decoder architecture for surgical instrument joint detection and localization that uses three-dimensional convolutional layers to exploit spatio-temporal features from laparoscopic videos. When tested on benchmark and custom-built datasets, a median Dice similarity coefficient of 85.1% with an interquartile range of 4.6% highlights performance better than the state of the art based on single-frame processing. Alongside novelty of the network architecture, the idea for inclusion of temporal information appears to be particularly useful when processing images with unseen backgrounds during the training phase, which indicates that spatio-temporal features for joint detection help to generalize the solution.},
}

@article{Cheng2019b,
  author = {Cheng, Zhuoqi and Dall'Alba, Diego and Foti, Simone and Mariani, Andrea and Chupin, Thibaud and Caldwell, Darwin G. and Ferrigno, Giancarlo and {De Momi}, Elena and Mattos, Leonardo S. and Fiorini, Paolo},

  doi = {10.3389/frobt.2019.00055},
  issn = {2296-9144},
  journal = {Frontiers in Robotics and AI},
  title = {{Design and Integration of Electrical Bio-impedance Sensing in Surgical Robotic Tools for Tissue Identification and Display}},
  volume = {6},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/f9c1c0c2ac503358cae7db3ad6ba639f7d20211e},
  abstract = {The integration of intra-operative sensors into surgical robots is a hot research topic since this can significantly facilitate complex surgical procedures by enhancing surgical awareness with real-time tissue information. However, currently available intra-operative sensing technologies are mainly based on image processing and force feedback, which normally require heavy computation or complicated hardware modifications of existing surgical tools. This paper presents the design and integration of electrical bio-impedance sensing into a commercial surgical robot tool, leading to the creation of a novel smart instrument that allows the identification of tissues by simply touching them. In addition, an advanced user interface is designed to provide guidance during the use of the system and to allow augmented-reality visualization of the tissue identification results. The proposed system imposes minor hardware modifications to an existing surgical tool, but adds the capability to provide a wealth of data about the tissue being manipulated. This has great potential to allow the surgeon (or an autonomous robotic system) to better understand the surgical environment. To evaluate the system, a series of ex-vivo experiments were conducted. The experimental results demonstrate that the proposed sensing system can successfully identify different tissue types with 100% classification accuracy. In addition, the user interface was shown to effectively and intuitively guide the user to measure the electrical impedance of the target tissue, presenting the identification results as augmented-reality markers for simple and immediate recognition.},
}

@article{Zhong2019,
  author = {Zhong, Fangxun and Wang, Yaqing and Wang, Zerui and Liu, Yun-Hui},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  publisher = {IEEE},
  title = {{Dual-Arm Robotic Needle Insertion With Active Tissue Deformation for Autonomous Suturing}},
  volume = {4},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a6bd27cee22069d3ccf56c3162cbbe78b2360c91},
  doi = {10.1109/LRA.2019.2913082},
  abstract = {A major issue for needle insertion into soft tissue during suturing is the induced tissue deformation that hinders the minimization of tip-target positioning error. In this letter, we present a new robot control framework to solve target deviation by integrating active deformation control. We characterize the motion behavior of the desired target under needle-tissue interaction by introducing the needle-induced deformation matrix. Note that the modeling does not require the exact knowledge of tissue or needle insertion properties. The unknown parameters are online updated during the insertion procedure by an adaptive estimator via sensor-based measurement. A closed-loop controller is then proposed for dual-arm robotic execution upon image guidance. The dual-arm control aims to regulate a feature vector concerning the tip-target alignment to ensure target reachability. The feasibility of the proposed algorithm is studied via simulations and experiments on different biological tissues to simulate robotic minimally-invasive suturing using the da Vinci Research Kit as the control platform.}
}

@article{Sozzi2019,
  author = {Sozzi, Alessio and Bonf{\`{e}}, Marcello and Farsoni, Saverio and {De Rossi}, Giacomo and Muradore, Riccardo},

  journal = {Electronics},
  number = {9},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title = {{Dynamic Motion Planning for Autonomous Assistive Surgical Robots}},
  volume = {8},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/e094bd8275dcec62865a473975ae00e6a76b080e},
  doi = {10.3390/electronics8090957},
  abstract = {The paper addresses the problem of the generation of collision-free trajectories for a robotic manipulator, operating in a scenario in which obstacles may be moving at non-negligible velocities. In particular, the paper aims to present a trajectory generation solution that is fully executable in real-time and that can reactively adapt to both dynamic changes of the environment and fast reconfiguration of the robotic task. The proposed motion planner extends the method based on a dynamical system to cope with the peculiar kinematics of surgical robots for laparoscopic operations, the mechanical constraint being enforced by the fixed point of insertion into the abdomen of the patient the most challenging aspect. The paper includes a validation of the trajectory generator in both simulated and experimental scenarios.},
}

@article{Wu2019,
  author = {Wu, Chuhao and Cha, Jackie and Sulek, Jay and Zhou, Tian and Sundaram, Chandru P and Wachs, Juan and Yu, Denny},

  issn = {0018-7208},
  journal = {Human Factors},
  publisher = {SAGE Publications Sage CA: Los Angeles, CA},
  title = {{Eye-Tracking Metrics Predict Perceived Workload in Robotic Surgical Skills Training}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a3fb9cb50ee91a2d5cb8ee3f1ace5203ce8204b6},
  doi = {10.1177/0018720819874544},
  abstract = {Objective The aim of this study is to assess the relationship between eye-tracking measures and perceived workload in robotic surgical tasks. Background Robotic techniques provide improved dexterity, stereoscopic vision, and ergonomic control system over laparoscopic surgery, but the complexity of the interfaces and operations may pose new challenges to surgeons and compromise patient safety. Limited studies have objectively quantified workload and its impact on performance in robotic surgery. Although not yet implemented in robotic surgery, minimally intrusive and continuous eye-tracking metrics have been shown to be sensitive to changes in workload in other domains. Methods Eight surgical trainees participated in 15 robotic skills simulation sessions. In each session, participants performed up to 12 simulated exercises. Correlation and mixed-effects analyses were conducted to explore the relationships between eye-tracking metrics and perceived workload. Machine learning classifiers were used to determine the sensitivity of differentiating between low and high workload with eye-tracking features. Results Gaze entropy increased as perceived workload increased, with a correlation of .51. Pupil diameter and gaze entropy distinguished differences in workload between task difficulty levels, and both metrics increased as task level difficulty increased. The classification model using eye-tracking features achieved an accuracy of 84.7% in predicting workload levels. Conclusion Eye-tracking measures can detect perceived workload during robotic tasks. They can potentially be used to identify task contributors to high workload and provide measures for robotic surgery training. Application Workload assessment can be used for real-time monitoring of workload in robotic surgical training and provide assessments for performance and learning.},
}

@article{Saracino2019a,
  author = {Saracino, Arianna and Deguet, Anton and Staderini, Fabio and Boushaki, Mohamed Nassim and Cianchi, Fabio and Menciassi, Arianna and Sinibaldi, Edoardo},

  doi = {10.1002/rcs.1999},
  issn = {1478-5951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {4},
  title = {{Haptic feedback in the da Vinci Research Kit (dVRK): A user study based on grasping, palpation, and incision tasks}},
  volume = {15},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/17526c58ac9e6ac4289a53e28aaa777fb7400dd0},
  abstract = {It was suggested that the lack of haptic feedback, formerly considered a limitation for the da Vinci robotic system, does not affect robotic surgeons because of training and compensation based on visual feedback. However, conclusive studies are still missing, and the interest in force reflection is rising again.},
}

@article{Hong2019c,
  author = {Hong, Nhayoung and Kim, Myungjoon and Lee, Chiwon and Kim, Sungwan},

  doi = {10.1007/s11517-018-1902-4},
  issn = {0140-0118},
  journal = {Medical & Biological Engineering & Computing},
  number = {3},
  pages = {=},
  title = {{Head-mounted interface for intuitive vision control and continuous surgical operation in a surgical robot system}},
  volume = {57},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/aa5abb28646fe8d4325c8c9bf84d986f20800c2d}
}

@inproceedings{Ettorre2019b,
  author = {Ettorre, C. D. and Stilli, A and Dwyer, G and Neves, J B and Tran, M and Stoyanov, D},

  booktitle = {IEEE International Conference on Intelligent Robots and Systems},
  doi = {10.1109/IROS40897.2019.8967789},
  isbn = {9781728140049},
  issn = {21530866},
  publisher = {IEEE},
  title = {{Semi-Autonomous Interventional Manipulation using Pneumatically Attachable Flexible Rails}},
  year = {2019},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/f60f67bdd453a5b337bc3e3ad9d6eba6d91c6e33},
  abstract = {During laparoscopic surgery, tissues frequently need to be retracted and mobilized for manipulation or visualisation. State-of-the-art robotic platforms for minimally invasive surgery (MIS) typically rely on rigid tools to interact with soft tissues. Such tools offer a very narrow contact surface thus applying relatively large forces that can lead to tissue damage, posing a risk for the success of the procedure and ultimately for the patient. In this paper, we show how the use of Pneumatically Attachable Flexible (PAF) rail, a vacuum-based soft attachment for laparoscopic applications, can reduce such risk by offering a larger contact surface between the tool and the tissue. Ex vivo experiments are presented investigating the short- and long-term effects of different levels of vacuum pressure on the tissues surface. These experiments aim at evaluating the best trade-off between applied pressure, potential damage, task duration and connection stability. A hybrid control system has been developed to perform and investigate the organ repositioning task using the proposed system. The task is only partially automated allowing the surgeon to be part of the control loop. A gradient-based planning algorithm is integrated with learning from teleoperation algorithm which allows the robot to improve the learned trajectory. The use of Similar Smooth Path Repositioning (SSPR) algorithm is proposed to improve a demonstrated trajectory based on a known cost function. The results obtained show that a smoother trajectory allows to decrease the minimum level of pressure needed to guarantee active suction during PAF positioning and placement.},
}

@inproceedings{Selvaggio2019a,
  author = {Selvaggio, Mario and {Ghalamzan Esfahani}, Amir and Moccia, Rocco and Ficuciello, Fanny and Siciliano, Bruno},

  booktitle = {IEEE/RSJ International Conference Intelligent Robotic System},
  title = {{Haptic-guided shared control for needle grasping optimization in minimally invasive robotic surgery}},
  year = {2019},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/44a5c3422c6de7559c5ec10bcf93f04c2d301693},
  doi = {10.1109/IROS.1992.594560}
}

@article{Rau2019,
  author = {Rau, Anita and Edwards, P J Eddie and Ahmad, Omer F and Riordan, Paul and Janatka, Mirek and Lovat, Laurence B and Stoyanov, Danail},

  issn = {1861-6410},
  journal = {International journal of computer assisted radiology and surgery},
  number = {7},
  publisher = {Springer},
  title = {{Implicit domain adaptation with conditional generative adversarial networks for depth prediction in endoscopy}},
  volume = {14},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/5069f827e5a165330aef63707f973e68bd96f37d},
  doi = {10.1007/s11548-019-01962-w},
  abstract = {PurposeColorectal cancer is the third most common cancer worldwide, and early therapeutic treatment of precancerous tissue during colonoscopy is crucial for better prognosis and can be curative. Navigation within the colon and comprehensive inspection of the endoluminal tissue are key to successful colonoscopy but can vary with the skill and experience of the endoscopist. Computer-assisted interventions in colonoscopy can provide better support tools for mapping the colon to ensure complete examination and for automatically detecting abnormal tissue regions.MethodsWe train the conditional generative adversarial network pix2pix, to transform monocular endoscopic images to depth, which can be a building block in a navigational pipeline or be used to measure the size of polyps during colonoscopy. To overcome the lack of labelled training data in endoscopy, we propose to use simulation environments and to additionally train the generator and discriminator of the model on unlabelled real video frames in order to adapt to real colonoscopy environments.ResultsWe report promising results on synthetic, phantom and real datasets and show that generative models outperform discriminative models when predicting depth from colonoscopy images, in terms of both accuracy and robustness towards changes in domains.ConclusionsTraining the discriminator and generator of the model on real images, we show that our model performs implicit domain adaptation, which is a key step towards bridging the gap between synthetic and real data. Importantly, we demonstrate the feasibility of training a single model to predict depth from both synthetic and real images without the need for explicit, unsupervised transformer networks mapping between the domains of synthetic and real data.},
}

@inproceedings{Cheng2019a,
  author = {Cheng, Zhuoqi and Dall'Alba, Diego and Caldwell, Darwin G and Fiorini, Paolo and Mattos, Leonardo S},

  booktitle = {International Conference on Electrical Bioimpedance},
  publisher = {Springer},
  title = {{Design and Integration of Electrical Bio-Impedance Sensing in a Bipolar Forceps for Soft Tissue Identification: A Feasibility Study}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/08cb7275459e899d0c64ee4d8eee8b78591275a9},
  doi = {10.1088/1742-6596/224/1/011001},
  abstract = {The XIVth International Conference on Electrical Bioimpedance, held in conjunction with the 11th Conference on Biomedical Applications of EIT (ICEBI & EIT 2010), took place from 4–8 April 2010 in the Reitz Union of the University of Florida, in Gainesville, USA. This was the first time since its inception in 1969 that the ICEBI was held in the United States. As in the last three conferences (Graz 2007, Gdansk 2004 and Oslo 2001) the ICEBI was combined with the Conference on Biomedical Applications of EIT – a mutually beneficial approach for those interested in the biophysics of tissue electrical properties and those developing imaging methods and measurement systems based thereon. This year's conference was particularly notable for the many papers presented on hybrid and emerging imaging techniques such as Electric Property Tomography (EPT), Magneto Acoustic Tomography using Magnetic Induction (MAT-MI) and Magnetic Resonance Electrical Impedance Tomography (MREIT); sessions on Cell Scale Impedance, Cardiac Impedance and Imaging Neural Activity. About 180 scientists from all over the world attended, including keynote speakers on topics of fundamental electromagnetic principles (Jaakko Malmivuo), Electrical Source and Impedance Imaging (Bin He), Bioimpedance applications in Nephrology, (Nathan Levin), and Lung EIT (Gerhard Wolf). The papers in this volume are peer-reviewed four-page works selected from over 150 presented in oral and poster sessions at the conference. The complete program is available from the conference website.},
}

@inproceedings{DiFlumeri2019,
  author = {{Di Flumeri}, Gianluca and Aric{\`{o}}, Pietro and Borghini, Gianluca and Sciaraffa, Nicolina and Ronca, Vincenzo and Vozzi, Alessia and Storti, Silvia Francesca and Menegaz, Gloria and Fiorini, Paolo and Babiloni, Fabio},

  booktitle = {International Symposium on Human Mental Workload: Models and Applications},
  publisher = {Springer},
  title = {{EEG-Based Workload Index as a Taxonomic Tool to Evaluate the Similarity of Different Robot-Assisted Surgery Systems}},
  year = {2019},
  research_field={TR},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/5b4bc2edf19576e6979d1497e49aeef8c7a82726},
  doi = {10.1007/978-3-030-91408-0}
}

@article{OSullivan2019a,
  author = {O'Sullivan, Shane and Nevejans, Nathalie and Allen, Colin and Blyth, Andrew and Leonard, Simon and Pagallo, Ugo and Holzinger, Katharina and Holzinger, Andreas and Sajid, Mohammed Imran and Ashrafian, Hutan},

  doi = {10.1002/rcs.1968},
  issn = {14785951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {1},
  title = {{Legal, regulatory, and ethical frameworks for development of standards in artificial intelligence (AI) and autonomous robotic surgery}},
  volume = {15},
  year = {2019},
  research_field={RE},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/1683a3a8d4be1c72bf086251ffc02263674e9d53},
  abstract = {This paper aims to move the debate forward regarding the potential for artificial intelligence (AI) and autonomous robotic surgery with a particular focus on ethics, regulation and legal aspects (such as civil law, international law, tort law, liability, medical malpractice, privacy and product/device legislation, among other aspects).}
}

@inproceedings{Wang2019b,
  author = {Wang, Congcong and Mohammed, Ahmed Kedir and Cheikh, Faouzi Alaya and Beghdadi, Azeddine and Elle, Ole Jacob},

  booktitle = {Medical Imaging 2019: Image Processing},
  publisher = {International Society for Optics and Photonics},
  title = {{Multiscale deep desmoking for laparoscopic surgery}},
  year = {2019},
  research_field={IM},
  data_type={RI},
  semanticscholar = {https://www.semanticscholar.org/paper/0e5b822eb7e7d3858e4ab6676bee9f73aa652d1d},
  doi = {10.1117/12.2532364},
}

@article{Richter2019a,
  author = {Richter, Florian and Orosco, Ryan K and Yip, Michael C},

  journal = {arXiv preprint arXiv:1903.02090},
  title = {{Open-sourced reinforcement learning environments for surgical robotics}},
  year = {2019},
  url = {https://arxiv.org/abs/1903.02090},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/8c247678b0f652b9c8f01d200767b6b5c5199cc0},
  abstract = {Reinforcement Learning (RL) is a machine learning framework for artificially intelligent systems to solve a variety of complex problems. Recent years has seen a surge of successes solving challenging games and smaller domain problems, including simple though non-specific robotic manipulation and grasping tasks. Rapid successes in RL have come in part due to the strong collaborative effort by the RL community to work on common, open-sourced environment simulators such as OpenAI's Gym that allow for expedited development and valid comparisons between different, state-of-art strategies. In this paper, we aim to bridge the RL and the surgical robotics communities by presenting the first open-sourced reinforcement learning environments for surgical robotics, called dVRL. Through the proposed RL environment, which are functionally equivalent to Gym, we show that it is easy to prototype and implement state-of-art RL algorithms on surgical robotics problems that aim to introduce autonomous robotic precision and accuracy to assisting, collaborative, or repetitive tasks during surgery. Learned policies are furthermore successfully transferable to a real robot. Finally, combining dVRL with the over 40+ international network of da Vinci Surgical Research Kits in active use at academic institutions, we see dVRL as enabling the broad surgical robotics community to fully leverage the newest strategies in reinforcement learning, and for reinforcement learning scientists with no knowledge of surgical robotics to test and develop new algorithms that can solve the real-world, high-impact challenges in autonomous surgery.}
}

@article{Stilli2019b,
  author = {Stilli, Agostino and Dimitrakakis, Emmanouil and D'Ettorre, Claudia and Tran, Maxine and Stoyanov, Danail},

  doi = {10.1109/LRA.2019.2894499},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  title = {{Pneumatically Attachable Flexible Rails for Track-Guided Ultrasound Scanning in Robotic-Assisted Partial Nephrectomy—A Preliminary Design Study}},
  volume = {4},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/9176c34c53547893b05fcbc14ad093685f699f23},
  abstract = {Robotic-assisted partial nephrectomy is a surgical operation in which part of a kidney is removed typically because of the presence of a mass. Pre-operative and intraoperative imaging techniques are used to identify and outline the target mass, thus the margins of the resection area on the kidney surface. Drop-in ultrasound probes are used to acquire intraoperative images: the probe is inserted through a trocar port, grasped with a robotic-assisted laparoscopic gripper and swiped on the kidney surface. Multiple swipes are performed to define the resection area. This is marked swipe by swipe using an electrocautery tool. During this procedure the probe often requires repositioning because of slippage from the target organ surface. Furthermore, the localization can be inaccurate when the target mass is in locations particularly hard to reach, and thus kidney repositioning could be required. A highly skilled surgeon is typically required to successfully perform this pre-operatory procedure. We propose a novel approach for the navigation of drop-in ultrasound probes: the use of pneumatically attachable flexible rails to enable swift, effortless, and accurate track-guided scanning of the kidney. The proposed system attaches on the kidney side surface with the use of a series of bio-inspired vacuum suckers. In this letter, the design of the proposed system and its use in robotic-assisted partial nephrectomy are presented for the first time.},
}

@inproceedings{Mariani2019a,
  author = {Mariani, Andrea and Colaci, Giorgia and Sanna, Nicole and Vendrame, Vendrame and Menciassi, Arianna and {De Momi}, Elena},

  booktitle = {Proceeding of Joint Workshop on Computer/Robot Assisted Surgery (CRAS)},
  title = {{Comparing Users Performances in a Virtual Reality Surgical Task under Different Camera Control Modalities: a Pilot Study Towards the Introduction of an Autonomous Camera}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2}
}

@article{Nagy2019b,
  author = {Nagy, Tam{\'{a}}s D and Haidegger, Tam{\'{a}}s},

  journal = {2019 IEEE 13th International Symposium on Applied Computational Intelligence and Informatics ( SACI)},
  title = {{Recent Advances in Robot-Assisted Surgery: Soft Tissue Contact Identification}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/64215bc2dd771c6810bcee78624de2487bac17c5},
  doi = {10.1109/SACI46893.2019.9111599},
  abstract = {Robot-Assisted Minimally Invasive Surgery (RAMIS) is becoming standard-of-care in western medicine. RAMIS offers better patient outcome compared to traditional open surgery, however, the surgeons’ ability to identify the tissues with the sense of touch is missing from most robotic systems. Regarding haptic feedback, the most promising diagnostic technique is probably palpation; a physical contact examination method through which information can be gathered about the underlying structures by gently pressing with the fingers. In open surgery, palpation is widely used to identify blood vessels, tendons or even tumors; and the knowledge on the exact location of such elements is often crucial with respect to the outcome of the intervention. This paper presents a review of the actual research directions in the field of palpation in RAMIS.},
}

@article{Dardona2019,
  author = {Dardona, Tareq and Eslamian, Shahab and Reisner, Luke A and Pandya, Abhilash},

  journal = {Robotics},
  number = {2},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title = {{Remote presence: Development and usability evaluation of a head-mounted display for camera control on the da vinci surgical system}},
  volume = {8},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/d531c046c4319465e1265f297343879d55765a25},
  doi = {10.3390/ROBOTICS8020031},
  abstract = {This paper describes the development of a new method to control the camera arm of a surgical robot and create a better sense of remote presence for the surgeon. The current surgical systems are entirely controlled by the surgeon, using hand controllers and foot pedals to manipulate either the instrument or the camera arms. The surgeon must pause the operation to move the camera arm to obtain a desired view and then resume the operation. The camera and tools cannot be moved simultaneously, leading to interrupted and unnatural movements. These interruptions can lead to medical errors and extended operation times. In our system, the surgeon controls the camera arm by his natural head movements while being immersed in a 3D-stereo view of the scene with a head-mounted display (HMD). The novel approach enables the camera arm to be maneuvered based on sensors of the HMD. We implemented this method on a da Vinci Standard Surgical System using the HTC Vive headset along with the Unity engine and the Robot Operating System framework. This paper includes the result of a subjective six-participant usability study that compares the workload of the traditional clutched camera control method against the HMD-based control. Initial results indicate that the system is usable, stable, and has a lower physical and mental workload when using the HMD control method.},
}

@article{Nakawala2019a,
  author = {Nakawala, Hirenkumar and {De Momi}, Elena and Tzemanaki, Antonia and Dogramadzi, Sanja and Russo, Andrea and Catellani, Michele and Bianchi, Roberto and {De Cobelli}, Ottavio and Sideridis, Aristotelis and Papacostas, Emmanuel},

  issn = {1729-8814},
  journal = {International Journal of Advanced Robotic Systems},
  number = {4},
  publisher = {SAGE Publications Sage UK: London, England},
  title = {{Requirements elicitation for robotic and computer-assisted minimally invasive surgery}},
  volume = {16},
  year = {2019},
  research_field={RE},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/ad1562ff37728723d22ecc72e0158bd7cb04dd59},
  doi = {10.1177/1729881419865805},
  abstract = {The robotic surgical systems and computer-assisted technologies market has seen impressive growth over the last decades, but uptake by end-users is still scarce. The purpose of this article is to provide a comprehensive and informed list of the end-user requirements for the development of new generation robot- and computer-assisted surgical systems and the methodology for eliciting them. The requirements were elicited, in the frame of the EU project SMARTsurg, by conducting interviews on use cases of chosen urology, cardiovascular and orthopaedics procedures, tailored to provide clinical foundations for scientific and technical developments. The structured interviews resulted in detailed requirement specifications which are ranked according to their priorities. Paradigmatic surgical scenarios support the use cases.},
}

@article{NagyneElek2019,
  author = {{Nagyn{\'{e}} Elek}, Ren{\'{a}}ta and Haidegger, Tam{\'{a}}s},

  issn = {1785-8860},
  journal = {Acta Polytechnica Hungarica},
  number = {8},
  publisher = {{\'{O}}buda University},
  title = {{Robot-Assisted Minimally Invasive Surgical Skill Assessment—Manual and Automated Platforms}},
  volume = {16},
  year = {2019},
  research_field={RE},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/0925224abe277b9b368988d6f5fc2cf1ac3bd9b3},
  doi = {10.12700/aph.16.8.2019.8.9},
  abstract = {The practice of Robot-Assisted Minimally Invasive Surgery (RAMIS) requires extensive skills from the human surgeons due to the special input device control, such as moving the surgical instruments, use of buttons, knobs, foot pedals and so. The global popularity of RAMIS created the need to objectively assess surgical skills, not just for quality assurance reasons, but for training feedback as well. Nowadays, there is still no routine surgical skill assessment happening during RAMIS training and education in the clinical practice. In this paper, a review of the manual and automated RAMIS skill assessment techniques is provided, focusing on their general applicability, robustness and clinical relevance.},
}

@article{Krishnan2019,
  author = {Krishnan, Sanjay and Garg, Animesh and Liaw, Richard and Thananjeyan, Brijen and Miller, Lauren and Pokorny, Florian T and Goldberg, Ken},

  issn = {0278-3649},
  journal = {The International Journal of Robotics Research},
  number = {2-3},
  publisher = {SAGE Publications Sage UK: London, England},
  title = {{SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards}},
  volume = {38},
  year = {2019},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/d720d601d5b835937b96b68d7119b239d45776f0},
  doi = {10.1177/0278364918784350},
  abstract = {We present sequential windowed inverse reinforcement learning (SWIRL), a policy search algorithm that is a hybrid of exploration and demonstration paradigms for robot learning. We apply unsupervised learning to a small number of initial expert demonstrations to structure future autonomous exploration. SWIRL approximates a long time horizon task as a sequence of local reward functions and subtask transition conditions. Over this approximation, SWIRL applies Q-learning to compute a policy that maximizes rewards. Experiments suggest that SWIRL requires significantly fewer rollouts than pure reinforcement learning and fewer expert demonstrations than behavioral cloning to learn a policy. We evaluate SWIRL in two simulated control tasks, parallel parking and a two-link pendulum. On the parallel parking task, SWIRL achieves the maximum reward on the task with 85% fewer rollouts than Q-learning, and one-eight of demonstrations needed by behavioral cloning. We also consider physical experiments on surgical tensioning and cutting deformable sheets using a da Vinci surgical robot. On the deformable tensioning task, SWIRL achieves a 36% relative improvement in reward compared with a baseline of behavioral cloning with segmentation.}
}

@inproceedings{Qian2019,
  author = {Qian, Long and Deguet, Anton and Kazanzides, Peter},

  booktitle = {The Hamlyn Symposium on Medical Robotics},
  doi = {10.31256/HSMR2019.47},
  publisher = {The Hamlyn Centre, Faculty of Engineering, Imperial College London},
  title = {{dVRK-XR: Mixed Reality Extension for da Vinci Research Kit}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/011be270619c0a58ced1a641ac9c3ecfe650933a},
  abstract = {INTRODUCTION The da Vinci Research Kit (dVRK) has become a widely adopted research platform for surgical robotics research since its initial public release in 2012. To date, it has been installed at 35 institutes worldwide. In the past decade, research interest in mixed reality (including augmented reality and virtual reality) has increased among both the robotics and medical communities [1], due to the improved usability of mixed reality headsets and the much reduced hardware cost. In this paper, we describe dVRK-XR (https://github.com/jhu-dvrk/dvrkxr), an extension to the dVRK open source package that facilitates the integration of mixed reality in surgical robotics research.}
}

@inproceedings{Mariani2019,
  author = {Mariani, Andrea and Pellegrini, Edoardo and Menciassi, Arianna and {De Momi}, Elena},

  booktitle = {The Hamlyn Symposium on Medical Robotics},
  title = {{Simulation-based Adaptive Training for Robot-Assisted Surgery: a Feasibility Study on Medical Residents}},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  doi = {10.31256/hsmr2018}
}

@inproceedings{Schwaner2019,
  author = {Schwaner, Kim Lindberg and Dall'Alba, Diego and Cheng, Zhuoqi and Mattos, Leonardo S and Fiorini, Paolo and Savarimuthu, Thiusius Rajeeth},

  booktitle = {The Hamlyn Symposium on Medical Robotics},
  publisher = {The Hamlyn Centre, Faculty of Engineering, Imperial College London},
  title = {{Robotically assisted electrical bio-impedance measurements for soft tissue characterization: a feasibility study}},
  year = {2019},
  research_field={HW},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  doi = {10.31256/hsmr2018}
}

@inproceedings{Caccianiga2019,
  author = {Caccianiga, Guido and Mariani, Andrea and {De Momi}, Elena and Brown, Jeremy D},

  booktitle = {The Hamlyn Symposium on Medical Robotics},
  title = {{Virtual Reality Training in Robot-Assisted Surgery: a Novel Experimental Setup for Skill Transfer Evaluation}},
  year = {2019},
  research_field={TR},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  doi = {10.31256/hsmr2018}
}

@article{Selvaggio2019,
  author = {Selvaggio, Mario and Fontanelli, Giuseppe Andrea and Marrazzo, Vincenzo Romano and Bracale, Umberto and Irace, Andrea and Breglio, Giovanni and Villani, Luigi and Siciliano, Bruno and Ficuciello, Fanny},

  issn = {1478-5951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {3},
  publisher = {Wiley Online Library},
  title = {{The MUSHA underactuated hand for robot‐aided minimally invasive surgery}},
  volume = {15},
  year = {2019},
  research_field={HW},
  data_type={RI},
  semanticscholar = {https://www.semanticscholar.org/paper/785b3406bd95d3ff62fc7300e4cb9af56669d4c7},
  doi = {10.1002/rcs.1981},
  abstract = {Keyhole surgery is characterized by loss of dexterity of surgeon's movements because of the limited workspace, nonintuitive motor skills of the surgical systems, and loss of tactile sensation that may lead to tissue damage and bad execution of the tasks.}
}

@article{Moradi2019b,
  author = {Moradi, Hamid and Tang, Shuo and Salcudean, Septimiu E.},

  doi = {10.1109/LRA.2019.2897168},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  title = {{Toward Robot-Assisted Photoacoustic Imaging: Implementation Using the da Vinci Research Kit and Virtual Fixtures}},
  volume = {4},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/212607b44ecfa372ff14c73c292e262077a9d9a4},
  abstract = {Photoacoustic imaging of the prostate is challenging due to the limited access and limited acoustic windows to the prostate gland. We aim to develop intraoperative prostate photoacoustic imaging using the da Vinci robotic system and a pick-up ultrasound transducer that can be easily picked up and manipulated by the robot. We propose a new approach in which the da Vinci robot is programmed to acquire trajectories in a shared control configuration with “virtual fixtures”; the pick-up transducer is controlled so that it stays parallel to a single axis defined as the tomography axis, and its translation is fixed to a single plane normal to this axis. The surgeon controls the transducer motion on the tissue along this virtual fixture while the laser is fired and photoacoustic data are collected periodically. The RMS errors of the photoacoustic tomography images are 0.06 a.u. This study confirms that intraoperative da Vinci robot-assisted photoacoustic imaging with a pick-up transducer is feasible.}
}

@article{Podolsky2019,
  author = {Podolsky, Dale J and Diller, Eric and Fisher, David M and {Wong Riff}, Karen W and Looi, Thomas and Drake, James M and Forrest, Christopher R},

  issn = {1932-6181},
  journal = {Journal of Medical Devices},
  number = {1},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  title = {{Utilization of Cable Guide Channels for Compact Articulation Within a Dexterous Three Degrees-of-Freedom Surgical Wrist Design}},
  volume = {13},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/13cf228dabea6e334b99ded2e4c26774c9cac8a5},
  doi = {10.1115/1.4041591},
  abstract = {Pin-jointed wrist mechanisms provide compact articulation for surgical robotic applications, but are difficult to miniaturize at scales suitable for small body cavity surgery. Solid surface cable guide channels, which eliminate the need for pulleys and reduce overall length to facilitate miniaturization, were developed within a three-degree-of-freedom cable-driven pin-jointed wrist mechanism. A prototype was 3D printed in steel at 5 mm diameter. Friction generated by the guide channels was experimentally tested to determine increases in cable tension during constant cable velocity conditions. Cable tension increased exponentially from 0 to 37% when the wrist pitched from 0 deg to 90 deg. The shape of the guide channel groove and angle, where the cable exits the channel impacts the magnitude of cable tension. A spring tensioning and cam actuation mechanism were developed to account for changing cable circuit path lengths during wrist pitch. This work shows that pulley-free cable wrist mechanisms can facilitate miniaturization below current feasible sizes while retaining compact articulation at the expense of increases in friction under constant cable velocity conditions.}
}

@article{Nakawala2019,
  author = {Nakawala, Hirenkumar and Bianchi, Roberto and Pescatori, Laura Erica and {De Cobelli}, Ottavio and Ferrigno, Giancarlo and {De Momi}, Elena},

  issn = {1861-6410},
  journal = {International journal of computer assisted radiology and surgery},
  number = {4},
  publisher = {Springer},
  title = {{“Deep-Onto” network for surgical workflow and context recognition}},
  volume = {14},
  year = {2019},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e843c8b159eab14df15473e138ee72927eea0ab4},
  doi = {10.1007/s11548-018-1882-8}
}

@inproceedings{Song2018,
  author = {Song, Chengzhi and Mok, Ivan Shuenshing and Chiu, Philip Waiyan and Li, Zheng},

  booktitle = {2018 13th World Congress on Intelligent Control and Automation (WCICA)},
  isbn = {1538673460},
  publisher = {IEEE},
  title = {{A Novel Tele-operated Flexible Manipulator Based on the da-Vinci Research Kit}},
  year = {2018},
  research_field={},
  data_type={}
}

@inproceedings{Wang2018d,
  author = {Wang, Ziheng and Fey, Ann Majewicz},

  booktitle = {2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  isbn = {1538636468},
  publisher = {IEEE},
  title = {{SATR-DL: Improving surgical skill assessment and task recognition in robot-assisted surgery with deep neural networks}},
  year = {2018},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/3b53eec8706908a0f8486773f39f7ab3c7735546}
}

@inproceedings{Mariani2018a,
  author = {Mariani, Andrea and Pellegrini, Edoardo and Enayati, Nima and Kazanzides, Peter and Vidotto, Marco and {De Momi}, Elena},

  booktitle = {2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  doi = {10.1109/EMBC.2018.8512728},
  isbn = {978-1-5386-3646-6},
  publisher = {IEEE},
  title = {{Design and Evaluation of a Performance-based Adaptive Curriculum for Robotic Surgical Training: a Pilot Study}},
  year = {2018},
  research_field={TR},
  data_type={KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/0086375564dec6aea1f9bce3f095cf084142bb64},
  abstract = {Training with simulation systems has become a primary alternative for learning the fundamental skills of robotic surgery. However, there exists no consensus regarding a standard training curriculum: sessions defined a priori by expert trainers or self-directed by the trainees feature lack of consistency. This study proposes an adaptive approach that structures the curriculum on the basis of an objective assessment of the trainee’s performance. The work comprised an experimental session with 12 participants performing training on virtual reality tasks with the da Vinci Research Kit surgical console. Half of the subjects self-managed their training session, while the others underwent the adaptive training. The final performance of the latter trainees was found to be higher compared to the former (p=0.002), showing how outcome-based, dynamic designs could constitute a promising advance in robotic surgical training.}
}

@inproceedings{Fontanelli2018f,
  author = {Fontanelli, G. A. and Selvaggio, M. and Ferro, M. and Ficuciello, F. and Vendittelli, M. and Siciliano, B.},

  booktitle = {2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob)},
  doi = {10.1109/BIOROB.2018.8487187},
  isbn = {978-1-5386-8183-1},
  publisher = {IEEE},
  title = {{A V-REP Simulator for the da Vinci Research Kit Robotic Platform}},
  year = {2018},
  research_field={SS},
  data_type={KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/1c8d436444369da2bff7c601196456c83a52e2cd},
  abstract = {In this work we present a V-REP simulator for the da Vinci Research Kit (dVRK). The simulator contains a full robot kinematic model and integrated sensors. A robot operating system (ROS) interface has been created for easy use and development of common software components. Moreover, several scenes have been implemented to illustrate the performance and potentiality of the developed simulator. Both the simulator and the example scenes are available to the community as an open source software.}
}

@inproceedings{Ji2018b,
  author = {Ji, Jessica J. and Krishnan, Sanjay and Patel, Vatsal and Fer, Danyal and Goldberg, Ken},

  booktitle = {2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/COASE.2018.8560468},
  isbn = {978-1-5386-3593-3},
  publisher = {IEEE},
  title = {{Learning 2D Surgical Camera Motion From Demonstrations}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/6838cac0edc58fceb6b708767f49dce52f14d598},
  abstract = {Automating camera movement during robot-assisted surgery has the potential to reduce burden on surgeons and remove the need to manually move the camera. An important sub-problem is automatic viewpoint selection, proposing camera poses that focus on important anatomical features. We use the 6 DoF Stewart Platform Research Kit (SPRK) to move the environment with a fixed endoscope, as a dual to moving the endoscope itself, to study camera motion in surgical robotics. To provide demonstrations, we link the platform's control directly to the da Vinci Research Kit (dVRK) master control system and allow control of the platform using the same pedals and tools as a clinical movable endoscope. We propose a probabilistic model that identifies image features that “dwell” close to the camera's focal point in expert demonstrations. Our experiments consider a surgical debridement scenario on silicone phantoms with inclusions of varying color and shape. We evaluate the extent to which the system correctly segments candidate debridement targets (box accuracy) and correctly ranks those targets (rank accuracy). For debridement of a single uniquely colored inclusion, the box accuracy is 80% and the rank accuracy is 100% after 100 training data points. For debridement of multiple inclusions of the same color, the box accuracy is 70.8% and the rank accuracy is 100% after 100 training data points. For debridement of inclusions of a particular shape, the box accuracy is 70.5% and the rank accuracy is 90% after 100 training data points. A demonstration video is available at: https://vimeo.com/260362958}
}

@inproceedings{Nagy2018e,
  author = {Nagy, Tamas D. and Takacs, Marta and Rudas, Imre J. and Haidegger, Tamas},

  booktitle = {2018 IEEE 16th World Symposium on Applied Machine Intelligence and Informatics (SAMI)},
  doi = {10.1109/SAMI.2018.8323986},
  isbn = {978-1-5386-4772-1},
  publisher = {IEEE},
  title = {{Surgical subtask automation — Soft tissue retraction}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/01d22cf342c050549d6b76100b05186a3915fc57},
}

@inproceedings{El-Saig2018,
  author = {El-Saig, D{\'{a}}vid and Elek, Ren{\'{a}}ta Nagyn{\'{e}} and Haidegger, Tam{\'{a}}s},

  booktitle = {2018 IEEE 18th International Symposium on Computational Intelligence and Informatics (CINTI)},
  isbn = {172811117X},
  publisher = {IEEE},
  title = {{A Graphical Tool for Parsing and Inspecting Surgical Robotic Datasets}},
  year = {2018},
  research_field={TR},
  data_type={ED},
  semanticscholar = {https://www.semanticscholar.org/paper/1f4d522a1e8425d6d881e041899b63c1eefe3e91},
  doi = {10.1109/cinti45972.2018}
}

@inproceedings{Gordon2018a,
  author = {Gordon, Alex and Looi, Thomas and Drake, James and Forrest, Christopher R.},

  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2018.8460797},
  isbn = {978-1-5386-3081-5},
  publisher = {IEEE},
  title = {{An Ultrasonic Bone Cutting Tool for the da Vinci Research Kit}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/f8fb56493a0c18aeef3feb49d713d5f91f7cea0f}
}

@inproceedings{Enayati2018b,
  author = {Enayati, Nima and Okamura, Allison M. and Mariani, Andrea and Pellegrini, Edoardo and Coad, Margaret M. and Ferrigno, Giancarlo and {De Momi}, Elena},

  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2018.8463168},
  isbn = {978-1-5386-3081-5},
  publisher = {IEEE},
  title = {{Robotic Assistance-as-Needed for Enhanced Visuomotor Learning in Surgical Robotics Training: An Experimental Study}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/8e328be0b5675a27f3ab9ff084c82fcbf49eb00d}
}

@inproceedings{Salman2018b,
  author = {Salman, Hadi and Ayvali, Elif and Srivatsan, Rangaprasad Arun and Ma, Yifei and Zevallos, Nicolas and Yasin, Rashid and Wang, Long and Simaan, Nabil and Choset, Howie},

  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2018.8460936},
  isbn = {978-1-5386-3081-5},
  publisher = {IEEE},
  title = {{Trajectory-Optimized Sensing for Active Search of Tissue Abnormalities in Robotic Surgery}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/eb302b3cf98ff8597eaa49163e5a03d5be86eeb5},
  abstract = {In this work, we develop an approach for guiding robots to automatically localize and find the shapes of tumors and other stiff inclusions present in the anatomy. Our approach uses Gaussian processes to model the stiffness distribution and active learning to direct the palpation path of the robot. The palpation paths are chosen such that they maximize an acquisition function provided by an active learning algorithm. Our approach provides the flexibility to avoid obstacles in the robot's path, incorporate uncertainties in robot position and sensor measurements, include prior information about location of stiff inclusions while respecting the robot-kinematics. To the best of our knowledge this is the first work in literature that considers all the above conditions while localizing tumors. The proposed framework is evaluated via simulation and experimentation on three different robot platforms: 6-DoF industrial arm, da Vinci Research Kit (dVRK), and the Insertable Robotic Effector Platform (IREP). Results show that our approach can accurately estimate the locations and boundaries of the stiff inclusions while reducing exploration time.},
}

@inproceedings{Kamikawa2018,
  author = {Kamikawa, Yasuhisa and Enayati, Nima and Okamura, Allison M},

  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1538630818},
  publisher = {IEEE},
  title = {{Magnified Force Sensory Substitution for Telemanipulation via Force-Controlled Skin Deformation}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/11c1cf0e6c0481bc99443c8777643905e2740855},
  doi = {10.1109/icra36916.2018}
}

@inproceedings{Ozguner2018a,
  author = {{\"{O}}zg{\"{u}}ner, Orhan and Hao, Ran and Jackson, Russell C and Shkurti, Tom and Newman, Wyatt and Cavusoglu, M Cenk},

  booktitle = {2018 IEEE international conference on robotics and automation (ICRA)},
  isbn = {1538630818},
  publisher = {IEEE},
  title = {{Three-dimensional surgical needle localization and tracking using stereo endoscopic image streams}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/11c1cf0e6c0481bc99443c8777643905e2740855},
  doi = {10.1109/icra36916.2018}
}

@inproceedings{DEttorre2018b,
  author = {D'Ettorre, C. and Dwyer, G. and Du, X. and Chadebecq, F. and Vasconcelos, F. and {De Momi}, E. and Stoyanov, D.},

  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2018.8461200},
  isbn = {978-1-5386-3081-5},
  publisher = {IEEE},
  title = {{Automated Pick-Up of Suturing Needles for Robotic Surgical Assistance}},
  year = {2018},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a3ff700c2352966565ba8fd9bae4bc6ce9a78905},
  abstract = {Robot-assisted laparoscopic prostatectomy (RALP) is a treatment for prostate cancer that involves complete or nerve sparing removal prostate tissue that contains cancer. After removal the bladder neck is successively sutured directly with the urethra. The procedure is called urethrovesical anastomosis and is one of the most dexterity demanding tasks during RALP. Two suturing instruments and a pair of needles are used in combination to perform a running stitch during urethrovesical anastomosis. While robotic instruments provide enhanced dexterity to perform the anastomosis, it is still highly challenging and difficult to learn. In this paper, we presents a vision-guided needle grasping method for automatically grasping the needle that has been inserted into the patient prior to anastomosis. We aim to automatically grasp the suturing needle in a position that avoids hand-offs and immediately enables the start of suturing. The full grasping process can be broken down into: a needle detection algorithm; an approach phase where the surgical tool moves closer to the needle based on visual feedback; and a grasping phase through path planning based on observed surgical practice. Our experimental results show examples of successful autonomous grasping that has the potential to simplify and decrease the operational time in RALP by assisting a small component of urethrovesical anastomosis.},
}

@inproceedings{Seita2018a,
  author = {Seita, Daniel and Krishnan, Sanjay and Fox, Roy and McKinley, Stephen and Canny, John and Goldberg, Ken},

  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2018.8460583},
  isbn = {978-1-5386-3081-5},
  publisher = {IEEE},
  title = {{Fast and Reliable Autonomous Surgical Debridement with Cable-Driven Robots Using a Two-Phase Calibration Procedure}},
  year = {2018},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/ce97028f90ad3379c49a6a6b0f7beccca18caef7},
  abstract = {Automating precision subtasks such as debridement (removing dead or diseased tissue fragments) with Robotic Surgical Assistants (RSAs) such as the da Vinci Research Kit (dVRK) is challenging due to inherent nOnlinearities in cable-driven systems. We propose and evaluate a novel two-phase coarse-to-fine calibration method. In Phase I (coarse), we place a red calibration marker on the end effector and let it randomly move through a set of open-loop trajectories to obtain a large sample set of camera pixels and internal robot end-effector configurations. This coarse data is then used to train a Deep Neural Network (DNN) to learn the coarse transformation bias. In Phase II (fine), the bias from Phase I is applied to move the end -effector toward a small set of specific target points on a printed sheet. For each target, a human operator manually adjusts the end -effector position by direct contact (not through teleoperation) and the residual compensation bias is recorded. This fine data is then used to train a Random Forest (RF) to learn the fine transformation bias. Subsequent experiments suggest that without calibration, position errors average 4.55mm. Phase I can reduce average error to 2.14mm and the combination of Phase I and Phase II can reduces average error to 1.08mm. We apply these results to debridement of raisins and pumpkin seeds as fragment phantoms. Using an endoscopic stereo camera with standard edge detection, experiments with 120 trials achieved average success rates of 94.5 %, exceeding prior results with much larger fragments (89.4%) and achieving a speedup of 2.1x, decreasing time per fragment from 15.8 seconds to 7.3 seconds. Source code, data, and videos are available at https://sites.google.com/view/calib-icra/.},
}

@inproceedings{Wang2018c,
  author = {Wang, Zerui and Li, Xiang and Navarro-Alarcon, David and Liu, Yun-hui},

  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2018.8593543},
  isbn = {978-1-5386-8094-0},
  publisher = {IEEE},
  title = {{A Unified Controller for Region-reaching and Deforming of Soft Objects}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/db09007f2b5fae22085e21d8fefbbbad0adbddaa},
  abstract = {Emerging applications of robotic manipulation of deformable objects have opened up new challenges in robot control. While several control techniques have been developed to manipulate deformable objects, the performance of existing methods is commonly limited by two issues: 1) implicit assumption that the physical contact between the end-effector and the object is always maintained, and 2) requirements of exact parameters of deformation model, which are difficult to obtain. This paper presents a new control scheme for robotic manipulation of deformable objects, which allows the robot to automatically contact then actively deform the deformable object by assessing the status of deformation in real time. Instead of designing multiple controllers and switching among them, the proposed method smoothly and stably integrates two control phases (i.e. region reaching and active deforming) into a single controller. The stability of the closed-loop system is rigorously proved with the consideration of the uncertain deformation model and uncalibrated cameras. Hence, the proposed control scheme enhances the autonomous capability of active deformable object manipulation. Experimental studies are conducted with different initial conditions to demonstrate the performance of the proposed controller.}
}

@inproceedings{Hao2018a,
  author = {Hao, Ran and Ozguner, Orhan and Cavusoglu, M. Cenk},

  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2018.8594471},
  isbn = {978-1-5386-8094-0},
  publisher = {IEEE},
  title = {{Vision-Based Surgical Tool Pose Estimation for the da Vinci{\textregistered} Robotic Surgical System}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/9ae709a669b30167c7f999b1c90a51d40f2cca70},
  abstract = {This paper presents an approach to surgical tool tracking using stereo vision for the da Vinci® Surgical Robotic System. The proposed method is based on robot kinematics, computer vision techniques and Bayesian state estimation. The proposed method employs a silhouette rendering algorithm to create virtual images of the surgical tool by generating the silhouette of the defined tool geometry under the da Vinci® robot endoscopes. The virtual rendering method provides the tool representation in image form, which makes it possible to measure the distance between the rendered tool and real tool from endoscopic stereo image streams. Particle Filter algorithm employing the virtual rendering method is then used for surgical tool tracking. The tracking performance is evaluated on an actual da Vinci® surgical robotic system and a ROS/Gazebo-based simulation of the da Vinci® system.},
}

@inproceedings{Gu2018b,
  author = {Gu, Yun and Hu, Yang and Zhang, Lin and Yang, Jie and Yang, Guang-Zhong},

  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1538680947},
  publisher = {IEEE},
  title = {{Cross-scene suture thread parsing for robot assisted anastomosis based on joint feature learning}},
  year = {2018},
  research_field={},
  data_type={}
}

@inproceedings{Wu2018a,
  author = {Wu, Jie Ying and Chen, Zihan and Deguet, Anton and Kazanzides, Peter},

  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2018.8594139},
  isbn = {978-1-5386-8094-0},
  publisher = {IEEE},
  title = {{FPGA-Based Velocity Estimation for Control of Robots with Low-Resolution Encoders}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/7c661912adc0c750f30e5281e16bb5b5f0fee1ea},
  abstract = {Robot control algorithms often rely on measurements of robot joint velocities, which can be estimated by measuring the time between encoder edges. When encoder edges occur infrequently, such as at low velocities and/or with low resolution encoders, this measurement delay may affect the stability of closed-loop control. This is evident in both the joint position control and Cartesian impedance control of the da Vinci Research Kit (dVRK), which contains several low-resolution encoders. We present a hardware-based method that gives more frequent velocity updates and is not affected by common encoder imperfections such as non-uniform duty cycles and quadrature phase error. The proposed method measures the time between consecutive edges of the same type but, unlike prior methods, is implemented for the rising and falling edges of both channels. Additionally, it estimates acceleration to enable software compensation of the measurement delay. The method is shown to improve Cartesian impedance control of the dVRK.}
}

@inproceedings{Fontanelli2018e,
  author = {Fontanelli, Giuseppe Andrea and Yang, Guang-Zhong and Siciliano, Bruno},

  booktitle = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2018.8593607},
  isbn = {978-1-5386-8094-0},
  publisher = {IEEE},
  title = {{A Comparison of Assistive Methods for Suturing in MIRS}},
  year = {2018},
  research_field={AU},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/354efa12f13fb4e78916e1519e4cd7276d7238cc},
  abstract = {In Minimally Invasive Robotic Surgery (MIRS) a robot is interposed between the surgeon and the surgical site to increase the precision, dexterity, and to reduce surgeon's effort and cognitive load with respect to the standard laparoscopic interventions. However, the modern robotic systems for MIRS are still based on the traditional telemanipulation paradigm, e.g. the robot behaviour is fully under surgeon's control, and no autonomy or assistance is implemented. In this work, supervised and shared controllers have been developed in a vision-free, human-in-the-Ioop, control framework to help surgeon during a surgical suturing procedure. Experiments conducted on the da Vinci Research Kit robot proves the effectiveness of the method indicating also the guidelines for improving results.}
}

@inproceedings{Zevallos2018b,
  author = {Zevallos, Nicolas and Srivatsan, Rangaprasad Arun and Salman, Hadi and Li, Lu and Qian, Jianing and Saxena, Saumya and Xu, Mengyun and Patath, Kartik and Choset, Howie},

  booktitle = {2018 International Symposium on Medical Robotics (ISMR)},
  doi = {10.1109/ISMR.2018.8333310},
  isbn = {978-1-5386-2512-5},
  publisher = {IEEE},
  title = {{A surgical system for automatic registration, stiffness mapping and dynamic image overlay}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/9786b89d7c56dad60e44a3b64bce3ce7aa397398},
  abstract = {In this paper we develop a surgical system using the da Vinci research kit (dVRK) that is capable of autonomously searching for tumors and dynamically displaying the tumor location using augmented reality. Such a system has the potential to quickly reveal the location and shape of tumors and visually overlay that information to reduce the cognitive overload of the surgeon. To the best of our knowledge, our approach is one of the first to incorporate state-of-the-art methods in registration, force sensing and tumor localization into a unified surgical system. First, the preoperative model is registered to the intra-operative scene using a Bingham distribution-based filtering approach. An active level set estimation is then used to find the location and the shape of the tumors. We use a recently developed miniature force sensor to perform the palpation. The estimated stiffness map is then dynamically overlaid onto the registered preoperative model of the organ. We demonstrate the efficacy of our system by performing experiments on phantom prostate models with embedded stiff inclusions.},
}

@inproceedings{Patel2018b,
  author = {Patel, Vatsal and Krishnan, Sanjay and Goncalves, Aimee and Goldberg, Ken},

  booktitle = {2018 International Symposium on Medical Robotics (ISMR)},
  doi = {10.1109/ISMR.2018.8333300},
  isbn = {978-1-5386-2512-5},
  publisher = {IEEE},
  title = {{SPRK: A low-cost stewart platform for motion study in surgical robotics}},
  year = {2018},
  research_field={HW},
  data_type={KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/92e4de2584eba6bd785ecd1963e391240c446b14},
  abstract = {To simulate body organ motion due to breathing, heart beats, or peristaltic movements, we designed a low-cost, miniaturized SPRK (Stewart Platform Research Kit) to translate and rotate phantom tissue. This platform is 20cm χ 20cm χ 10cm to fit in the workspace of a da Vinci Research Kit (DVRK) surgical robot and costs $250, two orders of magnitude less than a commercial Stewart platform. The platform has a range of motion of ± 1.27 cm in translation along x, y, and z, and of ± 15o in roll, pitch, and yaw directions. The platform also has motion modes for sinusoidal motion, breathing-inspired motion, and multi-axis motion. Modular mounts facilitate pattern cutting and debridement experiments. The platform's positional controller has a time-constant of 0.2 seconds and the root-mean-square error is 1.22 mm, 1.07 mm, and 0.20 mm in x, y, and z directions respectively. Construction directions, CAD models, and control software for the platform are available at github.com/BerkeleyAutomation/sprk.},
}

@inproceedings{Patel2018c,
  author = {Patel, Vatsal and Krishnan, Sanjay and Goncalves, Aimee and Chen, Carolyn and Boyd, Walter Doug and Goldberg, Ken},

  booktitle = {2018 International Symposium on Medical Robotics (ISMR)},
  doi = {10.1109/ISMR.2018.8333301},
  isbn = {978-1-5386-2512-5},
  publisher = {IEEE},
  title = {{Using intermittent synchronization to compensate for rhythmic body motion during autonomous surgical cutting and debridement}},
  year = {2018},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/608298f059b806a3c2e75b09ef619bd029321d5e},
  abstract = {Anatomical structures are rarely static during a surgical procedure due to breathing, heartbeats, and peristaltic movements. Inspired by observing an expert surgeon, we propose an intermittent synchronization with the extrema of the rhythmic motion (i.e., the lowest velocity windows). We performed 2 experiments: (1) pattern cutting and (2) debridement. In (1), we found that the intermittent synchronization approach, while 1.8× slower than tracking motion, is significantly more robust to noise and control latency, and it reduces the max cutting error by 2.6× except when motion is along 3 or more orthogonal axes. In (2), a baseline approach with no synchronization succeeds in 62% of debridement attempts while intermittent synchronization achieves an 80% success rate.},
}

@inproceedings{Chalasani2018c,
  author = {Chalasani, Preetham and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H},

  booktitle = {2018 Second IEEE International Conference on Robotic Computing (IRC)},
  isbn = {1538646528},
  publisher = {IEEE},
  title = {{A Computational Framework for Complementary Situational Awareness (CSA) in Surgical Assistant Robots}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/c0d91e78a238998aa19d11d06381716c1b45344a}
}

@article{Allan2018a,
  author = {Allan, M. and Ourselin, S. and Hawkes, D. J. and Kelly, J. D. and Stoyanov, D.},

  doi = {10.1109/TMI.2018.2794439},
  issn = {0278-0062},
  journal = {IEEE Transactions on Medical Imaging},
  number = {5},
  title = {{3-D Pose Estimation of Articulated Instruments in Robotic Minimally Invasive Surgery}},
  volume = {37},
  year = {2018},
  research_field={IM},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/27595732ddb379053a96128d2c8b3f67e0d578ea},
  abstract = {Estimating the 3-D pose of instruments is an important part of robotic minimally invasive surgery for automation of basic procedures as well as providing safety features, such as virtual fixtures. Image-based methods of 3-D pose estimation provide a non-invasive low cost solution compared with methods that incorporate external tracking systems. In this paper, we extend our recent work in estimating rigid 3-D pose with silhouette and optical flow-based features to incorporate the articulated degrees-of-freedom (DOFs) of robotic instruments within a gradient-based optimization framework. Validation of the technique is provided with a calibrated ex-vivo study from the da Vinci Research Kit (DVRK) robotic system, where we perform quantitative analysis on the errors each DOF of our tracker. Additionally, we perform several detailed comparisons with recently published techniques that combine visual methods with kinematic data acquired from the joint encoders. Our experiments demonstrate that our method is competitively accurate while relying solely on image data.},
}

@inproceedings{Mazomenos2018,
  author = {Mazomenos, E and Watson, Dave and Kotorov, Rado and Stoyanov, D},

  booktitle = {8th Joint Workshop on New Technology for Computer/Robot Assisted Surgery (CRAS 2018)},
  title = {{Gesture Classification in Robotic Surgery using Recurrent Neural Networks with Kinematic Information}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2}
}

@article{Chu2018,
  author = {Chu, Xiangyu and Yip, Hoi Wut and Cai, Yuanpei and Chung, Tsz Yin and Moran, Stuart and Au, K W Samuel},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {4},
  publisher = {IEEE},
  title = {{A Compliant Robotic Instrument With Coupled Tendon Driven Articulated Wrist Control for Organ Retraction}},
  volume = {3},
  year = {2018},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/5a68b4e773343a997d0faecc576ab707663c6a61},
  doi = {10.1109/LRA.2018.2863373},
  abstract = {Organ retraction is a common and important task in minimally invasive surgery. It is a surgical technique to push aside or manipulate tissues/organs to improve the access and visualization of the surgical sites. Typical manual retractors are rigid and without any articulation. Physicians often struggle with minimizing the force applying onto the tissue while positioning the retractor to maintain optimal exposure. In this letter, we propose a novel compliant robotic organ retractor with coupled tendon driven articulated wrist control to address the aforementioned clinical risks. The compliant retractor exploits the buckling principle of a continuum bending beam mechanism, allowing it to interact with organ safely and smoothly. We also present a new tendon-driven, high torsional strength articulated joint and corresponding kinematics model to address the lack of dexterity issue. For the instrument control, a general algorithm framework is proposed to design the controller gain systematically, while eliminating the “disturbance” leaking caused by the coupled multitendon driven mechanism. Initial prototypes were built and integrated with the state-of-the-art surgical robotic platform, da Vinci Research Kit for basic functional evaluation. Preliminary simulation and experimental results demonstrate the capability of the proposed device for organ retraction. It is our hope that this novel instrument will become a new benchmark for organ retraction in terms of safety, precision, and dexterity.}
}

@article{Fontanelli2018d,
  author = {Fontanelli, Giuseppe Andrea and Selvaggio, Mario and Buonocore, Luca Rosario and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},

  doi = {10.1109/LRA.2018.2809443},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  title = {{A New Laparoscopic Tool With In-Hand Rolling Capabilities for Needle Reorientation}},
  volume = {3},
  year = {2018},
  research_field={HW},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/55f43d2f0929d31b26ea113c2a02a94ba9902ce5},
  abstract = {In laparoscopic minimally invasive robotic surgery, a teleoperated robot is interposed between the patient and the surgeon. Despite the robot aid, the manipulation capabilities of surgical instruments are far from those of the human hand. In this letter, we want to make a step forward toward robotic solutions that can improve manipulation capabilities of the surgical instruments. A new concept of needle-driver tool is presented, which takes inspiration from the human hand model. The idea is to modify a standard laparoscopic tool by introducing an additional degree of freedom, which allows in-hand reorientation of the suturing needle. A 3D printed prototype has been built to validate the tool design. The improved manipulation capabilities have been assessed quantitatively by evaluating a weighted dexterity index along a single stitch trajectory. Moreover, a comparison between our tool and a standard needle driver has been done in terms of time required for the execution of a complete suturing sequence.}
}

@article{Alambeigi2018b,
  author = {Alambeigi, Farshid and Wang, Zerui and Hegeman, Rachel and Liu, Yun-Hui and Armand, Mehran},

  doi = {10.1109/LRA.2018.2863376},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {4},
  title = {{A Robust Data-Driven Approach for Online Learning and Manipulation of Unmodeled 3-D Heterogeneous Compliant Objects}},
  volume = {3},
  year = {2018},
  research_field={AU},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/3e8cda355d9400b04e8ea61da5e11dbb7df231a1},
  abstract = {We present a generic data-driven method to address the problem of manipulating a three-dimensional (3-D) compliant object (CO) with heterogeneous physical properties in the presence of unknown disturbances. In this study, we do not assume a prior knowledge about the deformation behavior of the CO and type of the disturbance (e.g., internal or external). We also do not impose any constraints on the CO's physical properties (e.g., shape, mass, and stiffness). The proposed optimal iterative algorithm incorporates the provided visual feedback data to simultaneously learn and estimate the deformation behavior of the CO in order to accomplish the desired control objective. To demonstrate the capabilities and robustness of our algorithm, we fabricated two novel heterogeneous compliant phantoms and performed experiments on the da Vinci Research Kit. Experimental results demonstrated the adaptivity, robustness, and accuracy of the proposed method and, therefore, its suitability for a variety of medical and industrial applications involving CO manipulation.}
}

@article{Zhang2018,
  author = {Zhang, Dandan and Xiao, Bo and Huang, Baoru and Zhang, Lin and Liu, Jindong and Yang, Guang-Zhong},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{A self-adaptive motion scaling framework for surgical robot remote control}},
  volume = {4},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/489c54a29d979ddf6dfe9830e0f5b14f334d8ab7},
  doi = {10.1109/LRA.2018.2890200},
  abstract = {Master–slave control is a common form of human–robot interaction for robotic surgery. To ensure seamless and intuitive control, a mechanism of self-adaptive motion scaling during teleoperaton is proposed in this letter. The operator can retain precise control when conducting delicate or complex manipulation, while the movement to a remote target is accelerated via adaptive motion scaling. The proposed framework consists of three components: 1) situation awareness, 2) skill level awareness, and 3) task awareness. The self-adaptive motion scaling ratio allows the operators to perform surgical tasks with high efficiency, forgoing the need of frequent clutching and instrument repositioning. The proposed framework has been verified on a da Vinci Research Kit to assess its usability and robustness. An in-house database is constructed for offline model training and parameter estimation, including both the kinematic data obtained from the robot and visual cues captured through the endoscope. Detailed user studies indicate that a suitable motion-scaling ratio can be obtained and adjusted online. The overall performance of the operators in terms of control efficiency and task completion is significantly improved with the proposed framework.},
}

@article{Shahbazi2018b,
  author = {Shahbazi, Mahya and Atashzar, Seyed Farokh and Patel, Rajni V},

  issn = {1939-1412},
  journal = {IEEE transactions on haptics},
  number = {3},
  publisher = {IEEE},
  title = {{A systematic review of multilateral teleoperation systems}},
  volume = {11},
  year = {2018},
  research_field={RE},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/08ce1195368e8a6b18b0b15721f440dccb5cd7e1},
  doi = {10.1109/TOH.2018.2818134},
  abstract = {While conventional bilateral Single-Master/Single-Slave (SM/SS) teleoperation systems have received considerable attention during the past several decades, multilateral teleoperation is only recently being studied. Unlike an SM/SS system, which consists of one master-slave set, multilateral teleoperation frameworks involve a minimum of three agents in order to remotely perform a task. This paper presents an overview of multilateral teleoperation systems and classifies the existing state-of-the-art architectures based on topologies, applications, and closed-loop stability analysis. For each category, the review discusses control strategies used for various architectures as well as control challenges (e.g., closed-loop instability as a result of a delay in the communication network) for each methodology.}
}

@article{Pachtrachai2018,
  author = {Pachtrachai, Krittin and Vasconcelos, Francisco and Chadebecq, Fran{\c{c}}ois and Allan, Max and Hailes, Stephen and Pawar, Vijay and Stoyanov, Danail},

  issn = {0090-6964},
  journal = {Annals of biomedical engineering},
  number = {10},
  publisher = {Springer},
  title = {{Adjoint transformation algorithm for hand–eye calibration with applications in robotic assisted surgery}},
  volume = {46},
  year = {2018},
  research_field={IM},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/56c22000c9c75ff2e33ca079a293ed462d30be88},
  doi = {10.1007/s10439-018-2097-4},
  abstract = {Hand–eye calibration aims at determining the unknown rigid transformation between the coordinate systems of a robot arm and a camera. Existing hand–eye algorithms using closed-form solutions followed by iterative non-linear refinement provide accurate calibration results within a broad range of robotic applications. However, in the context of surgical robotics hand–eye calibration is still a challenging problem due to the required accuracy within the millimetre range, coupled with a large displacement between endoscopic cameras and the robot end-effector. This paper presents a new method for hand–eye calibration based on the adjoint transformation of twist motions that solves the problem iteratively through alternating estimations of rotation and translation. We show that this approach converges to a solution with a higher accuracy than closed form initializations within a broad range of synthetic and real experiments. We also propose a stereo hand–eye formulation that can be used in the context of both our proposed method and previous state-of-the-art closed form solutions. Experiments with real data are conducted with a stereo laparoscope, the KUKA robot arm manipulator, and the da Vinci surgical robot, showing that both our new alternating solution and the explicit representation of stereo camera hand–eye relations contribute to a higher calibration accuracy.},
}

@article{Qian2018a,
  title = {ARssist: augmented reality on a head-mounted display for the first assistant in robotic surgery},
  author = {Qian, Long and Deguet, Anton and Kazanzides, Peter},

  doi = {10.1049/htl.2018.5065},
  year = {2018},
  date = {2018-10-01},
  journal = {IET Healthcare Technology Letters},
  volume = {5},
  number = {5},
  pages = {194-200},
  keywords = {arssist, dvrk, hmd},
  pubstate = {published},
  tppubtype = {article},
  semanticscholar = {https://www.semanticscholar.org/paper/bf1419f685e5cb2df513b3de64b4bae8d7b7e807},
  abstract = {In robot-assisted laparoscopic surgery, the first assistant (FA) is responsible for tasks such as robot docking, passing necessary materials, manipulating hand-held instruments, and helping with trocar planning and placement. The performance of the FA is critical for the outcome of the surgery. The authors introduce ARssist, an augmented reality application based on an optical see-through head-mounted display, to help the FA perform these tasks. ARssist offers (i) real-time three-dimensional rendering of the robotic instruments, hand-held instruments, and endoscope based on a hybrid tracking scheme and (ii) real-time stereo endoscopy that is configurable to suit the FA's hand–eye coordination when operating based on endoscopy feedback. ARssist has the potential to help the FA perform his/her task more efficiently, and hence improve the outcome of robot-assisted laparoscopic surgeries.},
}

@misc{Bodenstedt2018,
  author = {Bodenstedt, Sebastian and Allan, Max and Agustinos, Anthony and Du, Xiaofei and Garcia-Peraza-Herrera, Luis and Kenngott, Hannes and Kurmann, Thomas and M{\"{u}}ller-Stich, Beat and Ourselin, Sebastien and Pakhomov, Daniil},

  booktitle = {arXiv preprint arXiv:1805.02475},
  title = {{Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/69c48b29fcd88575745f33b5aa1839d496e85f54},
  abstract = {The emergence of large language models (LLMs) represents a significant technological shift within the scientific ecosystem, particularly within the field of artificial intelligence (AI). This paper examines structural changes in the AI research landscape using a dataset of arXiv preprints (cs.AI) from 2021 through 2025. Given the rapid pace of AI development, the preprint ecosystem has become a critical barometer for real-time scientific shifts, often preceding formal peer-reviewed publication by months or years. By employing a multi-stage data collection and enrichment pipeline in conjunction with LLM-based institution classification, we analyze the evolution of publication volumes, author team sizes, and academic--industry collaboration patterns. Our results reveal an unprecedented surge in publication output following the introduction of ChatGPT, with academic institutions continuing to provide the largest volume of research. However, we observe that academic--industry collaboration is still suppressed, as measured by a Normalized Collaboration Index (NCI) that remains significantly below the random-mixing baseline across all major subfields. These findings highlight a continuing institutional divide and suggest that the capital-intensive nature of generative AI research may be reshaping the boundaries of scientific collaboration.}
}

@article{Fard2018,
  author = {Fard, Mahtab J and Ameri, Sattar and {Darin Ellis}, R and Chinnam, Ratna B and Pandya, Abhilash K and Klein, Michael D},

  issn = {1478-5951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {1},
  publisher = {Wiley Online Library},
  title = {{Automated robot‐assisted surgical skill evaluation: Predictive analytics approach}},
  volume = {14},
  year = {2018},
  research_field={TR},
  data_type={SD and ED}
}

@article{Alambeigi2018e,
  author = {Alambeigi, Farshid and Wang, Zerui and Hegeman, Rachel and Liu, Yun-Hui and Armand, Mehran},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Autonomous data-driven manipulation of unknown anisotropic deformable tissues using unmodelled continuum manipulators}},
  volume = {4},
  year = {2018},
  research_field={AU},
  data_type={RI and KD and DD and SD and ED}
}

@inproceedings{Foti2018,
  author = {Foti, Simone and Mariani, Andrea and Chupin, Thibaud and Dall'alba, Diego and Cheng, Zhuoqi and Mattos, Leonardo and Caldwell, Darwin and Fiorini, Paolo and {De Momi}, Elena and Ferrigno, Giancarlo},

  booktitle = {Conference on New Technologies for Computer and Robot Assisted Surgery},
  title = {{Advanced User Interface for Augmented Information Display on Endoscopic Surgical Images}},
  year = {2018},
  research_field={HW},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/26a4530f1404285e2fa978266b648091f978b5ac},
  doi = {10.1007/s10151-015-1327-0}
}

@article{Wang2018b,
  author = {Wang, Ziheng and Fey, Ann Majewicz},

  issn = {1861-6410},
  journal = {International journal of computer assisted radiology and surgery},
  number = {12},
  publisher = {Springer},
  title = {{Deep learning with convolutional neural network for objective skill evaluation in robot-assisted surgery}},
  volume = {13},
  year = {2018},
  research_field={},
  data_type={}
}

@article{Xue2018,
  author = {Xue, Renfeng and Ren, Bingyin and Huang, Jiaqing and Yan, Zhiyuan and Du, Zhijiang},

  journal = {Sensors},
  number = {7},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title = {{Design and evaluation of FBG-based tension sensor in laparoscope surgical robots}},
  volume = {18},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/ed6d088267cdff4d95a54bfe6d321dac26e9984d},
  doi = {10.3390/s18072067},
  abstract = {Due to the narrow space and a harsh chemical environment in the sterilization processes for the end-effector of surgical robots, it is difficult to install and integrate suitable sensors for the purpose of effective and precise force control. This paper presents an innovative tension sensor for estimation of grasping force in our laparoscope surgical robot. The proposed sensor measures the tension of cable using fiber gratings (FBGs) which are pasted in the grooves on the inclined cantilevers of the sensor. By exploiting the stain measurement characteristics of FBGs, the small deformation of the inclined cantilevers caused by the cable tension can be measured. The working principle and the sensor model are analyzed. Based on the sensor model, the dimensions of the sensor are designed and optimized. A dedicated experimental setup is established to calibrate and test the sensor. The results of experiments for estimation the grasping force validate the sensor.},
}

@article{Francis2018a,
  author = {Francis, P. and Eastwood, K. W. and Bodani, V. and Looi, T. and Drake, J. M.},

  doi = {10.1007/s10439-018-2036-4},
  issn = {0090-6964},
  journal = {Annals of Biomedical Engineering},
  number = {10},
  title = {{Design, Modelling and Teleoperation of a 2 mm Diameter Compliant Instrument for the da Vinci Platform}},
  volume = {46},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/b38ca2141dbce93499c30616440421cf95d64932}
}

@article{Quek2018,
  author = {Quek, Zhan Fan and Provancher, William R and Okamura, Allison M},

  issn = {1939-1412},
  journal = {IEEE transactions on haptics},
  number = {2},
  publisher = {IEEE},
  title = {{Evaluation of skin deformation tactile feedback for teleoperated surgical tasks}},
  volume = {12},
  year = {2018},
  research_field={HW},
  data_type={KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/859fb2550a459f9b7446150be914e526d82baccf},
  doi = {10.1109/TOH.2018.2873398},
  abstract = {During interaction with objects using a tool, we experience force and tactile feedback. One form of tactile feedback is local fingerpad skin deformation. In this paper, we provide haptic feedback to users of a teleoperation system through a skin deformation tactile feedback device. The device is able to provide tangential and normal skin deformation in a coupled manner, and is designed so that users can grasp it with a precision grip using multiple fingerpads. By applying skin deformation feedback on multiple fingerpads, the device is able to provide multi-degree-of-freedom interaction force direction and magnitude information to the user. To evaluate the effectiveness of this approach for the performance of teleoperated manipulation tasks, we performed a study in which 20 participants used a teleoperation system to perform one of two manipulation tasks (peg transfer and tube connection) using force feedback, skin deformation feedback, and the combination of both feedback. Results showed that participants are able to use all feedback to improve task performance compared to the case without haptic feedback, although the degree of improvement depended on the nature of the task. The feedback also improved situation awareness, felt consistent with prior experience, and did not affect concentration on the task, as reported by participants.}
}

@article{Sharon2018a,
  author = {Sharon, Yarden and Nisky, Ilana},

  doi = {10.1142/S2424905X18410088},
  issn = {2424-905X},
  journal = {Journal of Medical Robotics Research},
  title = {{Expertise, Teleoperation, and Task Constraints Affect the Speed–Curvature–Torsion Power Law in RAMIS}},
  volume = {03},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/7ea957233110b7762cee3809114810167b9a7d18},
  abstract = {Quantitative characterization of surgical movements can improve the quality of patient care by informing the development of new training protocols for surgeons, and the design and control of surgical robots. Here, we focus on the relationship between the speed of movement and its geometry that was extensively studied in computational motor control. In three-dimensional movements, this relationship is defined by a family of speed–curvature–torsion power laws, such as the one-sixth power law. We present a novel characterization of open and teleoperated suturing movements using the speed–curvature–torsion power-law analysis. We fitted the gain and the exponents of this power law to suturing movements of participants with different levels of surgical experience in open (using sensorized forceps) and teleoperated (using the da Vinci Research Kit/da Vinci Surgical System) conditions from two different datasets. We found that expertise and teleoperation significantly affected the gain and exponents of the power law, and that there were large differences between different segments of movement. These results confirm that the relationship between the speed and geometry of surgical movements is indicative of surgical skill, open a new avenue for understanding the effect of teleoperation on the movements of surgeons, and lay the foundation for the development of new algorithms for automatic segmentation of surgical tasks.},
}

@article{Li2018a,
  author = {Li, Zhaoshuo and Tong, Irene and Metcalf, Leo and Hennessey, Craig and Salcudean, Septimiu E.},

  doi = {10.1109/LRA.2018.2809512},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  title = {{Free Head Movement Eye Gaze Contingent Ultrasound Interfaces for the da Vinci Surgical System}},
  volume = {3},
  year = {2018},
  research_field={HW},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/20bfdebb50a806e0e7cdc44f710b996fd0420b85},
  abstract = {The current practice of intraoperative ultrasound requires an assistant because the surgeon's hands are occupied with surgical tools or console instruments. This process can be tedious and prone to error. Eye gaze is a promising control modality that can help address this issue. In previous work, a novel feature-based retro-fit eye gaze tracker has been designed for the <italic>da Vinci</italic> surgical system. In this letter, leveraging the <italic>da Vinci</italic> research kit, three interfaces incorporate eye gaze, and voice recognition into the <italic> da Vinci</italic> surgical system for ultrasound control in one common framework. This letter aims to improve autonomous use of ultrasound for surgeons. Since eye gaze tracking is sensitive to head movement, a novel calibration procedure is also proposed to accommodate head motion by decomposing pupil movement into eye rotation and head motion. This ensures that the eye gaze tracking can be reliably used as a control modality. A user study (<italic>N</italic> = 20) has shown that the designed eye gaze tracker has a mean binocular accuracy of 1.98<inline-formula> <tex-math notation="LaTeX">$^\{\circ \}$</tex-math></inline-formula> with mean <inline-formula><tex-math notation="LaTeX"> $-$</tex-math></inline-formula>0.92 mm horizontal and 16.83-mm vertical head movement. A preliminary user study ( <italic>N</italic> = 9) has shown that eye gaze tracking for ultrasound control has the potential to improve the way surgeons interact with their instrumentation and increase surgical autonomy.}
}

@inproceedings{Stilli2018,
  author = {Stilli, Agostino and Dimitrakakis, E and Tran, Maxine and Stoyanov, Danail},

  booktitle = {Joint Workshop on New Technologies for Computer/Robot Assisted Surgery (CRAS 2018)},
  publisher = {CRAS},
  title = {{Track-Guided Ultrasound Scanning for Tumour Margins Outlining in Robot-Assisted Partial Nephrectomy}},
  volume = {8},
  year = {2018},
  research_field={},
  data_type={}
}

@article{Penza2018,
  author = {Penza, Veronica and Du, Xiaofei and Stoyanov, Danail and Forgione, Antonello and Mattos, Leonardo S and {De Momi}, Elena},

  issn = {1361-8415},
  journal = {Medical image analysis},
  publisher = {Elsevier},
  title = {{Long term safety area tracking (LT-SAT) with online failure detection and recovery for robotic minimally invasive surgery}},
  volume = {45},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/f52a2f32eb3eb2bbbc0c27779fa56613591c460e},
  doi = {10.1016/j.media.2017.12.010},
}

@inproceedings{Ferguson2018c,
  author = {Ferguson, James M. and Cai, Leon Y. and Reed, Alexander and Siebold, Michael and De, Smita and Herrell, Stanley D. and Webster, Robert J.},

  booktitle = {Medical Imaging 2018: Image-Guided Procedures, Robotic Interventions, and Modeling},
  doi = {10.1117/12.2296464},
  editor = {Webster, Robert J. and Fei, Baowei},
  isbn = {9781510616417},
  publisher = {SPIE},
  title = {{Toward image-guided partial nephrectomy with the da Vinci robot: exploring surface acquisition methods for intraoperative re-registration}},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/f87ff13b29f907eb56148eb8ae0856a919ff32f2}
}

@article{Shahbazi2018a,
  author = {Shahbazi, Mahya and Atashzar, S. Farokh and Ward, Christopher and Talebi, Heidar Ali and Patel, Rajni V.},

  doi = {10.1109/TRO.2018.2861916},
  issn = {1552-3098},
  journal = {IEEE Transactions on Robotics},
  number = {6},
  title = {{Multimodal Sensorimotor Integration for Expert-in-the-Loop Telerobotic Surgical Training}},
  volume = {34},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/785a4cb6047de170668317df035219edb245803c},
  abstract = {This paper presents a novel multimodal training platform integrated with hand-over-hand (HOH) haptic guidance for dual-console surgical robotic systems such as the da Vinci Si system. The expert-in-the-loop (EIL) framework incorporates a fuzzy interface system in order to provide a trainee with adaptive authority over the procedure as well as hand-over-hand haptic guidance adjusted in real time based on the proficiency level of the trainee. The EIL expertise-oriented framework enables performance of a surgical procedure by an expert surgeon on a patient, while simultaneously providing a trainee at any stage of the motor-skills development with multimodal training without jeopardizing patient safety. Closed-loop stability of the system is investigated using the circle criterion and it is shown that the proposed architecture is unconditionally stable. Experimental evaluations are presented in support of the proposed platform through the implementation of a dual-console surgical setup consisting of the classic da Vinci surgical system (Intuitive Surgical, Inc., Sunnyvale, CA, USA) and the dV-Trainer master console (Mimic Technology, Inc., Seattle, WA, USA). To the best of our knowledge, the implemented setup is the first research platform for dual-console studies involving the classic da Vinci surgical system.}
}

@article{Nagy2018d,
  author = {Nagy, D{\'{e}}nes {\'{A}}kos and Nagy, Tam{\'{a}}s D{\'{a}}niel and Elek, Ren{\'{a}}ta and Rudas, Imre J. and Haidegger, Tam{\'{a}}s},

  doi = {10.1142/S2424905X18410052},
  issn = {2424-905X},
  journal = {Journal of Medical Robotics Research},
  title = {{Ontology-Based Surgical Subtask Automation, Automating Blunt Dissection}},
  volume = {03},
  year = {2018},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/5ff23d8a9dfb88085b678a33a83f1c586f39672b},
  abstract = {Automation of surgical processes (SPs) is an utterly complex, yet highly demanded feature by medical experts. Currently, surgical tools with advanced sensory and diagnostic capabilities are only available. A major criticism towards the newly developed instruments that they are not fitting into the existing medical workflow often creating more annoyance than benefit for the surgeon. The first step in achieving streamlined integration of computer technologies is gaining a better understanding of the SP. Surgical ontologies provide a generic platform for describing elements of the surgical procedures. Surgical Process Models (SPMs) built on top of these ontologies have the potential to accurately represent the surgical workflow. SPMs provide the opportunity to use ontological terms as the basis of automation, allowing the developed algorithm to easily integrate into the surgical workflow, and to apply the automated SPMs wherever the linked ontological term appears in the workflow. In this work, as an example to this concept, the subtask level ontological term “blunt dissection” was targeted for automation. We implemented a computer vision-driven approach to demonstrate that automation on this task level is feasible. The algorithm was tested on an experimental silicone phantom as well as in several ex vivo environments. The implementation used the da Vinci surgical robot, controlled via the Da Vinci Research Kit (DVRK), relying on a shared code-base among the DVRK institutions. It is believed that developing and linking further building blocks of lower level surgical subtasks could lead to the introduction of automated soft tissue surgery. In the future, the building blocks could be individually unit tested, leading to incremental automation of the domain. This framework could potentially standardize surgical performance, eventually improving patient outcomes.},
}

@article{Selvaggio2018,
  author = {Selvaggio, Mario and Fontanelli, Giuseppe Andrea and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {4},
  publisher = {IEEE},
  title = {{Passive virtual fixtures adaptation in minimally invasive robotic surgery}},
  volume = {3},
  year = {2018},
  research_field={HW},
  data_type={RI and KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/0f13e38c8367ba8a94ee4e7caba648b6adeee08c},
  doi = {10.1109/LRA.2018.2849876},
  abstract = {During robot-aided surgical interventions, the surgeon can be benefitted from the application of virtual fixtures (VFs). Though very effective, this technique is very often not practicable in unstructured surgical environments. In order to comply with the environmental deformation, both the VF geometry and the constraint enforcement parameters need to be online defined/adapted. This letter proposes a strategy for an effective use of VF assistance in minimally invasive robotic surgical tasks. An online VF generation technique based on the interaction force measurements is presented. Pose and geometry adaptations of the VF are considered. Passivity of the overall system is guaranteed by using energy tanks passivity-based control. The proposed method is validated through experiments on the da Vinci Research Kit.}
}

@article{Abdelaal2018,
  author = {Abdelaal, Alaa Eldin and Sakr, Maram and Avinash, Apeksha and Mohammed, Shahed K and Bajwa, Armaan Kaur and Sahni, Mohakta and Hor, Soheil and Fels, Sidney and Salcudean, Septimiu E},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Play me back: a unified training platform for robotic and laparoscopic surgery}},
  volume = {4},
  year = {2018},
  research_field={TR},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/153e1ec0ff1907700f5a612271ab39b40b0227e6},
  doi = {10.1109/LRA.2018.2890209},
  abstract = {In this letter, we propose a training approach combining hand-over-hand and trial and error training approaches and we evaluate its effectiveness for both robotic and standard laparoscopic surgical training. The proposed approach makes use of the data of an expert collected while using the da Vinci Surgical System. We present our data collection system and how we use it in the proposed training approach. We conduct two user studies (N = 21 for each) to evaluate the effectiveness of this approach. Our results show that subjects trained using this combined approach can better balance the speed and accuracy of their task execution compared with others trained using only one of either hand-over-hand or trial and error training approaches. Moreover, this combined approach leads to the best performance when it comes to the transferability of the acquired skills when testing on another task. We show that the results of the two studies are consistent with an established model in the literature for motor skill learning. Moreover, our results show for the first time the feasibility of using a surgical robot and data collected from it as a training platform for conventional laparoscopic surgery without robotic assistance.}
}

@article{Chalasani2018a,
  author = {Chalasani, Preetham and Wang, Long and Yasin, Rashid and Simaan, Nabil and Taylor, Russell H.},

  doi = {10.1109/LRA.2018.2801481},
  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  title = {{Preliminary Evaluation of an Online Estimation Method for Organ Geometry and Tissue Stiffness}},
  volume = {3},
  year = {2018},
  research_field={AU},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/1e14db8cbe1bce66ea2104e0c387f599db47a2de},
  abstract = {During open surgeries, surgeons use tactile palpation to form an understanding of the anatomy, including any underlying anatomical structures such as arteries or tumors. This letter explores methods for restoring this capability in the context of minimally invasive robot-assisted surgery. Previous works have demonstrated the ability of robots to use discrete palpation to characterize organ shape and, when followed with offline data processing, to produce a stiffness map of the organ. In our earlier work, we presented an offline estimation technique, independent of palpation strategy, to estimate organ shape and stiffness using Gaussian processes. This study extends our prior work by demonstrating a fast online technique for estimation of organ shape and stiffness. Our goal is to provide near video-frame-rate updates of the organ geometry and tissue stiffness during force controlled exploration. Two different palpation modes are experimentally explored: autonomous palpation and constrained semiautonomous teleoperation. We report the experimental evaluation of our approach for stiffness estimation using autonomous palpation. And we demonstrate the feasibility of using our method during interactive teleoperation in a simulated surgical scenario. We believe that future use of the online stiffness and geometry information based on our proposed method can help surgeons in improving their understanding of the surgical scene and its correlation to preoperative imaging, thereby increasing safety and improving surgical outcomes.}
}

@inproceedings{Vantadori2018,
  author = {Vantadori, Luca and Mariani, Andrea and Chupin, Thibaud and {De Momi}, Elena and Ferrigno, Giancarlo},

  booktitle = {Proceedings of Computer/Robot Assisted Surgery CRAS},
  title = {{Design and evaluation of an intraoperative safety constraints definition and enforcement system for robot-assisted minimally invasive surgery}},
  year = {2018},
  research_field={HW},
  data_type={DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2}
}

@inproceedings{Zevallos2018a,
  author = {Zevallos, Nicolas and Rangaprasad, Arun Srivatsan and Salman, Hadi and Li, Lu and Qian, Jianing and Saxena, Saumya and Xu, Mengyun and Patath, Kartik and Choset, Howie},

  booktitle = {Robotics: Science and Systems},
  title = {{A Real-time Augmented Reality Surgical System for Overlaying Stiffness Information.}},
  year = {2018},
  research_field={HW},
  data_type={RI and KD}
}

@article{Vagvolgyi2018,
  author = {Vagvolgyi, Balazs P and Pryor, Will and Reedy, Ryan and Niu, Wenlong and Deguet, Anton and Whitcomb, Louis L and Leonard, Simon and Kazanzides, Peter},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {4},
  publisher = {IEEE},
  title = {{Scene modeling and augmented virtuality interface for telerobotic satellite servicing}},
  volume = {3},
  year = {2018},
  research_field={},
  data_type={}
}

@article{Ma2018,
  author = {Ma, Xin and Chiu, Philip Wai-Yan and Li, Zheng},

  issn = {1530-437X},
  journal = {IEEE Sensors Journal},
  number = {19},
  publisher = {IEEE},
  title = {{Shape sensing of flexible manipulators with visual occlusion based on Bezier curve}},
  volume = {18},
  year = {2018},
  research_field={HW},
  data_type={RI and KD and ED}
}

@article{Enayati2018a,
  author = {Enayati, Nima and Ferrigno, Giancarlo and {De Momi}, Elena},

  issn = {0929-5593},
  journal = {Autonomous Robots},
  number = {5},
  publisher = {Springer},
  title = {{Skill-based human–robot cooperation in tele-operated path tracking}},
  volume = {42},
  year = {2018},
  research_field={TR},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/e903ae33aae5a166b6d6da5daacc7e2c812596b8},
  doi = {10.1007/s10514-017-9675-4},
}

@article{Diodato2018a,
  author = {Diodato, Alessandro and Brancadoro, Margherita and {De Rossi}, Giacomo and Abidi, Haider and Dall'Alba, Diego and Muradore, Riccardo and Ciuti, Gastone and Fiorini, Paolo and Menciassi, Arianna and Cianchetti, Matteo},

  doi = {10.1177/1553350617745953},
  issn = {1553-3506},
  journal = {Surgical Innovation},
  number = {1},
  title = {{Soft Robotic Manipulator for Improving Dexterity in Minimally Invasive Surgery}},
  volume = {25},
  year = {2018},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/1d5f9d05cff4c1f823ab62746c16e76c0ea22171},
}

@article{Moradi2018,
  author = {Moradi, Hamid and Tang, Shuo and Salcudean, Septimiu E},

  issn = {0278-0062},
  journal = {IEEE transactions on medical imaging},
  number = {1},
  publisher = {IEEE},
  title = {{Toward intra-operative prostate photoacoustic imaging: configuration evaluation and implementation using the da Vinci research kit}},
  volume = {38},
  year = {2018},
  research_field={HW},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/c8a3bed5fffbc89a47ce7b0a11790fb971ce6805},
  doi = {10.1109/TMI.2018.2855166},
  abstract = {We compare different possible scanning geometries for prostate photoacoustic tomography (PAT) while considering a realistic reconstruction scenario in which the limited view of the prostate and the directivity effect of the transducer are considered. Simulations and experiments confirm that an intra-operative configuration in which the photoacoustic signal is received by a pickup transducer from the anterior surface of the prostate provides the best approach. We propose a PAT acquisition system that includes a da Vinci system controlled by the da Vinci Research Kit, an illumination laser, and an ultrasound machine with parallel data acquisition. The robot maneuvers the pickup transducer to form a cylindrical detection surface around the prostate. The robot is programmed to acquire trajectories in which the transducer face is parallel to and oriented toward a rotational tomography axis, while the laser is fired and PAT data are collected at regular intervals. We present our initial images acquired with this novel system.}
}

@article{Alambeigi2018f,
  author = {Alambeigi, Farshid and Wang, Zerui and Liu, Yun-hui and Taylor, Russell H. and Armand, Mehran},

  doi = {10.1007/s10439-018-2074-y},
  issn = {0090-6964},
  journal = {Annals of Biomedical Engineering},
  number = {10},
  title = {{Toward Semi-autonomous Cryoablation of Kidney Tumors via Model-Independent Deformable Tissue Manipulation Technique}},
  volume = {46},
  year = {2018},
  research_field={AU},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/6fa03b55fb2ac5520e29de81dbf2fbd3d4f0f879},
}

@article{Haouchine2018,
  author = {Haouchine, Nazim and Kuang, Winnie and Cotin, Stephane and Yip, Michael},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  publisher = {IEEE},
  title = {{Vision-based force feedback estimation for robot-assisted surgery using instrument-constrained biomechanical three-dimensional maps}},
  volume = {3},
  year = {2018},
  research_field={HW},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/e6508575b1d540e07aba7bb17c31f93c69034728},
  doi = {10.1109/LRA.2018.2810948},
  abstract = {We present a method for estimating visual and haptic force feedback on robotic surgical systems that currently do not include significant force feedback for the operator. Our approach permits to compute contact forces between instruments and tissues without additional sensors, relying only on endoscopic images acquired by a stereoscopic camera. Using an underlying biomechanical model built on-the-fly from the organ shape and by considering the surgical tool as boundary conditions acting on the surface of the model, contact force can be estimated at the tip of the tool. At the same time, these constraints generate stresses that permit to compose a new endoscopic image as visual feedback for the surgeon. The results are demonstrated on in vivo sequences of a human liver during robotic surgery, whereas quantitative validation is performed on a DejaVu and ex vivo experimentation with ground truth to show the advantage of our approach.},
}

@inproceedings{Penza2017a,
  author = {Penza, V and {De Momi}, E and Enayati, N and Chupin, T and Ortiz, J and Mattos, L. S},

  booktitle = {10th Hamlyn Symposium on Medical Robotics 2017},
  doi = {10.31256/HSMR2017.26},
  isbn = {9780956377685},
  publisher = {The Hamlyn Centre, Faculty of Engineering, Imperial College London},
  title = {{Safety Enhancement Framework for Robotic Minimally Invasive Surgery}},
  year = {2017},
  research_field={HW},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/136af21d8e212cda33f66488939aed946eb644ec},
}

@inproceedings{Liang2017b,
  author = {Liang, Jacky and Mahler, Jeffrey and Laskey, Michael and Li, Pusong and Goldberg, Ken},

  booktitle = {2017 13th IEEE Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/COASE.2017.8256067},
  isbn = {978-1-5090-6781-7},
  publisher = {IEEE},
  title = {{Using dVRK teleoperation to facilitate deep learning of automation tasks for an industrial robot}},
  year = {2017},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a7ab653e7bc6af1fa5442c5f56bcd2a4562e8e96}
}

@inproceedings{Chen2017,
  author = {Chen, Zihan and Deguet, Anton and Taylor, Russell H and Kazanzides, Peter},

  booktitle = {2017 First IEEE International Conference on Robotic Computing (IRC)},
  isbn = {1509067248},
  publisher = {IEEE},
  title = {{Software architecture of the da Vinci Research Kit}},
  year = {2017},
  research_field={},
  data_type={}
}

@inproceedings{Kim2017b,
  author = {Kim, Sungmin and Tan, Youri and Deguet, Anton and Kazanzides, Peter},

  booktitle = {2017 First IEEE International Conference on Robotic Computing (IRC)},
  isbn = {1509067248},
  publisher = {IEEE},
  title = {{Real-time image-guided telerobotic system integrating 3D Slicer and the da Vinci Research Kit}},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and DD and SD and ED}
}

@inproceedings{Elek2017,
  author = {Elek, Ren{\'{a}}ta and Nagy, Tam{\'{a}}s D and Nagy, D{\'{e}}nes {\'{A}} and Garamv{\"{o}}lgyi, Tivadar and Tak{\'{a}}cs, Bence and Galambos, P{\'{e}}ter and Tar, J{\'{o}}zsef K and Rudas, Imre J and Haidegger, Tam{\'{a}}s},

  booktitle = {2017 IEEE 21st International Conference on Intelligent Engineering Systems (INES)},
  isbn = {1479976784},
  publisher = {IEEE},
  title = {{Towards surgical subtask automation—Blunt dissection}},
  year = {2017},
  research_field={},
  data_type={}
}

@inproceedings{Haidegger2017,
  author = {Haidegger, Tam{\'{a}}s},

  booktitle = {2017 IEEE 30th Neumann Colloquium (NC)},
  isbn = {1538646366},
  publisher = {IEEE},
  title = {{Surgical robots of the next decade: New trends and paradigms in the 21th century}},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/36b7dbfb2abe2fb5e9f0d0f35fdf3d0f38b1cede},
  doi = {10.7275/R5WM1BKZ}
}

@inproceedings{Zhang2017i,
  author = {Zhang, Lin and Ye, Menglong and Giataganas, Petros and Hughes, Michael and Yang, Guang-Zhong},

  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2017.7989412},
  isbn = {978-1-5090-4633-1},
  publisher = {IEEE},
  title = {{Autonomous scanning for endomicroscopic mosaicing and 3D fusion}},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/c563baa3bb8452e487229b1b72591ec4dbd1416a},
  abstract = {Robot-assisted minimally invasive surgery can benefit from the automation of common, repetitive or well-defined but ergonomically difficult tasks. One such task is the scanning of a pick-up endomicroscopy probe over a complex, undulating tissue surface to enhance the effective field-of-view through video mosaicing. In this paper, the da Vinci® surgical robot, through the dVRK framework, is used for autonomous scanning and 2D mosaicing over a user-defined region of interest. To achieve the level of precision required for high quality mosaic generation, which relies on sufficient overlap between consecutive image frames, visual servoing is performed using a combination of a tracking marker attached to the probe and the endomicroscopy images themselves. The resulting sub-millimetre accuracy of the probe motion allows for the generation of large mosaics with minimal intervention from the surgeon. Images are streamed from the endomicroscope and overlaid live onto the surgeons view, while 2D mosaics are generated in real-time, and fused into a 3D stereo reconstruction of the surgical scene, thus providing intuitive visualisation and fusion of the multi-scale images. The system therefore offers significant potential to enhance surgical procedures, by providing the operator with cellular-scale information over a larger area than could typically be achieved by manual scanning.},
}

@inproceedings{Kim2017a,
  author = {Kim, Sungmin and Gandhi, Neeraj and Bell, Muyinatu A Lediju and Kazanzides, Peter},

  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {150904633X},
  publisher = {IEEE},
  title = {{Improving the safety of telerobotic drilling of the skull base via photoacoustic sensing of the carotid arteries}},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/e0aba478ab135e3b72b38dc9dd3e4608b213ba53},
  doi = {10.1109/icra33291.2017}
}

@inproceedings{Thananjeyan2017b,
  author = {Thananjeyan, Brijen and Garg, Animesh and Krishnan, Sanjay and Chen, Carolyn and Miller, Lauren and Goldberg, Ken},

  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2017.7989275},
  isbn = {978-1-5090-4633-1},
  publisher = {IEEE},
  title = {{Multilateral surgical pattern cutting in 2D orthotropic gauze with deep reinforcement learning policies for tensioning}},
  year = {2017},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/07be721c0268da8baf08db09da0dea8213b7bdcd}
}

@inproceedings{Coad2017a,
  author = {Coad, Margaret M. and Okamura, Allison M. and Wren, Sherry and Mintz, Yoav and Lendvay, Thomas S. and Jarc, Anthony M. and Nisky, Ilana},

  booktitle = {2017 IEEE World Haptics Conference (WHC)},
  doi = {10.1109/WHC.2017.7989900},
  isbn = {978-1-5090-1425-5},
  publisher = {IEEE},
  title = {{Training in divergent and convergent force fields during 6-DOF teleoperation with a robot-assisted surgical system}},
  year = {2017},
  research_field={TR},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/a0f8ab29fc09e480d6b187e93239347de172d0f6}
}

@inproceedings{Fontanelli2017a,
  author = {Fontanelli, Giuseppe Andrea and Buonocore, Luca Rosario and Ficuciello, Fanny and Villani, Luigi and Siciliano, Bruno},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1538626829},
  publisher = {IEEE},
  title = {{A novel force sensing integrated into the trocar for minimally invasive robotic surgery}},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  doi = {10.1109/iros37595.2017}
}

@inproceedings{Garcia-Peraza-Herrera2017a,
  author = {Garc{\'{i}}a-Peraza-Herrera, Luis C and Li, Wenqi and Fidon, Lucas and Gruijthuijsen, Caspar and Devreker, Alain and Attilakos, George and Deprest, Jan and {Vander Poorten}, Emmanuel and Stoyanov, Danail and Vercauteren, Tom},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1538626829},
  publisher = {IEEE},
  title = {{Toolnet: holistically-nested real-time segmentation of robotic surgical tools}},
  year = {2017},
  research_field={},
  data_type={}
}

@inproceedings{Vagvolgyi2017,
  author = {Vagvolgyi, Balazs and Niu, Wenlong and Chen, Zihan and Wilkening, Paul and Kazanzides, Peter},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1538626829},
  publisher = {IEEE},
  title = {{Augmented virtuality for model-based teleoperation}},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  doi = {10.1109/iros37595.2017}
}

@inproceedings{Fontanelli2017b,
  author = {Fontanelli, G. A. and Ficuciello, F. and Villani, L. and Siciliano, B.},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2017.8205948},
  isbn = {978-1-5386-2682-5},
  publisher = {IEEE},
  title = {{Modelling and identification of the da Vinci Research Kit robotic arms}},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/481add0c55ad9dcf348398f7ead578775b1ef61d}
}

@inproceedings{Chow2017b,
  author = {Chow, Der-Lin and Xu, Peng and Tuna, Eser and Huang, Siqi and Cavusoglu, M. Cenk and Newman, Wyatt},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2017.8206389},
  isbn = {978-1-5386-2682-5},
  publisher = {IEEE},
  title = {{Supervisory control of a DaVinci surgical robot}},
  year = {2017},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/ee5927a650f5ecb9b86994c3efca599be267bc24}
}

@inproceedings{Anooshahpour2017,
  author = {Anooshahpour, Farshad and Yadmellat, Peyman and Polushin, Ilia G and Patel, Rajni V},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1538626829},
  publisher = {IEEE},
  title = {{A motion transmission model for multi-DOF tendon-driven mechanisms with hysteresis and coupling: Application to a da Vinci{\textregistered} instrument}},
  year = {2017},
  research_field={HW},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  doi = {10.1109/iros37595.2017}
}

@inproceedings{Li2017,
  author = {Li, Lu and Yu, Bocheng and Yang, Chen and Vagdargi, Prasad and Srivatsan, Rangaprasad Arun and Choset, Howie},

  booktitle = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1538626829},
  publisher = {IEEE},
  title = {{Development of an inexpensive tri-axial force sensor for minimally invasive surgery}},
  year = {2017},
  research_field={HW},
  data_type={ED},
  semanticscholar = {https://www.semanticscholar.org/paper/58a1f8caa27bb7d5ec312491f841f87e63a2b7e1},
  doi = {10.1109/iros37595.2017}
}

@article{Naidu2017,
  author = {Naidu, Anish S and Naish, Michael D and Patel, Rajni V},

  issn = {1070-9932},
  journal = {IEEE Robotics & Automation Magazine},
  number = {2},
  publisher = {IEEE},
  title = {{A breakthrough in tumor localization: Combining tactile sensing and ultrasound to improve tumor localization in robotics-assisted minimally invasive surgery}},
  volume = {24},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/a92888855c3c0d7693026f4a73c81f54b4cbe6f0},
  doi = {10.1109/MRA.2017.2680544}
}

@article{Zhang2017e,
  author = {Zhang, Zhiqiang and Zhang, Lin and Yang, Guang-Zhong},

  issn = {1861-6410},
  journal = {International journal of computer assisted radiology and surgery},
  number = {10},
  publisher = {Springer},
  title = {{A computationally efficient method for hand–eye calibration}},
  volume = {12},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/50fc9397d7b33fae7d0815674a78c913364b6166},
  doi = {10.1007/s11548-017-1646-x},
  abstract = {PurposeSurgical robots with cooperative control and semiautonomous features have shown increasing clinical potential, particularly for repetitive tasks under imaging and vision guidance. Effective performance of an autonomous task requires accurate hand–eye calibration so that the transformation between the robot coordinate frame and the camera coordinates is well defined. In practice, due to changes in surgical instruments, online hand–eye calibration must be performed regularly. In order to ensure seamless execution of the surgical procedure without affecting the normal surgical workflow, it is important to derive fast and efficient hand–eye calibration methods.MethodsWe present a computationally efficient iterative method for hand–eye calibration. In this method, dual quaternion is introduced to represent the rigid transformation, and a two-step iterative method is proposed to recover the real and dual parts of the dual quaternion simultaneously, and thus the estimation of rotation and translation of the transformation.ResultsThe proposed method was applied to determine the rigid transformation between the stereo laparoscope and the robot manipulator. Promising experimental and simulation results have shown significant convergence speed improvement to 3 iterations from larger than 30 with regard to standard optimization method, which illustrates the effectiveness and efficiency of the proposed method.},
}

@article{Sang2017c,
  author = {Sang, Hongqiang and Monfaredi, Reza and Wilson, Emmanuel and Fooladi, Hadi and Preciado, Diego and Cleary, Kevin},

  doi = {10.1115/1.4036490},
  issn = {1932-6181},
  journal = {Journal of Medical Devices},
  number = {3},
  title = {{A New Surgical Drill Instrument With Force Sensing and Force Feedback for Robotically Assisted Otologic Surgery}},
  volume = {11},
  year = {2017},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/c4ac18cff9dc8669d0ba2760a0a5bd32c0c3b54f}
}

@inproceedings{Eslamian2017,
  author = {Eslamian, Shahab and Reisner, Luke A. and King, Brady W. and Pandya, Abhilash K.},

  booktitle = {arXiv preprint},
  title = {{An Autonomous Camera System using the da Vinci Research Kit}},
  year = {2017},
  research_field={AU},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/cfd637f236a1a8860360fdb1ba2148833cbc4793},
  doi = {10.1145/3613904.3642858},
  abstract = {Design space exploration (DSE) for Text-to-Image (TTI) models entails navigating a vast, opaque space of possible image outputs, through a commensurately vast input space of hyperparameters and prompt text. Perceptually small movements in prompt-space can surface unexpectedly disparate images. How can interfaces support end-users in reliably steering prompt-space explorations towards interesting results? Our design probe, DreamSheets, supports user-composed exploration strategies with LLM-assisted prompt construction and large-scale simultaneous display of generated results, hosted in a spreadsheet interface. Two studies, a preliminary lab study and an extended two-week study where five expert artists developed custom TTI sheet-systems, reveal various strategies for targeted TTI design space exploration—such as using templated text generation to define and layer semantic “axes” for exploration. We identified patterns in exploratory structures across our participants’ sheet-systems: configurable exploration “units” that we distill into a UI mockup, and generalizable UI components to guide future interfaces.},
}

@inproceedings{Enayati2017,
  author = {Enayati, N and Mariani, A and Pellegrini, E and Chupin, T and Ferrigno, G and {De Momi}, E},

  booktitle = {CRAS: Joint Workshop on New Technologies for Computer/Robot Assisted Surgery},
  title = {{A Framework for Assisted Tele-operation with Augmented Reality}},
  year = {2017},
  research_field={TR},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/162acade8931eed268f89608ec8ea37391f76df2}
}

@article{Kim2017,
  author = {Kim, Myungjoon and Lee, Chiwon and Hong, Nhayoung and Kim, Yoon Jae and Kim, Sungwan},

  issn = {1475-925X},
  journal = {Biomedical engineering online},
  number = {1},
  publisher = {Springer},
  title = {{Development of stereo endoscope system with its innovative master interface for continuous surgical operation}},
  volume = {16},
  year = {2017},
  research_field={},
  data_type={}
}

@article{Fard2017,
  author = {Fard, Mahtab J and Pandya, Abhilash K and Chinnam, Ratna B and Klein, Michael D and Ellis, R Darin},

  issn = {1478-5951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {3},
  publisher = {Wiley Online Library},
  title = {{Distance‐based time series classification approach for task recognition with application in surgical robot autonomy}},
  volume = {13},
  year = {2017},
  research_field={TR},
  data_type={RI and KD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/ac37b0c8349589ae2a7df78878717f4c685bca14},
  doi = {10.1002/rcs.1766},
}

@article{Penza2017,
  author = {Penza, Veronica and {De Momi}, Elena and Enayati, Nima and Chupin, Thibaud and Ortiz, Jes{\'{u}}s and Mattos, Leonardo S},

  issn = {2296-9144},
  journal = {Frontiers in Robotics and AI},
  publisher = {Frontiers},
  title = {{enVisors: enhanced Vision system for robotic surgery. a User-Defined safety Volume Tracking to Minimize the risk of intraoperative Bleeding}},
  volume = {4},
  year = {2017},
  research_field={},
  data_type={}
}

@article{Sang2017b,
  author = {Sang, Hongqiang and Yun, Jintian and Monfaredi, Reza and Wilson, Emmanuel and Fooladi, Hadi and Cleary, Kevin},

  doi = {10.1002/rcs.1824},
  issn = {14785951},
  journal = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  number = {2},
  title = {{External force estimation and implementation in robotically assisted minimally invasive surgery}},
  volume = {13},
  year = {2017},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/db28445114e3c6c74df9f6fcc1e31cb29da519d1}
}

@article{Wang2017,
  author = {Wang, Long and Chen, Zihan and Chalasani, Preetham and Yasin, Rashid M and Kazanzides, Peter and Taylor, Russell H and Simaan, Nabil},

  issn = {1942-4302},
  journal = {Journal of Mechanisms and Robotics},
  number = {2},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  title = {{Force-controlled exploration for updating virtual fixture geometry in model-mediated telemanipulation}},
  volume = {9},
  year = {2017},
  research_field={HW},
  data_type={RI and KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/6fc55e847886f0581497be762c1e128df1f8e8c2},
  doi = {10.1115/1.4035684}
}

@article{Zhang2017h,
  author = {Zhang, Lin and Ye, Menglong and Giataganas, Petros and Hughes, Michael and Bradu, Adrian and Podoleanu, Adrian and Yang, Guang-Zhong},

  doi = {10.1109/MRA.2017.2680543},
  issn = {1070-9932},
  journal = {IEEE Robotics & Automation Magazine},
  number = {2},
  title = {{From Macro to Micro: Autonomous Multiscale Image Fusion for Robotic Surgery}},
  volume = {24},
  year = {2017},
  research_field={IM},
  data_type={KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/db7c7614427f78953b1a19da0d68c298ad1de48a},
  abstract = {Minimally invasive surgery (MIS), performed through a small number of keyhole incisions, has become the standard of care for many general surgical procedures, reducing trauma, blood loss, and other complications and offering patients the prospect of a faster recovery with less postoperative pain. These improvements for the patient, however, require higher dexterity and complex instrument control by the surgeons. Keyhole incisions constrain the motion of surgical instruments, while the loss of stereovision when using a laparoscope or endoscope means that depth perception is much poorer than in traditional open surgery. The desire to tackle these issues has been the main driver behind the development of robotic MIS systems with stereovision. In particular, the da Vinci robot (Intuitive Surgical, Inc., Sunnyvale, California) is a successful surgical platform, used widely in the treatment of gynecological and urological cancers. While human guidance is essential for MIS, recent studies [1] have suggested that automation of some surgical subtasks, particularly those that are tedious and repetitive or require high precision, can be beneficial in improving accuracy and reducing the cognitive load of the surgeon. For example, several studies have investigated automation of surgical suturing subtasks, including using a suturing tool under fluorescence guidance [2], and other studies have explored areas such as autonomous tissue dissection [3].},
}

@article{Wang2017a,
  author = {Wang, Zerui and Lee, Sing Chun and Zhong, Fangxun and Navarro-Alarcon, David and Liu, Yun-hui and Deguet, Anton and Kazanzides, Peter and Taylor, Russell H},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {3},
  publisher = {IEEE},
  title = {{Image-based trajectory tracking control of 4-DOF laparoscopic instruments using a rotation distinguishing marker}},
  volume = {2},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and DD and SD}
}

@article{Sharon2017,
  author = {Sharon, Yarden and Lendvay, Thomas Sean and Nisky, Ilana},

  journal = {arXiv preprint},
  title = {{Instrument orientation-based metrics for surgical skill evaluation in robot-assisted and open needle driving}},
  year = {2017},
  research_field={TR},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/4a670217817c7c38cbce724d5b5d381e8391d123}
}

@inproceedings{Zhang2017d,
  author = {Zhang, Lin and Ye, Menglong and Giannarou, Stamatia and Pratt, Philip and {Yang Guang-Zhong"},
  editor="Descoteaux Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D Louis and Duchesne, Simon},

  booktitle = {Medical Image Computing and Computer Assisted Intervention – MICCAI 2017},
  title = {{Motion-Compensated Autonomous Scanning for Tumour Localisation Using Intraoperative Ultrasound}},
  year = {2017},
  research_field={HW},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/86a7eb7b016971465ba705cec826e9c254e3ca8c},
  doi = {10.1007/978-3-319-66185-8},
}

@article{Francis2017,
  author = {Francis, Peter and Eastwood, Kyle W and Bodani, Vivek and Price, Karl and Upadhyaya, Kunj and Podolsky, Dale and Azimian, Hamidreza and Looi, Thomas and Drake, James},

  issn = {1070-9932},
  journal = {IEEE Robotics & Automation Magazine},
  number = {2},
  publisher = {IEEE},
  title = {{Miniaturized instruments for the da Vinci research kit: Design and implementation of custom continuum tools}},
  volume = {24},
  year = {2017},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/10560a043afe159d2f45dc4fec29347640130c46},
  doi = {10.1109/MRA.2017.2680547}
}

@article{Gandhi2017,
  author = {Gandhi, Neeraj and Allard, Margaret and Kim, Sungmin and Kazanzides, Peter and Bell, Muyinatu A Lediju},

  issn = {1083-3668},
  journal = {Journal of Biomedical Optics},
  number = {12},
  publisher = {International Society for Optics and Photonics},
  title = {{Photoacoustic-based approach to surgical guidance performed with and without a da Vinci robot}},
  volume = {22},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/8c5036b90119ca9a5bc4d7a55865a3b4845f85cd},
  doi = {10.1117/1.JBO.22.12.121606},
  abstract = {Abstract. Death and paralysis are significant risks of modern surgeries, caused by injury to blood vessels and nerves hidden by bone and other tissue. We propose an approach to surgical guidance that relies on photoacoustic (PA) imaging to determine the separation between these critical anatomical features and to assess the extent of safety zones during surgical procedures. Images were acquired as an optical fiber was swept across vessel-mimicking targets, in the absence and presence of teleoperation with a research da Vinci Surgical System. Vessel separation distances were measured directly from PA images. Vessel positions were additionally recorded based on the fiber position (calculated from the da Vinci robot kinematics) that corresponded to an observed PA signal, and these recordings were used to indirectly measure vessel separation distances. Amplitude- and coherence-based beamforming were used to estimate vessel separations, resulting in 0.52- to 0.56-mm mean absolute errors, 0.66- to 0.71-mm root-mean-square errors, and 65% to 68% more accuracy compared to fiber position measurements obtained through the da Vinci robot kinematics. Similar accuracy was achieved in the presence of up to 4.5-mm-thick ex vivo tissue. Results indicate that PA image-based measurements of the separation among anatomical landmarks could be a viable method for real-time path planning in multiple interventional PA applications.},
}

@article{Zhang2017g,
  author = {Zhang, Lin and Ye, Menglong and Chan, Po-Ling and Yang, Guang-Zhong},

  issn = {1861-6410},
  journal = {International journal of computer assisted radiology and surgery},
  number = {6},
  publisher = {Springer},
  title = {{Real-time surgical tool tracking and pose estimation using a hybrid cylindrical marker}},
  volume = {12},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and SD and ED}
}

@article{Jackson2017,
  author = {Jackson, Russell C and Yuan, Rick and Chow, Der-Lin and Newman, Wyatt S and {\c{C}}avuşoğlu, M Cenk},

  issn = {1545-5955},
  journal = {IEEE Transactions on Automation science and Engineering},
  number = {3},
  publisher = {IEEE},
  title = {{Real-time visual tracking of dynamic surgical suture threads}},
  volume = {15},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e19ae705aec24e9f23086c81ceff44e88352050b},
  doi = {10.1109/TASE.2017.2726689},
  abstract = {In order to realize many of the potential benefits associated with robotically assisted minimally invasive surgery, the robot must be more than a remote controlled device. Currently, using a surgical robot can be challenging, fatiguing, and time-consuming. Teaching the robot to actively assist surgical tasks, such as suturing, has the potential to vastly improve both the patient’s outlook and the surgeon’s efficiency. One obstacle to completing surgical sutures autonomously is the difficulty in tracking surgical suture threads. This paper presents novel stereo image processing algorithms for the detection, initialization, and tracking of a surgical suture thread. A nonuniform rational B-spline (NURBS) curve is used to model a thin, deformable, and dynamic length thread. The NURBS model is initialized and grown from a single selected point located on the thread. The NURBS curve is optimized by minimizing the image matching energy between the projected stereo NURBS image and the segmented thread image. The algorithms are evaluated using suture threads, a calibrated test pattern, and a simulated thread image. In addition, the accuracies of the algorithms presented are validated as they track a suture thread undergoing translation, deformation, and apparent length changes. All of the tracking is in real time. Note to Practitioners—The problem of tracking a surgical suture thread was addressed in this paper. Since the suture thread is highly deformable, any tracking algorithm must be robust to intersections, occlusions, knot tying, and length changes. The detection algorithm introduced in this paper is capable of distinguishing different threads when they intersect. The tracking algorithm presented here demonstrates that it is possible, using polynomial curves, to track a suture thread as it deforms, becomes occluded, changes length, and even ties a knot in real time. The detection algorithm can enhance directional thin features, while the polynomial curve modeling can track any string-like structure. Further integration of the polynomial curve with a feed-forward thread model could improve the stability and robustness of the thread tracking.},
}

@article{Ye2017a,
  author = {Ye, Menglong and Johns, Edward and Handa, Ankur and Zhang, Lin and Pratt, Philip and Yang, Guang-Zhong},

  journal = {arXiv preprint arXiv:1705.08260},
  title = {{Self-supervised siamese learning on stereo image pairs for depth estimation in robotic surgery}},
  year = {2017},
  url = {https://arxiv.org/abs/1705.08260},
  research_field={IM},
  data_type={RI},
  semanticscholar = {https://www.semanticscholar.org/paper/16832680c0b9ec642cfdcd94b644eedcfb4cf266},
  doi = {10.31256/HSMR2017.14},
  abstract = {Robotic surgery has become a powerful tool for performing minimally invasive procedures, providing advantages in dexterity, precision, and 3D vision, over traditional surgery. One popular robotic system is the da Vinci surgical platform, which allows preoperative information to be incorporated into live procedures using Augmented Reality (AR). Scene depth estimation is a prerequisite for AR, as accurate registration requires 3D correspondences between preoperative and intraoperative organ models. In the past decade, there has been much progress on depth estimation for surgical scenes, such as using monocular or binocular laparoscopes [1,2]. More recently, advances in deep learning have enabled depth estimation via Convolutional Neural Networks (CNNs) [3], but training requires a large image dataset with ground truth depths. Inspired by [4], we propose a deep learning framework for surgical scene depth estimation using self-supervision for scalable data acquisition. Our framework consists of an autoencoder for depth prediction, and a differentiable spatial transformer for training the autoencoder on stereo image pairs without ground truth depths. Validation was conducted on stereo videos collected in robotic partial nephrectomy.},
}

@inproceedings{Fiorini2017,
  author = {Fiorini, P and Dall'Alba, D and {De Rossi}, G and Naftalovich, D and Burdick, J W},

  booktitle = {The Hamlyn Symposium on Medical Robotics},
  pages = {45},
  title = {{Mining Robotic Surgery Data: Training and Modeling using the DVRK}},
  year = {2017},
  research_field={TR},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a3235b3387d2460a0c43e5950de6c16b1a53e111},
  doi = {10.31256/hsmr2018}
}

@article{Krishnan2017,
  author = {Krishnan, Sanjay and Garg, Animesh and Patil, Sachin and Lea, Colin and Hager, Gregory and Abbeel, Pieter and Goldberg, Ken},

  issn = {0278-3649},
  journal = {The International Journal of Robotics Research},
  number = {13-14},
  publisher = {SAGE Publications Sage UK: London, England},
  title = {{Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning}},
  volume = {36},
  year = {2017},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/694d284d7c63c7f29c48b540bc4ece9f63b79160},
  doi = {10.1177/0278364917743319},
}

@article{Bouget2017,
  author = {Bouget, David and Allan, Max and Stoyanov, Danail and Jannin, Pierre},

  issn = {1361-8415},
  journal = {Medical image analysis},
  publisher = {Elsevier},
  title = {{Vision-based and marker-less surgical tool detection and tracking: a review of the literature}},
  volume = {35},
  year = {2017},
  research_field={IM},
  data_type={RI},
  semanticscholar = {https://www.semanticscholar.org/paper/703f8fb5aefdc684c2ef7794a47bbd3bfabca628},
  doi = {10.1016/j.media.2016.09.003},
}

@article{Wang2017b,
  author = {Wang, Zerui and Liu, Ziwei and Ma, Qianli and Cheng, Alexis and Liu, Yun-hui and Kim, Sungmin and Deguet, Anton and Reiter, Austin and Kazanzides, Peter and Taylor, Russell H},

  issn = {2377-3766},
  journal = {IEEE Robotics and Automation Letters},
  number = {2},
  publisher = {IEEE},
  title = {{Vision-based calibration of dual RCM-based robot arms in human-robot collaborative minimally invasive surgery}},
  volume = {3},
  year = {2017},
  research_field={IM},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/1ac0078cdfdb0080ddcdb46c42fe2f22405d1c5d},
  doi = {10.1109/LRA.2017.2737485}
}

@inproceedings{Kim2016a,
  author = {Kim, Sungmin and Tan, Youri and Kazanzides, Peter and Bell, Muyinatu A Lediju},

  booktitle = {2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  isbn = {1509032878},
  publisher = {IEEE},
  title = {{Feasibility of photoacoustic image guidance for telerobotic endonasal transsphenoidal surgery}},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/f0321d338a9689fe331612c9b597a2e999d6e7b0}
}

@inproceedings{Che2016,
  author = {Che, Yuhang and Haro, Gabriel M and Okamura, Allison M},

  booktitle = {2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
  isbn = {1509032878},
  publisher = {IEEE},
  title = {{Two is not always better than one: Effects of teleoperation and haptic coupling}},
  year = {2016},
  research_field={HW},
  data_type={KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/f0321d338a9689fe331612c9b597a2e999d6e7b0}
}

@inproceedings{Elek2016,
  author = {Elek, Ren{\'{a}}ta and Nagy, Tam{\'{a}}s D{\'{a}}niel and Nagy, D{\'{e}}nes {\'{A}}kos and Kronreif, Gernot and Rudas, Imre J and Haidegger, Tam{\'{a}}s},

  booktitle = {2016 IEEE 20th Jubilee International Conference on Intelligent Engineering Systems (INES)},
  isbn = {1509012168},
  publisher = {IEEE},
  title = {{Recent trends in automating robotic surgery}},
  year = {2016},
  research_field={},
  data_type={}
}

@inproceedings{Garg2016b,
  author = {Garg, Animesh and Sen, Siddarth and Kapadia, Rishi and Jen, Yiming and McKinley, Stephen and Miller, Lauren and Goldberg, Ken},

  booktitle = {2016 IEEE International Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/COASE.2016.7743380},
  isbn = {978-1-5090-2409-4},
  publisher = {IEEE},
  title = {{Tumor localization using automated palpation with Gaussian Process Adaptive Sampling}},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/d319a1ac5e30272aeaafca8ca61eec7466af82c9},
  abstract = {In surgical tumor removal, inaccurate localization can lead to removal of excessive healthy tissue and failure to completely remove cancerous tissue. Automated palpation with a tactile sensor has the potential to precisely estimate the geometry of embedded tumors during robot-assisted minimally invasive surgery (RMIS). We formulate tumor boundary localization as a Bayesian optimization model along implicit curves over estimated tissue stiffness. We propose a Gaussian Process Adaptive Sampling algorithm called Implicit Level Set Upper Confidence Bound (ILS-UCB), that prioritizes sampling near a level set of the estimate. We compare the ILS-UCB algorithm to two alternative palpation algorithms: (1) Expected Variance Reduction (EVR), which emphasizes exploration by minimizing variance, and (2) Upper Confidence Bound (UCB), which balances exploration with exploitation using only the estimated mean. We compare these algorithms in simulated experiments varying the levels of measurement noise and bias. We find that ILS-UCB significantly outperforms the other two algorithms as measured by the symmetric difference between tumor boundary estimate and ground truth, reducing error by up to 10×. Physical experiments on a dVRK show that our approach can localize the tumor boundary with approximately the same accuracy as a dense raster scan while requiring at least 10× fewer measurements.}
}

@inproceedings{McKinley2016b,
  author = {McKinley, Stephen and Garg, Animesh and Sen, Siddarth and Gealy, David V. and McKinley, Jonathan P. and Jen, Yiming and {Menglong Guo} and Boyd, Doug and Goldberg, Ken},

  booktitle = {2016 IEEE International Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/COASE.2016.7743487},
  isbn = {978-1-5090-2409-4},
  publisher = {IEEE},
  title = {{An interchangeable surgical instrument system with application to supervised automation of multilateral tumor resection}},
  year = {2016},
  research_field={AU},
  data_type={KD},
  semanticscholar = {https://www.semanticscholar.org/paper/fd12947f733e7a8a76c1eaa2e1bd6c1be76a4099}
}

@inproceedings{Ruszkowski2016,
  author = {Ruszkowski, Angelica and Schneider, Caitlin and Mohareri, Omid and Salcudean, Septimiu},

  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1467380261},
  publisher = {IEEE},
  title = {{Bimanual teleoperation with heart motion compensation on the da Vinci{\textregistered} Research Kit: Implementation and preliminary experiments}},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e44596de68c5289a56998a1c5215434c381fc0c9},
  doi = {10.1109/ICRA.2016.7487141}
}

@inproceedings{Sen2016,
  author = {Sen, Siddarth and Garg, Animesh and Gealy, David V and McKinley, Stephen and Jen, Yiming and Goldberg, Ken},

  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1467380261},
  publisher = {IEEE},
  title = {{Automating multi-throw multilateral surgical suturing with a mechanical needle guide and sequential convex optimization}},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e44596de68c5289a56998a1c5215434c381fc0c9},
  doi = {10.1109/ICRA.2016.7487141}
}

@inproceedings{Murali2016,
  author = {Murali, Adithyavairavan and Garg, Animesh and Krishnan, Sanjay and Pokorny, Florian T and Abbeel, Pieter and Darrell, Trevor and Goldberg, Ken},

  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1467380261},
  publisher = {IEEE},
  title = {{Tsc-dl: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with deep learning}},
  year = {2016},
  research_field={TR},
  data_type={RI and KD}
}

@inproceedings{Kaplan2016a,
  author = {Kaplan, Kirsten E. and Nichols, Kirk A. and Okamura, Allison M.},

  booktitle = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2016.7487199},
  isbn = {978-1-4673-8026-3},
  publisher = {IEEE},
  title = {{Toward human-robot collaboration in surgery: Performance assessment of human and robotic agents in an inclusion segmentation task}},
  year = {2016},
  research_field={AU},
  data_type={RI and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/25d2eb190571d46d18c046fc6758d83db80f8bbf}
}

@inproceedings{Pachtrachai2016a,
  author = {Pachtrachai, Krittin and Allan, Max and Pawar, Vijay and Hailes, Stephen and Stoyanov, Danail},

  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2016.7759387},
  isbn = {978-1-5090-3762-9},
  publisher = {IEEE},
  title = {{Hand-eye calibration for robotic assisted minimally invasive surgery without a calibration object}},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/2031c29890fecbe746039194b6d422f39850b0d8},
}

@inproceedings{Chen2016a,
  author = {Chen, Zihan and Malpani, Anand and Chalasani, Preetham and Deguet, Anton and Vedula, S. Swaroop and Kazanzides, Peter and Taylor, Russell H.},

  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2016.7759365},
  isbn = {978-1-5090-3762-9},
  publisher = {IEEE},
  title = {{Virtual fixture assistance for needle passing and knot tying}},
  year = {2016},
  research_field={AU},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/9ef4b2ab8ecfba1451a21b5820d1aeacd00fa904}
}

@inproceedings{Munawar2016a,
  author = {Munawar, Adnan and Fischer, Gregory},

  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1509037624},
  publisher = {IEEE},
  title = {{Towards a haptic feedback framework for multi-DOF robotic laparoscopic surgery platforms}},
  year = {2016},
  research_field={HW},
  data_type={KD and DD and SD}
}

@inproceedings{Grammatikopoulou2016a,
  author = {Grammatikopoulou, Maria and Leibrandt, Konrad and Yang, Guang-Zhong},

  booktitle = {2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2016.7759635},
  isbn = {978-1-5090-3762-9},
  publisher = {IEEE},
  title = {{Motor channelling for safe and effective dynamic constraints in Minimally Invasive Surgery}},
  year = {2016},
  research_field={HW},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/370c41b720a7bfd13cf607778c87407ff88a235f},
  abstract = {Motor channelling is a concept to provide navigation and sensory feedback to operators in master-slave surgical setups. It is beneficial since the introduction of robotic surgery creates a physical separation between the surgeon and patient anatomy. Active Constraints/Virtual Fixtures are proposed which integrate Guidance and Forbidden Region Constraints into a unified control framework. The developed approach provides guidance and safe manipulation to improve precision and reduce the risk of inadvertent tissue damage. Online three-degree-of-freedom motion prediction and compensation of the target anatomy is performed to complement the master constraints. The presented Active Constraints concept is applied to two clinical scenarios; surface scanning for in situ medical imaging and vessel manipulation in cardiac surgery. The proposed motor channelling control strategy is implemented on the da Vinci Surgical System using the da Vinci Research Kit (dVRK) and its effectiveness is demonstrated through a detailed user study.},
}

@article{Kim2016,
  author = {Kim, Myungjoon and Lee, Chiwon and Park, Woo Jung and Suh, Yun Suhk and Yang, Han Kwang and Kim, H Jin and Kim, Sungwan},

  issn = {1475-925X},
  journal = {Biomedical engineering online},
  number = {1},
  publisher = {Springer},
  title = {{A development of assistant surgical robot system based on surgical-operation-by-wire and hands-on-throttle-and-stick}},
  volume = {15},
  year = {2016},
  research_field={HW},
  data_type={RI and KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/9d56fc4aecd26875376a74abc822ef645a62ace6},
  doi = {10.1186/s12938-016-0189-7},
  abstract = {Robot-assisted laparoscopic surgery offers several advantages compared with open surgery and conventional minimally invasive surgery. However, one issue that needs to be resolved is a collision between the robot arm and the assistant instrument. This is mostly caused by miscommunication between the surgeon and the assistant. To resolve this limitation, an assistant surgical robot system that can be simultaneously manipulated via a wireless controller is proposed to allow the surgeon to control the assistant instrument. The system comprises two novel master interfaces (NMIs), a surgical instrument with a gripper actuated by a micromotor, and 6-axis robot arm. Two NMIs are attached to master tool manipulators of da Vinci research kit (dVRK) to control the proposed system simultaneously with patient side manipulators of dVRK. The developments of the surgical instrument and NMI are based on surgical-operation-by-wire concept and hands-on-throttle-and-stick concept from the earlier research, respectively. Tests for checking the accuracy, latency, and power consumption of the NMI are performed. The gripping force, reaction time, and durability are assessed to validate the surgical instrument. The workspace is calculated for estimating the clinical applicability. A simple peg task using the fundamentals of laparoscopic surgery board and an in vitro test are executed with three novice volunteers. The NMI was operated for 185 min and reflected the surgeon’s decision successfully with a mean latency of 132 ms. The gripping force of the surgical instrument was comparable to that of conventional systems and was consistent even after 1000 times of gripping motion. The reaction time was 0.4 s. The workspace was calculated to be 8397.4 cm3. Recruited volunteers were able to execute the simple peg task within the cut-off time and successfully performed the in vitro test without any collision. Various experiments were conducted and it is verified that the proposed assistant surgical robot system enables collision-free and simultaneous operation of the dVRK’s robot arm and the proposed assistant robot arm. The workspace is appropriate for the performance of various kinds of surgeries. Therefore, the proposed system is expected to provide higher safety and effectiveness for the current surgical robot system.},
}

@article{Munawar2016b,
  author = {Munawar, Adnan and Fischer, Gregory},

  doi = {10.3389/frobt.2016.00047},
  issn = {2296-9144},
  journal = {Frontiers in Robotics and AI},
  title = {{A Surgical Robot Teleoperation Framework for Providing Haptic Feedback Incorporating Virtual Environment-Based Guidance}},
  volume = {3},
  year = {2016},
  research_field={HW},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/af7aacf2f3c60bbf7d69aae4c48ee62caaee385a},
  abstract = {In robot-assisted tele-operated laparoscopic surgeries, the patient side manipulators are controlled via the master manipulators that are controlled by the surgeon. The current generation of robots approved for laparoscopic surgery lack haptic feedback. In theory, haptic feedback would enhance the surgical procedures by enabling better coordination between the hand movements that are improved by the tactile sense of the operating environment. This research presents an overall control framework for a haptic feedback on existing robot platforms, and demonstrated on the daVinci Research Kit (dVRK) system. The paper discusses the implementation of a flexible framework that incorporates a stiffness control with gravity compensation for the surgeons manipulator and a sensing and collision detection algorithm for calculating the interaction between the patients manipulators and the surgical area.},
}

@inproceedings{Wang2016,
  author = {Wang, Long and Chen, Zihan and Chalasani, Preetham and Pile, Jason and Kazanzides, Peter and Taylor, Russell H and Simaan, Nabil},

  booktitle = {ASME 2016 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference},
  publisher = {American Society of Mechanical Engineers Digital Collection},
  title = {{Updating virtual fixtures from exploration data in force-controlled model-based telemanipulation}},
  year = {2016},
  research_field={HW},
  data_type={RI and KD and DD and SD and ED}
}

@article{Du2016a,
  author = {Du, Xiaofei and Allan, Maximilian and Dore, Alessio and Ourselin, Sebastien and Hawkes, David and Kelly, John D. and Stoyanov, Danail},

  doi = {10.1007/s11548-016-1393-4},
  issn = {1861-6410},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  number = {6},
  title = {{Combined 2D and 3D tracking of surgical instruments for minimally invasive and robotic-assisted surgery}},
  volume = {11},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/e6b49eb5a3b4c62902f73563fbed88a8d92d1adc},
  abstract = {PurposeComputer-assisted interventions for enhanced minimally invasive surgery (MIS) require tracking of the surgical instruments. Instrument tracking is a challenging problem in both conventional and robotic-assisted MIS, but vision-based approaches are a promising solution with minimal hardware integration requirements. However, vision-based methods suffer from drift, and in the case of occlusions, shadows and fast motion, they can be subject to complete tracking failure.MethodsIn this paper, we develop a 2D tracker based on a Generalized Hough Transform using SIFT features which can both handle complex environmental changes and recover from tracking failure. We use this to initialize a 3D tracker at each frame which enables us to recover 3D instrument pose over long sequences and even during occlusions.ResultsWe quantitatively validate our method in 2D and 3D with ex vivo data collected from a DVRK controller as well as providing qualitative validation on robotic-assisted in vivo data.ConclusionsWe demonstrate from our extended sequences that our method provides drift-free robust and accurate tracking. Our occlusion-based sequences additionally demonstrate that our method can recover from occlusion-based failure. In both cases, we show an improvement over using 3D tracking alone suggesting that combining 2D and 3D tracking is a promising solution to challenges in surgical instrument tracking.},
}

@inproceedings{Ye2016,
  author = {Ye, Menglong and Zhang, Lin and Giannarou, Stamatia and Yang, Guang-Zhong},

  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention -- MICCAI},
  publisher = {Springer},
  title = {{Real-time 3d tracking of articulated tools for robotic surgery}},
  year = {2016},
  research_field={},
  data_type={}
}

@article{Liu2016,
  author = {Liu, Taoming and Cavusoglu, Murat Cenk},

  issn = {1545-5955},
  journal = {IEEE Transactions on Automation Science and Engineering},
  number = {2},
  publisher = {IEEE},
  title = {{Needle grasp and entry port selection for automatic execution of suturing tasks in robotic minimally invasive surgery}},
  volume = {13},
  year = {2016},
  research_field={AU},
  data_type={RI and KD and ED}
}

@inproceedings{Shahzada2016a,
  author = {Shahzada, Kaspar S. and Yurkewich, Aaron and Xu, Ran and Patel, Rajni V.},

  booktitle = {Optical Fibers and Sensors for Medical Diagnostics and Treatment Applications XVI},
  doi = {10.1117/12.2213385},
  editor = {Gannot, Israel},
  title = {{Sensorization of a surgical robotic instrument for force sensing}},
  year = {2016},
  research_field={HW},
  data_type={KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/4e8f6e73bfbfbdda0a4388108ba355143ef4d22b}
}

@inproceedings{Eslamian2016,
  author = {Eslamian, Shahab and Reisner, Luke A and King, Brady W and Pandya, Abhilash K},

  booktitle = {Stud Health Technol Inform},
  title = {{Towards the Implementation of an Autonomous Camera Algorithm on the da Vinci Platform.}},
  year = {2016},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/0d15aff91a3c60089003db2c958d38e80925ea5f},
  doi = {10.1002/aorn.12159}
}

@inproceedings{Ferraguti2015,
  author = {Ferraguti, Federica and Preda, Nicola and {De Rossi}, Giacomo and Bonfe, Marcello and Muradore, Riccardo and Fiorini, Paolo and Secchi, Cristian},

  booktitle = {2015 European Control Conference (ECC)},
  isbn = {3952426938},
  publisher = {IEEE},
  title = {{A two-layer approach for shared control in semi-autonomous robotic surgery}},
  year = {2015},
  research_field={},
  data_type={}
}

@inproceedings{Qian2015,
  author = {Qian, Long and Chen, Zihan and Kazanzides, Peter},

  booktitle = {2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)},
  isbn = {1467379298},
  publisher = {IEEE},
  title = {{An Ethernet to FireWire bridge for real-time control of the da Vinci Research Kit (dVRK)}},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/ea13cc8fc5198fec59645f2d729036f9e8707f1e}
}

@inproceedings{McKinley2015b,
  author = {McKinley, Stephen and Garg, Animesh and Sen, Siddarth and Kapadia, Rishi and Murali, Adithyavairavan and Nichols, Kirk and Lim, Susan and Patil, Sachin and Abbeel, Pieter and Okamura, Allison M. and Goldberg, Ken},

  booktitle = {2015 IEEE International Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/CoASE.2015.7294253},
  isbn = {978-1-4673-8183-3},
  publisher = {IEEE},
  title = {{A single-use haptic palpation probe for locating subcutaneous blood vessels in robot-assisted minimally invasive surgery}},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/2f63e718e5593ef5151c7024a94ea05eb4730f6d}
}

@inproceedings{Vozar2015a,
  author = {Vozar, Steve and L{\'{e}}onard, Simon and Kazanzides, Peter and Whitcomb, Louis L},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1479969230},
  publisher = {IEEE},
  title = {{Experimental evaluation of force control for virtual-fixture-assisted teleoperation for on-orbit manipulation of satellite thermal blanket insulation}},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d}
}

@inproceedings{Anooshahpour2015a,
  author = {Anooshahpour, Farshad and Polushin, Ilia G. and Patel, Rajni V.},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2015.7139945},
  isbn = {978-1-4799-6923-4},
  publisher = {IEEE},
  title = {{Tissue compliance determination using a da Vinci instrument}},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/8634b048d81cee79e92c911c19225d13fb0c6506}
}

@inproceedings{Ruszkowski2015a,
  author = {Ruszkowski, Angelica and Mohareri, Omid and Lichtenstein, Sam and Cook, Richard and Salcudean, Septimiu},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2015.7139812},
  isbn = {978-1-4799-6923-4},
  publisher = {IEEE},
  title = {{On the feasibility of heart motion compensation on the daVinci surgical robot for coronary artery bypass surgery: Implementation and user studies}},
  year = {2015},
  research_field={HW},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/183a31c43ccdaf52c88e772ee251717ca827a497}
}

@inproceedings{Khalaji2015a,
  author = {Khalaji, Iman and Naish, Michael D. and Patel, Rajni V.},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2015.7139236},
  isbn = {978-1-4799-6923-4},
  publisher = {IEEE},
  title = {{Articulating minimally invasive ultrasonic tool for robotics-assisted surgery}},
  year = {2015},
  research_field={HW},
  data_type={KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/a80f824e53015d36c548403a92cf80105edc0c60}
}

@inproceedings{Kim2015a,
  author = {Kim, Lawrence H. and Bargar, Clifford and Che, Yuhang and Okamura, Allison M.},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2015.7139948},
  isbn = {978-1-4799-6923-4},
  publisher = {IEEE},
  title = {{Effects of master-slave tool misalignment in a teleoperated surgical robot}},
  year = {2015},
  research_field={HW},
  data_type={KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2788f85b0e28f9a170737d763c33f99bdc26e6ca}
}

@inproceedings{Nisky2015,
  author = {Nisky, Ilana and Che, Yuhang and Quek, Zhan Fan and Weber, Matthew and Hsieh, Michael H and Okamura, Allison M},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1479969230},
  publisher = {IEEE},
  title = {{Teleoperated versus open needle driving: Kinematic analysis of experienced surgeons and novice users}},
  year = {2015},
  research_field={TR},
  data_type={KD and DD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d}
}

@inproceedings{Leonard2015,
  author = {Leonard, Simon},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1479969230},
  publisher = {IEEE},
  title = {{Registration of planar virtual fixtures by using augmented reality with dynamic textures}},
  year = {2015},
  research_field={IM},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d}
}

@inproceedings{Liu2015,
  author = {Liu, Taoming and {\c{C}}avuşoğlu, M Cenk},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1479969230},
  publisher = {IEEE},
  title = {{Optimal needle grasp selection for automatic execution of suturing tasks in robotic minimally invasive surgery}},
  year = {2015},
  research_field={AU},
  data_type={RI and KD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d}
}

@inproceedings{Murali2015c,
  author = {Murali, Adithyavairavan and Sen, Siddarth and Kehoe, Ben and Garg, Animesh and McFarland, Seth and Patil, Sachin and Boyd, W. Douglas and Lim, Susan and Abbeel, Pieter and Goldberg, Ken},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2015.7139344},
  isbn = {978-1-4799-6923-4},
  publisher = {IEEE},
  title = {{Learning by observation for surgical subtasks: Multilateral cutting of 3D viscoelastic and 2D Orthotropic Tissue Phantoms}},
  year = {2015},
  research_field={AU},
  data_type={RI and KD and DD},
  semanticscholar = {https://www.semanticscholar.org/paper/7ff7a763c7b09459524329d689d953d43b740d04},
}

@inproceedings{Quek2015,
  author = {Quek, Zhan Fan and Schorr, Samuel B and Nisky, Ilana and Provancher, William R and Okamura, Allison M},

  booktitle = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1479969230},
  publisher = {IEEE},
  title = {{Sensory substitution of force and torque using 6-DoF tangential and normal skin deformation feedback}},
  year = {2015},
  research_field={HW},
  data_type={KD and DD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/2e23f44b2f31bfb32ae92544cfee5bfcfb3b2f5d}
}

@inproceedings{Shamaei2015b,
  author = {Shamaei, Kamran and Che, Yuhang and Murali, Adithyavairavan and Sen, Siddarth and Patil, Sachin and Goldberg, Ken and Okamura, Allison M.},

  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2015.7353556},
  isbn = {978-1-4799-9994-1},
  publisher = {IEEE},
  title = {{A paced shared-control teleoperated architecture for supervised automation of multilateral surgical tasks}},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/930f4e1c4a3b0e9b28ac4a782e40cbcd22ce35ee}
}

@inproceedings{Tong2015a,
  author = {Tong, Irene and Mohareri, Omid and Tatasurya, Samuel and Hennessey, Craig and Salcudean, Septimiu},

  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  doi = {10.1109/IROS.2015.7353648},
  isbn = {978-1-4799-9994-1},
  publisher = {IEEE},
  title = {{A retrofit eye gaze tracker for the da Vinci and its integration in task execution using the da Vinci Research Kit}},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/9fa9e607fca7601c94abac984d307cf1ecc6b50a}
}

@inproceedings{Vozar2015,
  author = {Vozar, Steve and Chen, Zihan and Kazanzides, Peter and Whitcomb, Louis L},

  booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  isbn = {1479999946},
  publisher = {IEEE},
  title = {{Preliminary study of virtual nonholonomic constraints for time-delayed teleoperation}},
  year = {2015},
  research_field={},
  data_type={}
}

@article{Kumar2015,
  author = {Kumar, Suren and Singhal, Pankaj and Krovi, Venkat N},

  issn = {2168-2356},
  journal = {IEEE Design & Test},
  number = {5},
  publisher = {IEEE},
  title = {{Computer-vision-based decision support in surgical robotics}},
  volume = {32},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/62b95792400fb513f02eb21640fda332ed55d888},
  doi = {10.1109/MDAT.2015.2465135}
}

@inproceedings{RuszkowskiHamlyn2015,
  author = {Ruszkowski, Angelica and Quek, Zhan Fan and Okamura, Allison M. and Salcudean, Septimiu},
  title = {{Dynamic Non-Continuous Virtual Fixtures for Operations on a Beating Heart using the da Vinci{\textregistered} Surgical Robots}},
  booktitle = {The Hamlyn Symposium on Medical Robotics},
  year = {2015},
  pages = {45--46},
  month = {June},
  address = {London, UK}
}

@inproceedings{Ruszkowski2015,
  author = {Ruszkowski, Angelica and Quek, Zhan Fan and Okamura, Allison and Salcudean, Septimiu},
  booktitle = {ICRA Workshop on Shared Frameworks for Medical Robotics Research},
  title = {{Simulink{\textregistered} to C++ interface for controller development on the da Vinci{\textregistered} Research Kit (dVRK)}},
  year = {2015},
  month = {May}
}

@article{Chen2015,
  author = {Chen, Zihan and Deguet, Anton and Vozar, Steve and Munawar, Adnan and Fischer, Gregory and Kazanzides, Peter},

  journal = {2015 IEEE International Conference on Robotics and Automation (ICRA)},
  title = {{Interfacing the da Vinci Research Kit (dVRK) with the Robot Operating System (ROS)}},
  year = {2015},
  research_field={HW},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/896c8df3bee4dd806bea30e29be62ab5e71ea3c8}
}

@inproceedings{Allan2015,
  author = {Allan, Max and Chang, Ping-Lin and Ourselin, S{\'{e}}bastien and Hawkes, David J and Sridhar, Ashwin and Kelly, John and Stoyanov, Danail},

  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  publisher = {Springer},
  title = {{Image based surgical instrument pose estimation with multi-class labelling and optical flow}},
  year = {2015},
  research_field={IM},
  data_type={RI}
}

@article{Takacs2015,
  author = {Tak{\'{a}}cs, {\'{A}}rp{\'{a}}d and Jord{\'{a}}n, S{\'{a}}ndor and Nagy, D{\'{e}}nes and Pausits, P{\'{e}}ter and Haidegger, Tam{\'{a}}s and Tar, J{\'{o}}zsef K and Rudas, Imre J},

  issn = {2247-0948},
  journal = {MACRo 2015},
  number = {1},
  publisher = {Sciendo},
  title = {{Joint platforms and community efforts in surgical robotics research}},
  volume = {1},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/1a2fdc5568ce11bd668e7a5c1bd39a207f221f96},
  doi = {10.1515/macro-2015-0009},
  abstract = {Abstract In modern medical research and development, the variety of research tools has extended in the previous years. Exploiting the benefits of shared hardware platforms and software frameworks is crucial to keep up with the technological development rate. Sharing knowledge in terms of algorithms, applications and instruments allows researchers to help each other’s work effectively. Community workshops and publications provide a throughout overview of system design, capabilities, know-how sharing and limitations. This paper provides sneak peek into the emerging collaborative platforms, focusing on available open-source research kits, software frameworks, cloud applications, teleoperation training environments and shared domain ontologies.},
}

@inproceedings{Pratt2015c,
  author = {Pratt, Philip and Hughes-Hallett, Archie and Zhang, Lin and Patel, Nisha and Mayer, Erik and Darzi, Ara and Yang, Guang-Zhong},

  booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
  title = {{Autonomous Ultrasound-Guided Tissue Dissection}},
  year = {2015},
  research_field={},
  data_type={}
}

@article{Kazanzides2015,
  author = {Kazanzides, Peter and Deguet, Anton and Vagvolgyi, Balazs and Chen, Zihan and Taylor, Russell H},

  issn = {0025-6501},
  journal = {Mechanical Engineering},
  number = {09},
  publisher = {American Society of Mechanical Engineers},
  title = {{Modular interoperability in surgical robotics software}},
  volume = {137},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/0d210fdfe1fc2413594944d725dcd33ff241b468},
  doi = {10.1115/1.2015-SEP-10},
}

@article{Takacs2015a,
  author = {Tak{\'{a}}cs, {\'{A}}rp{\'{a}}d and Rudas, Imre and Haidegger, Tam{\'{a}}s},

  issn = {2069-7449},
  journal = {Acta Universitatis Sapientiae; Electrical and Mechanical Engineering},
  number = {6},
  publisher = {Sapientia Hungarian University of Transylvania, Scientia Publishing House},
  title = {{Open-source research platforms and system integration in modern surgical robotics}},
  volume = {14},
  year = {2015},
  research_field={AU},
  data_type={RI and KD and SD and ED},
  semanticscholar = {https://www.semanticscholar.org/paper/d27bbc03cd3c28a08991ed44c08b5d7a2a8085a1}
}

@article{Jarc2015,
  author = {Jarc, Anthony M and Nisky, Ilana},

  issn = {1662-5161},
  journal = {Frontiers in human neuroscience},
  publisher = {Frontiers},
  title = {{Robot-assisted surgery: an emerging platform for human neuroscience research}},
  volume = {9},
  year = {2015},
  research_field={TR},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/0bd0a15287d42b363d1d7096831df45863dffb2e},
  doi = {10.3389/fnhum.2015.00315},
  abstract = {Classic studies in human sensorimotor control use simplified tasks to uncover fundamental control strategies employed by the nervous system. Such simple tasks are critical for isolating specific features of motor, sensory, or cognitive processes, and for inferring causality between these features and observed behavioral changes. However, it remains unclear how these theories translate to complex sensorimotor tasks or to natural behaviors. Part of the difficulty in performing such experiments has been the lack of appropriate tools for measuring complex motor skills in real-world contexts. Robot-assisted surgery (RAS) provides an opportunity to overcome these challenges by enabling unobtrusive measurements of user behavior. In addition, a continuum of tasks with varying complexity—from simple tasks such as those in classic studies to highly complex tasks such as a surgical procedure—can be studied using RAS platforms. Finally, RAS includes a diverse participant population of inexperienced users all the way to expert surgeons. In this perspective, we illustrate how the characteristics of RAS systems make them compelling platforms to extend many theories in human neuroscience, as well as, to develop new theories altogether.},
}

@article{Schneider2015,
  author = {Schneider, Caitlin and Nguan, Christopher and Rohling, Robert and Salcudean, Septimiu},

  issn = {0018-9294},
  journal = {IEEE Transactions on Biomedical Engineering},
  number = {2},
  publisher = {IEEE},
  title = {{Tracked “pick-up” ultrasound for robot-assisted minimally invasive surgery}},
  volume = {63},
  year = {2015},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/119345923301d08bfccdbcb1d7ebf8396bf8a4a5},
  doi = {10.1109/TBME.2015.2453173}
}

@inproceedings{Mahler2014b,
  author = {Mahler, Jeffrey and Krishnan, Sanjay and Laskey, Michael and Sen, Siddarth and Murali, Adithyavairavan and Kehoe, Ben and Patil, Sachin and Wang, Jiannan and Franklin, Mike and Abbeel, Pieter and Goldberg, Ken},

  booktitle = {2014 IEEE International Conference on Automation Science and Engineering (CASE)},
  doi = {10.1109/CoASE.2014.6899377},
  isbn = {978-1-4799-5283-0},
  publisher = {IEEE},
  title = {{Learning accurate kinematic control of cable-driven surgical robots using data cleaning and Gaussian Process Regression}},
  year = {2014},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/3fe1910395e97a5ea36b64f3372155c9a6c8a119},
}

@inproceedings{Kazanzides2014b,
  author = {Kazanzides, Peter and Chen, Zihan and Deguet, Anton and Fischer, Gregory S and Taylor, Russell H and DiMaio, Simon P},

  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  isbn = {1479936855},
  publisher = {IEEE},
  title = {{An open-source research kit for the da Vinci{\textregistered} Surgical System}},
  year = {2014},
  research_field={HW},
  data_type={RI and KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/5a6b873ae7d4b9b3dcc95841a58761b076082a3b},
  doi = {10.1109/icra18334.2014}
}

@inproceedings{Kehoe2014b,
  author = {Kehoe, Ben and Kahn, Gregory and Mahler, Jeffrey and Kim, Jonathan and Lee, Alex and Lee, Anna and Nakagawa, Keisuke and Patil, Sachin and Boyd, W. Douglas and Abbeel, Pieter and Goldberg, Ken},

  booktitle = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  doi = {10.1109/ICRA.2014.6907040},
  isbn = {978-1-4799-3685-4},
  publisher = {IEEE},
  title = {{Autonomous multilateral debridement with the Raven surgical robot}},
  year = {2014},
  research_field={AU},
  data_type={RI and KD},
  semanticscholar = {https://www.semanticscholar.org/paper/f3f45d9d37d0e2efbc97ce862a70bba1cbc6c4e6},
}

@inproceedings{Chen2014a,
  author = {Chen, Zihan and Kazanzides, Peter},

  booktitle = {2014 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)},
  doi = {10.1109/TePRA.2014.6869144},
  isbn = {978-1-4799-4605-1},
  publisher = {IEEE},
  title = {{Multi-kilohertz control of multiple robots via IEEE-1394 (firewire)}},
  year = {2014},
  research_field={HW},
  data_type={KD and DD and SD},
  semanticscholar = {https://www.semanticscholar.org/paper/cb7c11568f1337b6cbb35a5963df5e76d1050ace}
}

@inproceedings{Mohareri2014,
  author = {Mohareri, Omid and Schneider, Caitlin and Salcudean, Septimiu},

  booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  isbn = {1479969346},
  publisher = {IEEE},
  title = {{Bimanual telerobotic surgery with asymmetric force feedback: a daVinci{\textregistered} surgical system implementation}},
  year = {2014},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/267cb7ecfc9fb0f5380aa74cadac4ed608461810},
  doi = {10.1109/iros20755.2014}
}

@inproceedings{Kazanzides2014,
  title = {An Open-Source Research Kit for the da Vinci (R) Surgical System},
  author = {Kazanzides, Peter and Chen, Zihan and Deguet, Anton and Fischer, Gregory S. and Taylor, Russell H. and DiMaio, Simon P.},

  year = {2014},
  date = {2014-06-01},
  booktitle = {IEEE Intl. Conf. on Robotics and Auto. (ICRA)},
  pages = {6434-6439},
  address = {Hong Kong, China},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/014da00e522ee5a5391a615d855287fb2d4a8f54},
  doi = {10.1109/ICRA.2014.6907809}
}

@article{Lee2014,
  author = {Lee, Chiwon and Park, Woo Jung and Kim, Myungjoon and Noh, Seungwoo and Yoon, Chiyul and Lee, Choonghee and Kim, Youdan and Kim, Hyeon Hoe and Kim, Hee Chan and Kim, Sungwan},

  issn = {1475-925X},
  journal = {Biomedical engineering online},
  number = {1},
  publisher = {Springer},
  title = {{Pneumatic-type surgical robot end-effector for laparoscopic surgical-operation-by-wire}},
  volume = {13},
  year = {2014},
  research_field={},
  data_type={},
  semanticscholar = {https://www.semanticscholar.org/paper/25426d85be595c91ae1e2fa5d737eb2f024001a6},
  doi = {10.1186/1475-925X-13-130},
  abstract = {BackgroundAlthough minimally invasive surgery (MIS) affords several advantages compared to conventional open surgery, robotic MIS systems still have many limitations. One of the limitations is the non-uniform gripping force due to mechanical strings of the existing systems. To overcome this limitation, a surgical instrument with a pneumatic gripping system consisting of a compressor, catheter balloon, micro motor, and other parts is developed.MethodThis study aims to implement a surgical instrument with a pneumatic gripping system and pitching/yawing joints using micro motors and without mechanical strings based on the surgical-operation-by-wire (SOBW) concept. A 6-axis external arm for increasing degrees of freedom (DOFs) is integrated with the surgical instrument using LabVIEW® for laparoscopic procedures. The gripping force is measured over a wide range of pressures and compared with the simulated ideal step function. Furthermore, a kinematic analysis is conducted. To validate and evaluate the system’s clinical applicability, a simple peg task experiment and workspace identification experiment are performed with five novice volunteers using the fundamentals of laparoscopic surgery (FLS) board kit. The master interface of the proposed system employs the hands-on-throttle-and-stick (HOTAS) controller used in aerospace engineering. To develop an improved HOTAS (iHOTAS) controller, 6-axis force/torque sensor was integrated in the special housing.ResultsThe mean gripping force (after 1,000 repetitions) at a pressure of 0.3 MPa was measured to be 5.8 N. The reaction time was found to be 0.4 s, which is almost real-time. All novice volunteers could complete the simple peg task within a mean time of 176 s, and none of them exceeded the 300 s cut-off time. The system’s workspace was calculated to be 11,157.0 cm3.ConclusionsThe proposed pneumatic gripping system provides a force consistent with that of other robotic MIS systems. It provides near real-time control. It is more durable than the existing other surgical robot systems. Its workspace is sufficient for clinical surgery. Therefore, the proposed system is expected to be widely used for laparoscopic robotic surgery. This research using iHOTAS will be applied to the tactile force feedback system for surgeon’s safe operation.},
}

@inproceedings{Xia2013,
  title = {Model-Based Telerobotic Control with Virtual Fixtures For Satellite Servicing Tasks},
  author = {Xia, T. and Leonard, S. and Kandaswamy, I. and Blank, A. and Whitcomb, L.L. and Kazanzides, P.},

  year = {2013},
  date = {2013-05-01},
  booktitle = {IEEE Intl. Conf. on Robotics and Automation (ICRA)},
  pages = {1479-1484},
  address = {Karlsruhe, Germany},
  keywords = {dvrk, space},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/5bcf1f1d5f29517017d40a241eb6684275cc3229},
  doi = {10.1109/ICRA.2013.6630766}
}

@inproceedings{Chen2013,
  author = {Chen, Zihan and Deguet, Anton and Taylor, Russell and DiMaio, Simon and Fischer, Gregory and Kazanzides, Peter},

  booktitle = {Proceedings of the MICCAI Workshop on Systems and Architecture for Computer Assisted Interventions, Nagoya, Japan},
  title = {{An open-source hardware and software platform for telesurgical robotics research}},
  year = {2013},
  research_field={HW},
  data_type={RI and KD and DD and SD}
}

@inproceedings{Xia2012,
  title = {Augmented Reality Environment with Virtual Fixtures for Robotic Telemanipulation in Space},
  author = {Xia, Tian and Leonard, Simon and Deguet, Anton and Whitcomb, Louis L. and Kazanzides, Peter},

  year = {2012},
  date = {2012-10-01},
  booktitle = {IEEE/RSJ Intl. Conf. on Intell. Robots and Systems (IROS)},
  pages = {5059-5064},
  address = {Vilamoura, Portugal},
  keywords = {dvrk, space},
  pubstate = {published},
  tppubtype = {inproceedings}
}

@inproceedings{Xia2011,
  title = {A Constrained Optimization Approach to Virtual Fixtures for Multi-Robot Collaborative Teleoperation},
  author = {Xia, Tian and Kapoor, Ankur and Kazanzides, Peter and H., Taylor Russell},

  year = {2011},
  date = {2011-09-01},
  booktitle = {IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)},
  pages = {639-644},
  address = {San Francisco, CA},
  keywords = {dvrk},
  pubstate = {published},
  tppubtype = {inproceedings},
  semanticscholar = {https://www.semanticscholar.org/paper/12654cc0342fa3c70df33417d93dc7a6a87f31a2},
  doi = {10.1109/IROS.2011.6095056}
}
